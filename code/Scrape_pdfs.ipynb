{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rational-palestinian",
   "metadata": {},
   "source": [
    "Logic of web-scraping\n",
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to know the following:\n",
    "\n",
    "The location (i.e., web address) where the web page can be accessed. For example, the UK Data Service homepage can be accessed via https://ukdataservice.ac.uk/.\n",
    "The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser.\n",
    "And do the following:\n",
    "\n",
    "Request the web page using its web address.\n",
    "Parse the structure of the web page so your programming language can work with its contents.\n",
    "Extract the information we are interested in.\n",
    "Write this information to a file for future use.\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this pseudo-code, as it is captures the main tasks and the order in which they need to be executed.\n",
    "\n",
    "For our first example, let's convert the steps above into executable Python code for capturing data about Covid-19.\n",
    "\n",
    "Example: Capturing Covid-19 data\n",
    "Worldometer is a website that provides up-to-date statistics on the following domains: the global population; food, water and energy consumption; environmental degradation and many others (known as its Real Time Statistics Project). In its own words:[1]\n",
    "\n",
    "Worldometer is run by an international team of developers, researchers, and volunteers with the goal of making world statistics available in a thought-provoking and time relevant format to a wide audience around the world. Worldometer is owned by Dadax, an independent company. We have no political, governmental, or corporate affiliation.\n",
    "\n",
    "Since the outbreak of Covid-19 it has provided regular daily snapshots on the progress of this disease, both globally and at a country level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-mailing",
   "metadata": {},
   "source": [
    "Identifying the web address\n",
    "The website can be accessed here: https://www.worldometers.info/coronavirus/\n",
    "\n",
    "Let's work through the steps necessary to collect data about the number of Covid-19 cases, deaths and recoveries globally.\n",
    "\n",
    "First, let's become familiar with this website: click on the link below to view the web page in your browser: https://www.worldometers.info/coronavirus/\n",
    "\n",
    "(Note: it possible to load websites into Python in order to view them, however the website we are interested in doesn't allow this. See the example code below for how it would work for a different website - just remove the quotation marks enclosing the code and run the cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-manitoba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom IPython.display import IFrame\\n\\nIFrame(\"https://ukdataservice.ac.uk/\", width=\"600\", height=\"650\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://ukdataservice.ac.uk/\", width=\"600\", height=\"650\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-archive",
   "metadata": {},
   "source": [
    "Locating information\n",
    "The statistics we need are near the top of the page under the following headings:\n",
    "\n",
    "Coronavirus Cases:\n",
    "Deaths:\n",
    "Recovered:\n",
    "However, we need more information than this in order to scrape the statistics. Websites are written in a langauge called HyperText Markup Language (HTML), which can be understood as follows:[2]\n",
    "\n",
    "HTML describes the structure of a web page\n",
    "HTML consists of a series of elements\n",
    "HTML elements tell the browser how to display the content\n",
    "HTML elements are represented by tags\n",
    "HTML tags label pieces of content such as \"heading\", \"paragraph\", \"table\", and so on\n",
    "Browsers do not display the HTML tags, but use them to render the content of the page\n",
    "Visually inspecting the underlying HTML code\n",
    "Therefore, what we need are the tags that identify the section of the web page where the statistics are stored. We can discover the tags by examining the source code (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select View Page Source from the list of options.\n",
    "\n",
    "TASK: Try this yourself with the Worldometer web page.\n",
    "\n",
    "The snippet below shows sample source code for the section of the Covid-19 web page we are interested in.\n",
    "\n",
    "\n",
    "In the above example, we can see multiple tags that contain various elements (e.g., text content, other tags). For instance, we can see that the Covid-19 statistics are enclosed in <span><\\span> tags, which themselves are located within <div><\\div> tags.\n",
    "\n",
    "As you can see, exploring and locating the contents of a web page remains a manual and visual process, and in Brooker's estimation (2020, 252):\n",
    "\n",
    "Hence, more so than the actual Python, it's the detective work of unpicking the internal structure of a webpage that is probably the most vital skill here.\n",
    "\n",
    "Requesting the web page\n",
    "Now that we possess the necessary information, let's begin the process of scraping the web page. There is a preliminary step, which is setting up Python with the modules it needs to perform the web-scrape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "combined-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Using legacy 'setup.py install' for bs4, since package 'wheel' is not installed.\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "    Running setup.py install for bs4: started\n",
      "    Running setup.py install for bs4: finished with status 'done'\n",
      "Successfully installed beautifulsoup4-4.11.1 bs4-0.0.1 soupsieve-2.3.2.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "whole-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported necessary modules\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "\n",
    "import os # module for navigating your machine (e.g., file directories)\n",
    "import requests # module for requesting urls\n",
    "import csv # module for handling csv files\n",
    "import pandas as pd # module for handling data\n",
    "from datetime import datetime # module for working with dates and time\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "print(\"Succesfully imported necessary modules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-contrary",
   "metadata": {},
   "source": [
    "Modules are additional techniques or functions that are not present when you launch Python. Some do not even come with Python when you download it and must be installed on your machine separately - think of using ssc install <package> in Stata, or install.packages(<package>) in R. For now just understand that many useful modules need to be imported every time you start a new Python session.\n",
    "\n",
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-vatican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL where the web page can be accessed\n",
    "\n",
    "url = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "# Request the web page from the URL\n",
    "\n",
    "response = requests.get(url, allow_redirects=True) # request the url\n",
    "# response.headers\n",
    "response.status_code # check if page was requested successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-collar",
   "metadata": {},
   "source": [
    "First, we define a variable (also known as an 'object' in Python) called url that contains the web address of the page we want to request. Next, we use the get() method of the requests module to request the web page, and in the same line of code, we store the results of the request in a variable called response. Finally, we check whether the request was successful by calling on the status_code attribute of the response variable.\n",
    "\n",
    "Confused? Don't worry, the conventions of Python and using its modules take a bit of getting used to. At this point, just understand that you can store the results of commands in variables, and a variable can have different attributes that can be accessed when needed. Also note that you have a lot of freedom in how you name your variables (subject to certain restrictions - see here for some guidance).\n",
    "\n",
    "For example, the following would also work:\n",
    "\n",
    "web_address = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "scrape_result = requests.get(web_address, allow_redirects=True)\n",
    "scrape_result.status_code\n",
    "Back to the request:\n",
    "\n",
    "Good, we get a status code of 200 - this means we successfully requested the web page. Lau, Gonzalez and Nolan provide a succinct description of different types of response status codes:\n",
    "\n",
    "100s - Informational: More input is expected from client or server (e.g. 100 Continue, 102 Processing)\n",
    "200s - Success: The client's request was successful (e.g. 200 OK, 202 Accepted)\n",
    "300s - Redirection: Requested URL is located elsewhere; May need user's further action (e.g. 300 Multiple Choices, 301 Moved Permanently)\n",
    "400s - Client Error: Client-side error (e.g. 400 Bad Request, 403 Forbidden, 404 Not Found)\n",
    "500s - Server Error: Server-side error or server is incapable of performing the request (e.g. 500 Internal Server Error, 503 Service Unavailable)\n",
    "For clarity:\n",
    "\n",
    "Client: your machine\n",
    "Server: the machine you are requesting the web page from\n",
    "You may be wondering exactly what it is we requested: if you were to type the URL (https://www.worldometers.info/coronavirus/) into your browser and hit enter, the web page should appear on your screen. This is not the case when we request the URL through Python but rest assured, we have successfully requested the web page. To see the content of our request, we can examine the text attribute of the response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "improving-light",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<!--[if IE 8]> <html lang=\"en\" class=\"ie8\"> <![endif]-->\\n<!--[if IE 9]> <html lang=\"en\" class=\"ie9\"> <![endif]-->\\n<!--[if !IE]><!-->\\n<html lang=\"en\">\\n<!--<![endif]-->\\n<head>\\n<meta charset=\"utf-8\">\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n<title>COVID Live - Coronavirus Statistics - Worldometer</title>\\n<meta name=\"description\" content=\"Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China. Coronavirus counter with new cases, deaths, and number of tests per 1 Million population. Historical data and info. Daily charts, graphs, news and updates\">\\n\\n<link rel=\"shortcut icon\" href=\"/favicon/favicon.ico\" type=\"image/x-icon\">\\n<link rel=\"apple-touch-icon\" sizes=\"57x57\" href=\"/favicon/apple-icon-57x57.png\">\\n<link rel=\"apple-touch-icon\" sizes=\"60x60\" href=\"/favicon/apple-icon-60x60.png\">'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-pattern",
   "metadata": {},
   "source": [
    "This shows us a sample of the underlying code (HTML) of the web page we requested. It should be obvious that in its current form, the result of this request will be difficult to work with. This is where the BeautifulSoup module comes in handy.\n",
    "\n",
    "(See Appendix A for more examples of how the requests module works and what information it returns.)\n",
    "\n",
    "Parsing the web page\n",
    "\n",
    "Now it's time to identify and understand the structure of the web page we requested. We do this by converting the content contained in the response.text attribute into a BeautifulSoup variable. BeautifulSoup is a Python module that provides a systematic way of navigating the elements of a web page and extracting its contents. Let's see how it works in practice:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "short-involvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<!--[if IE 8]> <html lang=\"en\" class=\"ie8\"> <![endif]-->\n",
       "<!--[if IE 9]> <html lang=\"en\" class=\"ie9\"> <![endif]-->\n",
       "<!--[if !IE]><!-->\n",
       "<html lang=\"en\">\n",
       "<!--<![endif]-->\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<title>COVID Live - Coronavirus Statistics - Worldometer</title>\n",
       "<meta content=\"Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China. Coronavirus counter with new cases, deaths, and number of tests per 1 Million population. Historical data and info. Daily charts, graphs, news and updates\" name=\"description\"/>\n",
       "<link href=\"/favicon/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n",
       "<link href=\"/favicon/apple-icon-57x57.png\" rel=\"apple-touch-icon\" sizes=\"57x57\"/>\n",
       "<link href=\"/favicon/apple-icon-60x60.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/></head></html>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the contents of the webpage from the response\n",
    "\n",
    "soup_response = soup(response.text, \"html.parser\") # Parse the text as a Beautiful Soup object\n",
    "soup_sample = soup(response.text[:1000], \"html.parser\") # Parse a sample of the text\n",
    "soup_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-graduation",
   "metadata": {},
   "source": [
    "BeautifulSoup has taken the unstructured text contained in response.text and parsed it as HTML: now we can clearly see the hierarchical structure and tags that comprise a web page's HTML.\n",
    "\n",
    "Note again how we call on a method (soup()) from a module (BeautifulSoup) and store the results in a variable (soup_response).\n",
    "\n",
    "Of course, we've only displayed a sample of the code here for readability. What about the full text contained in soup_response: how do we navigate such voluminous results? Thankfully the BeautifulSoup module provides some intuitive methods for doing so.\n",
    "\n",
    "TASK: view the full contents of the web page stored in the variable soup_response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dressed-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-matrix",
   "metadata": {},
   "source": [
    "Extracting information\n",
    "Now that we have parsed the web page, we can use Python to navigate and extract the information of interest. To begin with, let's locate the section of the web page containing the overall Covid-19 statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "undefined-advertising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
       " <h1>Coronavirus Cases:</h1>\n",
       " <div class=\"maincounter-number\">\n",
       " <span style=\"color:#aaa\">547,512,638 </span>\n",
       " </div>\n",
       " </div>,\n",
       " <div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
       " <h1>Deaths:</h1>\n",
       " <div class=\"maincounter-number\">\n",
       " <span>6,347,923</span>\n",
       " </div>\n",
       " </div>,\n",
       " <div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
       " <h1>Recovered:</h1>\n",
       " <div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
       " <span>522,845,791</span>\n",
       " </div>\n",
       " </div>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the sections containing the data of interest\n",
    "\n",
    "sections = soup_response.find_all(\"div\", id=\"maincounter-wrap\")\n",
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-norway",
   "metadata": {},
   "source": [
    "We used the find_all() method to search for all <div> tags where the id=\"maincounter-wrap\". And because there is more than one set of tags matching this id, we get a list of results. We can check how many tags match this id by calling on the len() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "killing-recommendation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-subsection",
   "metadata": {},
   "source": [
    "We can view each element in the list of results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "general-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
      "<h1>Coronavirus Cases:</h1>\n",
      "<div class=\"maincounter-number\">\n",
      "<span style=\"color:#aaa\">547,512,638 </span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n",
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
      "<h1>Deaths:</h1>\n",
      "<div class=\"maincounter-number\">\n",
      "<span>6,347,923</span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n",
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
      "<h1>Recovered:</h1>\n",
      "<div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
      "<span>522,845,791</span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "for section in sections:\n",
    "    print(\"--------\")\n",
    "    print(section)\n",
    "    print(\"--------\")\n",
    "    print(\"\\r\") # print some blank space for better formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-retrieval",
   "metadata": {},
   "source": [
    "We are nearing the end of our scrape. The penultimate task is to extract the statistics within the <span> tags and store them in some variables. We do this by accessing each item in the sections list using its positional value (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial-melissa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 547512638; deaths: 6347923; and recoveries: 522845791.\n"
     ]
    }
   ],
   "source": [
    "cases = sections[0].find(\"span\").text.replace(\" \", \"\").replace(\",\", \"\")\n",
    "deaths = sections[1].find(\"span\").text.replace(\",\", \"\")\n",
    "recov = sections[2].find(\"span\").text.replace(\",\", \"\")\n",
    "print(\"Number of cases: {}; deaths: {}; and recoveries: {}.\".format(cases, deaths, recov))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-apartment",
   "metadata": {},
   "source": [
    "The above code performs a couple of operations:\n",
    "\n",
    "For each item (i.e., set of <div> tags) in the list, it finds the <span> tags and extracts the text enclosed within them.\n",
    "We clean the text by removing blank spaces and commas.\n",
    "In this example, referring to an item's positional index works because our list of <div> tags stored in the sections variable is ordered: the tag containing the number of cases appears before the tag containing the number of deaths, which appears before the tag containing the number of recovered patients.\n",
    "\n",
    "In Python, indexing begins at zero (in R indexing begins at 1). Therefore, the first item in the list is accessed using sections[0], the second using sections[1] etc.\n",
    "\n",
    "(To learn more about lists in Python, see Chapter 22 of *How to Code in Python 3*)\n",
    "\n",
    "Saving results from the scrape\n",
    "The final task is to save the variables to a file that we can use in the future. We'll write to a Comma-Separated Values (CSV) file for this purpose, as it is an open-source, text-based file format that is commonly used for sharing data on the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diagnostic-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a downloads folder\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except:\n",
    "    print(\"Unable to create folder: already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-preservation",
   "metadata": {},
   "source": [
    "The use of \"./\" tells the os.mkdir() command that the \"downloads\" folder should be created at the same level of the directory where this notebook is located. So if this notebook was stored in a directory located at \"C:/Users/joebloggs/notebooks\", the os.mkdir() command would result in a new folder located at \"C:/Users/joebloggs/notebooks/downloads\".\n",
    "\n",
    "(Technically the \"./\" is not needed and you could just write os.mkdir(\"downloads\") but it's good practice to be explicit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "saving-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-24\n"
     ]
    }
   ],
   "source": [
    "# Write the results to a CSV file\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\") # get today's date in YYYY-MM-DD format\n",
    "print(date)\n",
    "\n",
    "variables = [\"Cases\", \"Deaths\", \"Recoveries\"] # define variable names for the file\n",
    "outfile = \"./downloads/covid-19-statistics-\" + date + \".csv\" # define a file for writing the results\n",
    "obs = cases, deaths, recov # define an observation (row)\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(variables) # write the variable names to the first row of the file\n",
    "    writer.writerow(obs) # write the observation to the next row in the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-terry",
   "metadata": {},
   "source": [
    "The code above defines some headers and a name and location for the file which will store the results of the scrape. We then open the file in write mode, and write the headers to the first row, and the statistics to subsequent rows.\n",
    "\n",
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "outer-wildlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases,Deaths,Recoveries\n",
      "547512638,6347923,522845791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check presence of file in \"downloads\" folder\n",
    "\n",
    "os.listdir(\"./downloads\")\n",
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(outfile, \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/downloading-pdfs-with-python-using-requests-and-beautifulsoup/\n",
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "  \n",
    "# URL from which pdfs to be downloaded\n",
    "url = \"https://www.geeksforgeeks.org/how-to-extract-pdf-tables-in-python/\"\n",
    "  \n",
    "# Requests URL and get response object\n",
    "response = requests.get(url)\n",
    "  \n",
    "# Parse text obtained\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  \n",
    "# Find all hyperlinks present on webpage\n",
    "links = soup.find_all('a')\n",
    "  \n",
    "i = 0\n",
    "  \n",
    "# From all links check for pdf link and\n",
    "# if present download file\n",
    "for link in links:\n",
    "    if ('.pdf' in link.get('href', [])):\n",
    "        i += 1\n",
    "        print(\"Downloading file: \", i)\n",
    "  \n",
    "        # Get response object for link\n",
    "        response = requests.get(link.get('href'))\n",
    "  \n",
    "        # Write content in pdf file\n",
    "        pdf = open(\"pdf\"+str(i)+\".pdf\", 'wb')\n",
    "        pdf.write(response.content)\n",
    "        pdf.close()\n",
    "        print(\"File \", i, \" downloaded\")\n",
    "  \n",
    "print(\"All PDF files downloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
