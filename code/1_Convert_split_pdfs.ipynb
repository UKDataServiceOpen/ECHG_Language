{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "positive-implementation",
   "metadata": {},
   "source": [
    "# Converting raw .pdf files to .txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbbb47",
   "metadata": {},
   "source": [
    "This .ipynb contains all of the .pdf-to-.txt work done by JK in the initial run in 2022. Although this process was broadly successful, JK felt that a more professional team could do a lot better job of it in much less time than would be needed for JK to do a much better job. This code and logic is preserved mostly for transparency sake and for potential re-use by JK or others for future projects. \n",
    "\n",
    "## 1 - Install and import necessary things for all of the processes in this .ipynb\n",
    "\n",
    "Start off by installing the PyPDF2 module (if you don't already have it installed) and importing that module so that it can be used in this notebook. I did this in two separate code chunks, one for installing and one for importing, because I wanted to keep the output manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eligible-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# installing necessary pdf conversion package via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install PyPDF2       \n",
    "!pip install pdfplumber\n",
    "!pip install regex\n",
    "\n",
    "# importing required modules (first for displaying screenshots, second for converting pdfs)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "import PyPDF2 \n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import pdfplumber\n",
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import regex\n",
    "import re\n",
    "\n",
    "date = datetime.date.today()    # This is a way to make sure that work done on one day does not overwrite previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44de262",
   "metadata": {},
   "source": [
    "## 2 - Define the functions used in this for all of the processes in this .ipynb\n",
    "\n",
    "First, define a function that takes a .pdf filename and two page numbers and returns a smaller .pdf that only contains the pages between the provided page numbers. \n",
    "\n",
    "This code is adapted slightly from the original here -> https://gist.github.com/khanfarhan10/464d44086327369953327a7320716100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06804e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_split(fname, start, end=None):\n",
    "    print('pdf_split', fname, start, end)\n",
    "\n",
    "    inputpdf = PdfReader(open(fname, \"rb\"))\n",
    "    output = PdfWriter()\n",
    "\n",
    "    # turn 1,4 to 0,3\n",
    "    num_pages = len(inputpdf.pages)\n",
    "    if start:\n",
    "        start-=1\n",
    "    if not start:\n",
    "        start=0\n",
    "    if not end or end > num_pages:\n",
    "        end=num_pages\n",
    "\n",
    "    get_pages = list(range(start,end))\n",
    "    #print('get_pages', get_pages, 'of', num_pages)\n",
    "    # get_pages [0, 1, 2, 3]\n",
    "\n",
    "    for i in range(start,end):\n",
    "        if i < start:\n",
    "            continue\n",
    "        #output = PdfFileWriter()\n",
    "        output.add_page(inputpdf.pages[i])\n",
    "\n",
    "    fname_no_pdf = row[0]\n",
    "    if row[0][:-4].lower() == '.pdf':\n",
    "        fname_no_pdf = row[0][:-4]\n",
    "    out_filename = f\"{outfolder + fname_no_pdf}\"\n",
    "    with open(out_filename, \"wb\") as outputStream:\n",
    "        output.write(outputStream)\n",
    "    print('saved', out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de97aca",
   "metadata": {},
   "source": [
    "Then, define the function that opens files and converts the contents by various steps. \n",
    "\n",
    "Specifically, this function opens each of the files in the folder given as an argument and for each one removes the '.pdf' suffix from the file name, removes the 'ESHG' bit of the filename, finds how many pages there are in the file, opens each page, extracts the text, appends that text to a string and then writes that string out to a new .txt file with the name (the short version, without '.pdf' or 'ESHG' in it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75936532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdfs(input,output):\n",
    "    lines = []\n",
    "    for filename in os.listdir(input):\n",
    "        pdf_contents = \"\"\n",
    "        with pdfplumber.open(input + \"\\\\\" + filename) as pdf:\n",
    "            name = filename.replace(r'.pdf', \"\")\n",
    "            name = name.replace(r'ESHG', \"\")\n",
    "            pages = pdf.pages\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                pdf_contents = pdf_contents + text\n",
    "            with open(output + \"\\\\\" + name + \".txt\", \"w\", newline='', encoding='utf-8') as f:\n",
    "                pdf_contents = \" \".join(pdf_contents.split())\n",
    "                f.write(pdf_contents)\n",
    "                f.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43f54b",
   "metadata": {},
   "source": [
    "## 3 - Split the .pdfs \n",
    "\n",
    "###  Run a splitting .pdf test\n",
    "\n",
    "The next 3 code cells are option if you want a low-stakes test or walkthrough of the process which has 3 parts. \n",
    "* 1 - Check the contents of the test folder. \n",
    "* 2 - Create a list of lists that holds the arguments needed to run the function. The start and end numbers have no real significance, they are just numbers used to test. Importantly, they are within the bounds of the .pdf which requires you to know how many pages are in it. \n",
    "* 3 - A for-loop runs the function on each item in the defined list of list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa995d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_pdf_1.pdf', 'input_pdf_2.pdf', 'input_pdf_3.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\\\\raw_pdfs\\\\Test\") # This is how to see the contents of any folders shown in the last contents check\n",
    "                            # For example, 'input_pdfs' which is likely to contain things we want to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476df634",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_split_test = [[\"input_pdf_1.pdf\", 0, 0],\n",
    " [\"input_pdf_2.pdf\", 1, 0],\n",
    " [\"input_pdf_3.pdf\", 2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d34e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_split ..\\raw_pdfs\\Test\\input_pdf_1.pdf 0 0\n",
      "saved ..\\input_pdfs\\Test\\input_pdf_1.pdf\n",
      "pdf_split ..\\raw_pdfs\\Test\\input_pdf_2.pdf 1 0\n",
      "saved ..\\input_pdfs\\Test\\input_pdf_2.pdf\n",
      "pdf_split ..\\raw_pdfs\\Test\\input_pdf_3.pdf 2 5\n",
      "saved ..\\input_pdfs\\Test\\input_pdf_3.pdf\n"
     ]
    }
   ],
   "source": [
    "for row in to_split_test:\n",
    "    folder = \"..\\\\raw_pdfs\\\\Test\\\\\"\n",
    "    outfolder =  \"..\\\\input_pdfs\\\\Test\\\\\"\n",
    "    fname = folder + row[0]\n",
    "    pdf_split(fname, row[1], row[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9189924",
   "metadata": {},
   "source": [
    "The output of the for-loop suggests it all went well, but you may want to inspect the files to double check every thing went as you intended. \n",
    "\n",
    "### Run the splitting process on the files of interest\n",
    "\n",
    "Assuming there were no major problems, the process is repeated for the files of interest:\n",
    "\n",
    "* check the contents of the relevant folder,\n",
    "* define a list of lists to hold the function arguments, and\n",
    "* run the for-loop to apply the function to the defined list of list. \n",
    "\n",
    "As before, defining the start and end numbers is a bit of a manual process. I had to open the files, identify the first page that I wanted to keep and the last page that I wanted to keep (paying special attention to the actual .pdf page number rather than the page numbers within the document). \n",
    "\n",
    "I also strongly recommend a lot of manual checking to be sure it came out correctly. I had to edit the list a couple of time and re-run the code to get everything correct. In the process of manually checking the ESHG files, I noticed that only the pre-2017 files needed to be trimmed. The more recent files do not contain cover pages, indices, adverts, etc. Winning!\n",
    "\n",
    "Unfortunately, the raw .pdfs were too voluminous for github to host, so the next three code cells do notwork. You will have to create/edit the folders and files as needed for your actual project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea39cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\..\\\\PIFL\\\\raw_pdfs\\\\ESHG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mPIFL\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mraw_pdfs\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mESHG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This is how to see the contents of any folders shown in the last contents check\u001b[39;00m\n\u001b[0;32m      2\u001b[0m                             \u001b[38;5;66;03m# For example, 'input_pdfs' which is likely to contain things we want to import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\..\\\\PIFL\\\\raw_pdfs\\\\ESHG'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"..\\\\..\\\\PIFL\\\\raw_pdfs\\\\ESHG\") # This is how to see the contents of any folders shown in the last contents check\n",
    "                            # For example, 'input_pdfs' which is likely to contain things we want to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc175279",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_split = [[\"ESHG2001abstractICHG.pdf\", 64, 434],\n",
    " [\"ESHG2002Abstracts.pdf\", 54, 327],\n",
    " [\"ESHG2003Abstracts.pdf\", 44, 261],\n",
    " [\"ESHG2004Abstracts.pdf\", 58, 373],\n",
    " [\"ESHG2005Abstracts.pdf\", 55, 388],\n",
    " [\"ESHG2006Abstracts.pdf\", 74, 410],\n",
    " [\"ESHG2007Abstracts.pdf\", 5, 351],\n",
    " [\"ESHG2008Abstracts.pdf\", 5, 469],\n",
    " [\"ESHG2009Abstracts.pdf\", 6, 401],\n",
    " [\"ESHG2010Abstracts.pdf\", 6, 400],\n",
    " [\"ESHG2011Abstracts.pdf\", 5, 484],\n",
    " [\"ESHG2012Abstracts.pdf\", 6, 438],\n",
    " [\"ESHG2013Abstracts.pdf\", 6, 611],\n",
    " [\"ESHG2014Abstracts.pdf\", 6, 518],\n",
    " [\"ESHG2015Abstracts.pdf\", 6, 485],\n",
    " [\"ESHG2016Abstracts.pdf\", 6, 506]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in to_split:\n",
    "    folder = \"..\\\\..\\\\PIFL\\\\raw_pdfs\\\\ESHG\\\\\"\n",
    "    outfolder =  \"..\\\\input_pdfs\\\\ESHG\\\\\"\n",
    "    fname = folder + row[0]\n",
    "    pdf_split(fname, row[1], row[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-charity",
   "metadata": {},
   "source": [
    "If you get the above code to work, you may notice an error message. I got the error message (PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected.) after the 2004 file. \n",
    "\n",
    "Online advice suggests this is an encoding error coming from how the .pdf file was created that let's it be read but not handled in the standard way. The solution seems to be to open it in Adobe and save it under a new file name, but that did not work for me. \n",
    "\n",
    "## 4 - Create and/or check the input .pdf\n",
    "\n",
    "I first created 2 test files in word and then printed each of them off to .pdf. I specifically put a few key features into these files to test how the text would be converted, such as a heading, multiple blank lines between text, an image with a caption, multiple pages, and two column text. \n",
    "\n",
    "I then saved these new .pdfs (and screenshots of both) into the same location as my .ipynb so that I can \n",
    "* 1 - paste in the images to show how the .pdfs looked to begin with (see next cell) and\n",
    "* 2 - import the .pdfs so that PyPDF2 can convert them (see cell after next). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-latino",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<table><tr><td><img src='..\\images\\Input_pdf_image_1.png'></td><td><img src='..\\images\\Input_pdf_image_2.png'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-ready",
   "metadata": {},
   "source": [
    "The following code cell tells you all the files in a particular folder (in this case, the raw_pdfs\\Test folder). \n",
    "\n",
    "Running it shows there are the two test .pdfs shown above as well as a third .pdf that I created later with many more pages (I created it to test the split_pdf function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"..\\\\raw_pdfs\\\\Test\") # This is how to see the contents of any folders shown in the last contents check\n",
    "                            # For example, 'input_pdfs' which is likely to contain things we want to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating .pdf file objects from existing .pdfs in the same folder as this .ipynb code\n",
    "# this ipmorts the .pdfs and creates accessible objects for the PyPDF2 module to work with\n",
    "\n",
    "pdfFileObj_1 = open('..\\input_pdfs\\Test\\input_pdf_1.pdf', 'rb') \n",
    "pdfFileObj_2 = open('..\\input_pdfs\\Test\\input_pdf_2.pdf', 'rb') \n",
    "pdfFileObj_3 = open('..\\input_pdfs\\Test\\input_pdf_3.pdf', 'rb') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-sierra",
   "metadata": {},
   "source": [
    "# 3 - Convert and check the .pdf objects\n",
    "\n",
    "The first step is to create a PyPDF2 oject from the imported .pdf. This allows you to do things like:\n",
    "* check how many pages are in the original .pdf, \n",
    "* convert some or all of those pages to page objects, and\n",
    "* then extract the text from those page ojbects (optionally saving the text for later analysis). \n",
    "\n",
    "Of course, good coding etiquette suggests you should always close any opened files when you are done with them. \n",
    "\n",
    "\n",
    "PyPDF2 is not the only option for converting .pdf files, and in the convert_multiple_pdfs notebook I use pdfplumber instead. But still, PyPDF2 is straightforward and a good way to get to grip with the basic steps and to see what a converted .pdf would look like as a string of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating .pdf reader objects, which the module will use for the actual conversion work\n",
    "pdfReader_1 = PyPDF2.PdfFileReader(pdfFileObj_1) \n",
    "pdfReader_2 = PyPDF2.PdfFileReader(pdfFileObj_2) \n",
    "pdfReader_3 = PyPDF2.PdfFileReader(pdfFileObj_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing number of pages in each .pdf file \n",
    "print(pdfReader_1.numPages) \n",
    "print(pdfReader_2.numPages) \n",
    "print(pdfReader_3.numPages) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-metallic",
   "metadata": {},
   "source": [
    "So far so good. We have the .pdfs imported, converted to module specific objects, and the module has correctly identified the number of pages in each. But really, we need to know how well the module can recognise the text within those objects cause I deliberately made that text a bit tricky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating .pdf file objects from existing .pdfs in the same folder as this .ipynb code\n",
    "# this ipmorts the .pdfs and creates accessible objects for the PyPDF2 module to work with\n",
    "\n",
    "pdfFileObj_1 = open('..\\input_pdfs\\Test\\input_pdf_1.pdf', 'rb') \n",
    "pdfFileObj_2 = open('..\\input_pdfs\\Test\\input_pdf_2.pdf', 'rb') \n",
    "pdfFileObj_3 = open('..\\input_pdfs\\Test\\input_pdf_2.pdf', 'rb') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-ranch",
   "metadata": {},
   "source": [
    "# 4 - Get individual pages, extract text, save it as strings, etc. \n",
    "\n",
    "Now, we get down to the real work. We want to  \n",
    "* convert some or all of those pages to page objects, and\n",
    "* then extract the text from those page ojbects, \n",
    "* (optionally) save that text as string objects for later analysis, and \n",
    "* tidy up (good coding etiquette suggests you should always close any opened files when you are done with them) . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating page objects for each page\n",
    "# something to note here - the pages start counting from 0 so you get page 1 of our first test .pdf\n",
    "#                          by asking getPage to getPage(0). \n",
    "#                          In turn, when we want to get both pages from the second test .pdf, we ask for \n",
    "#                          getPage(0) and also getPage(1)\n",
    "pageObj_1_1 = pdfReader_1.getPage(0) \n",
    "pageObj_2_1 = pdfReader_2.getPage(0)\n",
    "pageObj_2_2 = pdfReader_2.getPage(1) \n",
    "pageObj_3_3 = pdfReader_3.getPage(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from page to print on screen\n",
    "print(pageObj_2_1.extractText()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from page to print on screen\n",
    "print(pageObj_2_2.extractText()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-terrorism",
   "metadata": {},
   "source": [
    "So, good news. This .pdf conversion module has successfully recognised that input_pdf_2 was structured in two columns and it has converted the text appropriately (ish) with the text flowing properly from the end of one line in a column to the start of the next line *in the same column* rather than reading on to the equivalent line in the next column. \n",
    "\n",
    "It might be better if the lines were not cut short to replicate the actual number of words in each column as they appear in the text, but that is a step for later on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from page to save for later use, in this case as a string object\n",
    "test_file_1 = (pageObj_1_1.extractText()) \n",
    "type(test_file_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-acquisition",
   "metadata": {},
   "source": [
    "# 5 - Converting a single file to inspect it for problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-contents",
   "metadata": {},
   "source": [
    "## Inspect the problem file\n",
    "In the split_pdfs notebook, we got an error message with the 2004 file. Inspecting it, it seemed fine - the cover pages, indices, adverts, etc. were cut off leaving only the pages with abstracts remaining. \n",
    "\n",
    "However, proceeding through the other notebooks in order (convert_multiple_pdfs, preliminary_clean, and analysis) it seemed like there were problems with the 2004 file. It seems that the encoding issues ran a bit deeper, so I had to go back and convert that file again, with some extra steps to deal with the encoding issues. \n",
    "\n",
    "As above with the test files, we start by loading a reader object from the file and check how many pages it has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2004 = PyPDF2.PdfFileReader('..\\input_pdfs\\ESHG\\ESHG2004abstracts.pdf')\n",
    "print(pdf2004.numPages) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-diesel",
   "metadata": {},
   "source": [
    "Then, we take a look at a few pages. After playing around for a bit, I found a page that shows the encoding problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_50 = pdf2004.getPage(50) \n",
    "print(page_50.extractText()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-contamination",
   "metadata": {},
   "source": [
    "I tried with pdfplumber too, just to check whether the encoding problem could be solved by importing and extracting the text in a different way. \n",
    "\n",
    "\n",
    "Scroll down to see the problem. Before too long, all the text starts to apear as a sequence in the following format:\n",
    "\n",
    "(cid:73)(cid:68)(cid:87)(cid:68)(cid:86)(cid:72)(cid:3)(cid:39)(cid:72)(cid:191)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)(cid:29)(cid:3)(cid:48)(cid:82)(cid:79)(cid:72)(cid:70)(cid:88)(cid:79)(cid:68)(cid:85)(cid:3)(cid:71)(cid:72)(cid:73)(cid:72)(cid:70)(cid:87)(cid:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open('..\\input_pdfs\\ESHG\\ESHG2004abstracts.pdf') as pdf:\n",
    "        pages = pdf.pages\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-gregory",
   "metadata": {},
   "source": [
    "I don't include it here, but there is some code at the end of the convert_pdf_multiple notebook that corrects the problems with this file. I hope it corrects the problems anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435af2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Define the conversion function \n",
    "\n",
    "This bit of code does a fair bit. It opens each of the files in the folder given as an argument and for each one removes the '.pdf' suffix from the file name, removes the 'ESHG' bit of the filename, finds how many pages there are in the file, opens each page, extracts the text, appends that text to a string and then writes that string out to a new .txt file with the name (the short version, without '.pdf' or 'ESHG' in it). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b774f",
   "metadata": {},
   "source": [
    "# 7 - Run on Test folder\n",
    "\n",
    "Let's just test it on the Test folder to make sure it all goes to plan. It will remove the '.pdf' from the names but looking for and replacing 'ESHG' will have no effect. \n",
    "\n",
    "Be sure to look in the output_texts folder, open and inspect a file or two to make sure it worked as expected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"..\\input_pdfs\\Test\") # This is how to see the contents of any folders shown in the last contents check\n",
    "                            # For example, 'input_pdfs' which is likely to contain things we want to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f89a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_pdfs('..\\input_pdfs\\Test', '..\\output_texts\\Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8d4d7",
   "metadata": {},
   "source": [
    "# 8 - Run on folder of interest \n",
    "\n",
    "Now let's do the same for the target folder. This time, the name shortening lines in the function will take full effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"..\\input_pdfs\\ESHG\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ca56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_pdfs('..\\input_pdfs\\ESHG', '..\\output_texts\\ESHG')       # This takes a long time to run. At least it did for me.\n",
    "                                                                 # Go do something elso for a while or \n",
    "                                                                 # do this last thing before you go in the evening. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff756596",
   "metadata": {},
   "source": [
    "If you remember correctly, the 2004 file threw up an error when we were splitting it to chop off the irrelevant bits. \n",
    "\n",
    "It looks fine when viewed in a .pdf reader but I noticed that running the preliminary clean and analysis notebooks showed that it was not behaving like the other files. I double checked it using the code in the convert_pdf_single notebook and saw that it started off looking normal but quickly went very squiffy. It seems that particular .txt file is full of squiffy text due to some weird encoding. \n",
    "\n",
    "As such, we need to do a bit of work to recode the 2004 file, giving it a unique name to distinguish it from the file converted by the previous code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cidToChar(cidx):\n",
    "    return chr(int(re.findall(r'\\(cid\\:(\\d+)\\)',cidx)[0]) + 29)\n",
    "\n",
    "with open('..\\\\output_texts\\\\ESHG\\\\2004Abstracts.txt', \"r\", newline='', encoding='utf-8') as file:\n",
    "    for item in file:\n",
    "        abc = re.findall(r'\\(cid\\:\\d+\\)',item)\n",
    "        if len(abc) > 0:\n",
    "            for cid in abc: item=item.replace(cid, cidToChar(cid))\n",
    "        output = repr(item).strip(\"'\")\n",
    "        with open('..\\\\output_texts\\\\ESHG\\\\2004.txt', \"w\", newline='', encoding='utf-8') as f:\n",
    "            f.write(output)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27b4d4",
   "metadata": {},
   "source": [
    "Aaaaaaaaaaaaand, let's just have a look at the new (and uniquely named) recoded file to see what it looks like. At this point, you may want to save the original (and squiffy) file somewhere else and only continue working with the better encoded 2004.txt file. Or you can just delete the original one as you could always get it back again by re-running the conversion process on all the files (or just on the one file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b19cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\ESHG\\\\2004.txt','r', encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
