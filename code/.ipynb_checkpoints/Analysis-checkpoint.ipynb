{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rolled-channels",
   "metadata": {},
   "source": [
    "# Import and get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-terrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\python39\\lib\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported necessary modules\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect          \n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize    \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "\n",
    "import numpy as np\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)  \n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\ESHG\")) # This is how to see the contents of any folders shown in the last contents check\n",
    "print(os.listdir(\"..\\\\counts\\\\ESHG\")) # This is how to see the contents of any folders shown in the last contents check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-content",
   "metadata": {},
   "source": [
    "# Read in and check useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = pd.read_csv('..\\\\counts\\\\ESHG\\\\totals.csv', header=None)\n",
    "totals_df.columns = [\"filename\", \"abstracts\"]\n",
    "totals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "powerful-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toascii():\n",
    "    with open(r'C:\\log.convert', 'r', encoding='utf8') as origfile, open(r'C:\\log.toascii', 'w', encoding='ascii') as convertfile:\n",
    "        for line in origfile:\n",
    "            line = unidecode(line)\n",
    "            convertfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-brazil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-columbia",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comic-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_select_abstracts = []\n",
    "\n",
    "with open('..\\\\counts\\\\ESHG\\\\select.csv', newline='', encoding = \"ISO-8859-1\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        raw_select_abstracts.append(row)\n",
    "        \n",
    "select_abstracts = (list(filter(lambda x: x, raw_select_abstracts)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = pd.DataFrame.from_records(select_abstracts)\n",
    "total_select = select_df.iloc[:,0].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "autism_count = select_df.iloc[:,0].value_counts().rename_axis('unique_values').to_frame('counts')\n",
    "\n",
    "autism_count = autism_count.reset_index()\n",
    "autism_count.columns = ['filename', 'autism_abstracts']\n",
    "\n",
    "autism_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts = pd.merge(totals_df, autism_count, on = 'filename')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts['year'] = merged_counts['filename'].str.extract(r'(\\d{4})')\n",
    "merged_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_counts['abstracts'].sum())\n",
    "print(merged_counts['autism_abstracts'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-closure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_counts[['year', 'abstracts']].groupby('year').sum('abstracts'))\n",
    "#print(merged_counts['autism_abstracts'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-picking",
   "metadata": {},
   "source": [
    "# Working with the contents of the selected abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-identifier",
   "metadata": {},
   "source": [
    "##  Dump the content of the selected abtsracts into one big string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "naughty-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_abstracts = \"\"\n",
    "\n",
    "for abstract in (select_abstracts):\n",
    "    bag_of_abstracts += abstract[3]\n",
    "        \n",
    "type(bag_of_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-seattle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P08 .37 Ho'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_abstracts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-shield",
   "metadata": {},
   "source": [
    "## tokenize that big string, and transform the tokens into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educational-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '.37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbao-basurto', 'â\\x80\\x93', 'osakidetza', ',', 'bilbao']\n"
     ]
    }
   ],
   "source": [
    "abstract_token_word = word_tokenize(bag_of_abstracts)\n",
    "abstract_token_word_lower = [word.lower() for word in abstract_token_word]\n",
    "\n",
    "print(abstract_token_word_lower[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-damage",
   "metadata": {},
   "source": [
    "## Remove the punctuation from the lowercased tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alleged-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza', '', 'bilbao']\n"
     ]
    }
   ],
   "source": [
    "abstract_t_w_l_np = [w.translate(table_punctuation) for w in abstract_token_word_lower]  \n",
    "                                                               # Iterate over corpus_words, turning punctuation to nothing.\n",
    "print(abstract_t_w_l_np[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bigger-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '.37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbao-basurto', 'â\\x80\\x93', 'osakidetza', ',', 'bilbao']\n"
     ]
    }
   ],
   "source": [
    "for token in abstract_token_word_lower:\n",
    "    token = token.replace('â\\x80\\x93', 'â')\n",
    "    \n",
    "    \n",
    "print(abstract_token_word_lower[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "important-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza', 'bilbao', 'spain']\n"
     ]
    }
   ],
   "source": [
    "abstract_t_w_l_np = (list(filter(lambda x: x, abstract_t_w_l_np)))\n",
    "print(abstract_t_w_l_np[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-tract",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stop_words))        # just an option to check what counts as a stopword if you want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "orange-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza', 'bilbao', 'spain']\n"
     ]
    }
   ],
   "source": [
    "abstract_t_w_l_np_nsw = []\n",
    "\n",
    "for word in abstract_t_w_l_np:\n",
    "    if word not in stop_words:\n",
    "        abstract_t_w_l_np_nsw.append(word)\n",
    "        \n",
    "        \n",
    "print(abstract_t_w_l_np_nsw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-minimum",
   "metadata": {},
   "source": [
    "# Remove weird things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "impossible-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['p08', '37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza',]\n",
    "clean_test =[]\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str, output_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    print(''.join(c if c <= '\\xff' else unidecode(c) for c in nfkd_form))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "southern-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['p08', '37', 'hospital', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza',]\n",
    "clean_test =[]\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str, output_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    print( u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)]))\n",
    "\n",
    "''.join(c if c <= '\\xff' else unidecode(c) for c in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "theoretical-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p08\n",
      "37\n",
      "hospital\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unidecode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-778c39654baa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mremove_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-2bf3201bbf66>\u001b[0m in \u001b[0;36mremove_accents\u001b[1;34m(input_str, output_str)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnfkd_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NFKD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;34m'\\xff'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnfkd_form\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-2bf3201bbf66>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnfkd_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NFKD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;34m'\\xff'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnfkd_form\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'unidecode' is not defined"
     ]
    }
   ],
   "source": [
    "for item in test:\n",
    "    remove_accents(item, clean_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-blind",
   "metadata": {},
   "source": [
    "## Spell check the lowercased \"real word\" tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "agricultural-bahamas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362901"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstract_t_w_l_np_nsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "based-consistency",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p08',\n",
       " '37',\n",
       " 'hospital',\n",
       " 'â\\x80\\x93',\n",
       " 'osi',\n",
       " 'bilbaobasurto',\n",
       " 'â\\x80\\x93',\n",
       " 'osakidetza',\n",
       " 'biblio',\n",
       " 'spain',\n",
       " 'recurrent',\n",
       " 'de',\n",
       " 'novo',\n",
       " 'mutation',\n",
       " 'second',\n",
       " 'variant',\n",
       " '3genetics',\n",
       " 'unit',\n",
       " 'basurto',\n",
       " 'university',\n",
       " 'hospital',\n",
       " 'â\\x80\\x93',\n",
       " 'osakidetza',\n",
       " 'unknown',\n",
       " 'signï¬\\x81dance',\n",
       " 'swim6',\n",
       " 'boy',\n",
       " 'severe',\n",
       " 'biblio',\n",
       " 'spain',\n",
       " '4neuropediatrics',\n",
       " 'unit',\n",
       " 'basurto',\n",
       " 'university',\n",
       " 'intellectual',\n",
       " 'disability',\n",
       " 'microcephaly',\n",
       " 'strabism',\n",
       " 'hospital',\n",
       " 'â\\x80\\x93',\n",
       " 'osakidetza',\n",
       " 'biblio',\n",
       " 'spain',\n",
       " '5molecular',\n",
       " 'genetics',\n",
       " 'hyperopia',\n",
       " 'laboratory',\n",
       " 'genetics',\n",
       " 'service',\n",
       " 'biocruces',\n",
       " 'health',\n",
       " 'research',\n",
       " 'institute',\n",
       " 'crucesuniversityhospitalâ\\x80\\x93osakidetza',\n",
       " 'barakaldo',\n",
       " 'leader1',\n",
       " 'gamer2',\n",
       " 'praised1',\n",
       " 'spell1',\n",
       " 'may1',\n",
       " 'spain',\n",
       " '6molecular',\n",
       " 'epi',\n",
       " 'genetics',\n",
       " 'laboratory',\n",
       " 'bioaraba',\n",
       " 'national',\n",
       " 'health',\n",
       " 'institute',\n",
       " 'arab',\n",
       " 'university',\n",
       " 'hospital',\n",
       " 'â\\x80\\x93',\n",
       " '1division',\n",
       " 'clinical',\n",
       " 'genetics',\n",
       " 'department',\n",
       " 'pediatrics',\n",
       " 'osakidetza',\n",
       " 'victoria',\n",
       " 'spain',\n",
       " 'salzburg',\n",
       " 'landeskliniken',\n",
       " 'talk',\n",
       " 'salzburg',\n",
       " 'austria',\n",
       " '2departmentofpediatrics',\n",
       " 'salzburgerlandeskliniken',\n",
       " 'talk',\n",
       " 'introduction',\n",
       " '2012',\n",
       " 'new',\n",
       " 'microdeletion',\n",
       " 'syndrome',\n",
       " 'salzburg',\n",
       " 'austria',\n",
       " 'affecting',\n",
       " 'short',\n",
       " 'arm',\n",
       " 'chromosome',\n",
       " '10',\n",
       " '10p153',\n",
       " 'described',\n",
       " 'implication',\n",
       " 'mind11',\n",
       " 'gene',\n",
       " 'located',\n",
       " 'recently',\n",
       " 'arecurrentdenovononsensevariant',\n",
       " 'c2737c',\n",
       " 'within',\n",
       " 'region',\n",
       " 'phenotype',\n",
       " 'essentially',\n",
       " 'character',\n",
       " 'part913ter',\n",
       " 'penultimate',\n",
       " 'exon',\n",
       " 'swim6',\n",
       " 'sized',\n",
       " 'developmental',\n",
       " 'motor',\n",
       " 'delay',\n",
       " 'craniofacial',\n",
       " 'days',\n",
       " 'reported',\n",
       " 'cause',\n",
       " 'intellectual',\n",
       " 'disability',\n",
       " 'additional',\n",
       " 'cen',\n",
       " 'morphism',\n",
       " 'hypotonia',\n",
       " 'also',\n",
       " 'hypothesized',\n",
       " 'trial',\n",
       " 'peripheral',\n",
       " 'nervous',\n",
       " 'system',\n",
       " 'symptoms',\n",
       " 'palmer',\n",
       " 'patientandmethods',\n",
       " 'wepresentthecaseofa8month',\n",
       " 'etal',\n",
       " 'amjhumgenet2017dec7',\n",
       " '101',\n",
       " '6',\n",
       " '9951005',\n",
       " 'male',\n",
       " 'referred',\n",
       " 'consultation',\n",
       " 'due',\n",
       " 'developmental',\n",
       " 'frontonasal',\n",
       " 'limb',\n",
       " 'malformations',\n",
       " 'delay',\n",
       " 'facial',\n",
       " 'dysmorphisms',\n",
       " 'second',\n",
       " 'child',\n",
       " 'phenotype',\n",
       " 'initially',\n",
       " 'discovered',\n",
       " 'recurrent',\n",
       " 'misses',\n",
       " 'nonconsanguineous',\n",
       " 'couple',\n",
       " 'born',\n",
       " 'uneventful',\n",
       " 'mutation',\n",
       " 'c3487c',\n",
       " 'part1163trp',\n",
       " 'bysmithetal',\n",
       " 'pregnancy',\n",
       " 'normal',\n",
       " 'prenatal',\n",
       " 'period',\n",
       " 'elder',\n",
       " 'j',\n",
       " 'hum',\n",
       " 'gene',\n",
       " '2014',\n",
       " 'aug',\n",
       " '7',\n",
       " '95',\n",
       " '2',\n",
       " '23540',\n",
       " 'present',\n",
       " 'brother',\n",
       " 'presents',\n",
       " 'autism',\n",
       " 'without',\n",
       " 'specï¬\\x81c',\n",
       " 'phenotypic',\n",
       " 'phenotype',\n",
       " 'genotype',\n",
       " 'boy',\n",
       " 'severe',\n",
       " 'intellectual',\n",
       " 'features',\n",
       " 'disability',\n",
       " 'strabism',\n",
       " 'hyperopia',\n",
       " 'microcephaly',\n",
       " 'under',\n",
       " '8',\n",
       " 'months',\n",
       " 'presented',\n",
       " 'hypotonia',\n",
       " 'growth',\n",
       " 'failure',\n",
       " 'cendedtestisthemolecularkaryotypewasnormalexome',\n",
       " 'bilateral',\n",
       " 'cryptorchidism',\n",
       " 'hypospadias',\n",
       " 'analysis',\n",
       " 'revealed',\n",
       " 'recurrent',\n",
       " 'de',\n",
       " 'novo',\n",
       " 'nonsense',\n",
       " 'variant',\n",
       " 'neurophysiological',\n",
       " 'studies',\n",
       " 'biochemical',\n",
       " 'analyses',\n",
       " 'c2737c',\n",
       " 'part913ter',\n",
       " 'penultimate',\n",
       " 'exon',\n",
       " 'showed',\n",
       " 'normal',\n",
       " 'results',\n",
       " 'swim6',\n",
       " 'addition',\n",
       " 'heterozygous',\n",
       " 'misses',\n",
       " 'variant',\n",
       " 'nowadays',\n",
       " '6y',\n",
       " 'major',\n",
       " 'features',\n",
       " 'verbal',\n",
       " 'c3119g',\n",
       " 'part1040his',\n",
       " 'thatwaspredictedbyseveral',\n",
       " 'language',\n",
       " 'presents',\n",
       " 'peculiar',\n",
       " 'facial',\n",
       " 'phenotype',\n",
       " 'tools',\n",
       " 'probably',\n",
       " 'pathogenic',\n",
       " 'variant',\n",
       " 'present',\n",
       " 'hypertelorism',\n",
       " 'narrow',\n",
       " 'palpebral',\n",
       " 'ï¬\\x81assures',\n",
       " 'lowest',\n",
       " 'ears',\n",
       " 'nbsp',\n",
       " 'rs192222164',\n",
       " 'andingnomad',\n",
       " 'f00003779',\n",
       " 'prominent',\n",
       " 'crush',\n",
       " 'curricular',\n",
       " 'pit',\n",
       " 'among',\n",
       " 'others',\n",
       " 'wasinheritedfromtheunaffectedmotherthephenotypeof',\n",
       " 'informed',\n",
       " 'consent',\n",
       " 'obtained',\n",
       " 'parents',\n",
       " 'patient',\n",
       " 'evident',\n",
       " 'ï¬\\x81ts',\n",
       " 'spectrum',\n",
       " 'subsequent',\n",
       " 'studies',\n",
       " 'conventional',\n",
       " 'karyotype',\n",
       " 'ach',\n",
       " 'prayer',\n",
       " 'described',\n",
       " 'recurrent',\n",
       " 'de',\n",
       " 'novo',\n",
       " 'nonsense',\n",
       " 'variant',\n",
       " 'willisyndromeassociateddefectsandexomevariantswere',\n",
       " 'remains',\n",
       " 'elucidated',\n",
       " 'whether',\n",
       " 'materially',\n",
       " 'inherited',\n",
       " 'studied',\n",
       " 'misses',\n",
       " 'variant',\n",
       " 'might',\n",
       " 'relevant',\n",
       " 'possible',\n",
       " 'recessive',\n",
       " 'results',\n",
       " 'karyotype',\n",
       " 'ach',\n",
       " 'analysis',\n",
       " 'revealed',\n",
       " 'modeofinheritanceanyway',\n",
       " 'themissensemutationofour',\n",
       " 'alterations',\n",
       " 'praderwilli',\n",
       " 'syndrome',\n",
       " 'discarded',\n",
       " 'neither',\n",
       " 'patient',\n",
       " 'complicated',\n",
       " 'interpretation',\n",
       " 'mode',\n",
       " 'methylation',\n",
       " 'cn',\n",
       " 'alterations',\n",
       " 'found',\n",
       " '15q11',\n",
       " 'inheritance',\n",
       " 'patient',\n",
       " 'ï¬\\x81endings',\n",
       " 'widen',\n",
       " 'spectrum',\n",
       " 'region',\n",
       " 'genotype',\n",
       " 'phenotypes',\n",
       " 'associated',\n",
       " 'swim6abstractsfromthe51steuropeansocietyofhumangeneticsconference',\n",
       " 'â\\x80¦',\n",
       " '957',\n",
       " 'baker',\n",
       " 'none',\n",
       " 'gamer',\n",
       " 'none',\n",
       " 'praised',\n",
       " 'none',\n",
       " 'ep1',\n",
       " 'mutations',\n",
       " 'genotype',\n",
       " 'frequency',\n",
       " 'phenotype',\n",
       " 'correlations',\n",
       " 'cohort',\n",
       " '25',\n",
       " 'dutch',\n",
       " 'patients',\n",
       " 'robert',\n",
       " 'syndrome',\n",
       " 'gg',\n",
       " '47',\n",
       " '4',\n",
       " '58',\n",
       " '2',\n",
       " '53',\n",
       " '6',\n",
       " 'h',\n",
       " 'roes1',\n",
       " 'p',\n",
       " 'h',\n",
       " 'van',\n",
       " 'on1',\n",
       " 'lindhout1',\n",
       " 'green1',\n",
       " 'e',\n",
       " 'france',\n",
       " 'van',\n",
       " 'de',\n",
       " 'il6',\n",
       " 'cg',\n",
       " 'cc',\n",
       " 'putt1',\n",
       " 'r',\n",
       " 'j',\n",
       " 'nievelstein2',\n",
       " 'r',\n",
       " 'since1',\n",
       " '52',\n",
       " '6',\n",
       " '41',\n",
       " '8',\n",
       " '46',\n",
       " '4',\n",
       " '1\\x18\\x18',\n",
       " 'allele',\n",
       " '1dept',\n",
       " 'medical',\n",
       " 'genetics',\n",
       " 'university',\n",
       " 'medical',\n",
       " 'center',\n",
       " 'utrecht',\n",
       " 'neither',\n",
       " 'c\\x18\\x18\\x18',\n",
       " 'c\\x186\\x18',\n",
       " 'c\\x18\\x18',\n",
       " 'frequency',\n",
       " 'lands',\n",
       " '2dept',\n",
       " 'pediatric',\n",
       " 'radiology',\n",
       " 'university',\n",
       " 'medical',\n",
       " 'center',\n",
       " 'utrecht',\n",
       " 'netherlands',\n",
       " 'tt',\n",
       " 'robert',\n",
       " 'syndrome',\n",
       " 'lbs',\n",
       " 'mental',\n",
       " 'retardation',\n",
       " 'syndrome',\n",
       " '\\x1861',\n",
       " '\\x18\\x18\\x18',\n",
       " '1\\x18\\x18',\n",
       " 'ing',\n",
       " 'aa',\n",
       " 'hypotonia',\n",
       " 'ataxia',\n",
       " 'terms',\n",
       " 'hyperplasia',\n",
       " 'oculomotor',\n",
       " 'abnormalities',\n",
       " '\\x18\\x18\\x18',\n",
       " '6\\x18\\x18',\n",
       " '\\x18\\x18\\x18',\n",
       " '\\x18\\x18\\x18',\n",
       " 'allele',\n",
       " 'characteristic',\n",
       " 'breathing',\n",
       " 'pattern',\n",
       " 'associated',\n",
       " 'anomalies',\n",
       " 'include',\n",
       " 'renal',\n",
       " 'a60',\n",
       " 'a\\x18\\x18',\n",
       " 'a\\x18\\x18',\n",
       " 'retinal',\n",
       " 'disease',\n",
       " 'lbs',\n",
       " 'shows',\n",
       " 'autosomal',\n",
       " 'recessive',\n",
       " 'inheritance',\n",
       " 'frequencyclinical',\n",
       " 'genetics',\n",
       " '1\\x18\\x18',\n",
       " 'genetically',\n",
       " 'heterogeneous',\n",
       " 'mutations',\n",
       " 'hi1gene',\n",
       " 'asd',\n",
       " 'vs',\n",
       " 'dextroposition',\n",
       " 'recorded',\n",
       " '7174',\n",
       " 'cases',\n",
       " 'half',\n",
       " 'homologous',\n",
       " 'deletions',\n",
       " 'nph57',\n",
       " 'audit',\n",
       " 'spots',\n",
       " 'demonstrating',\n",
       " 'predisposition',\n",
       " 'hyperpigmented',\n",
       " 'lesions',\n",
       " 'heterozygous',\n",
       " 'deletion',\n",
       " '4',\n",
       " 'exon',\n",
       " 'part',\n",
       " 'gene',\n",
       " 'confirms',\n",
       " 'positive',\n",
       " 'ns',\n",
       " 'individuals',\n",
       " 'moreover',\n",
       " 'affected',\n",
       " 'father',\n",
       " 'family',\n",
       " 'contribution',\n",
       " 'intellectual',\n",
       " 'disability',\n",
       " 'presented',\n",
       " 'hearing',\n",
       " 'deficit',\n",
       " 'since',\n",
       " 'birth',\n",
       " 'together',\n",
       " 'lentigines',\n",
       " 'ilencikova1',\n",
       " 'g',\n",
       " 'webersinke2',\n",
       " 'deutschbauer2',\n",
       " 'matter1',\n",
       " 'h',\n",
       " 'dub1',\n",
       " 'two',\n",
       " 'characteristics',\n",
       " 'ns',\n",
       " 'multiple',\n",
       " 'lentigines',\n",
       " 'previously',\n",
       " 'leopard',\n",
       " '1department',\n",
       " 'human',\n",
       " 'genetics',\n",
       " 'landesfrauen',\n",
       " 'und',\n",
       " 'kinderklinik',\n",
       " 'link',\n",
       " 'austria',\n",
       " '2laboratory',\n",
       " 'molecular',\n",
       " 'biology',\n",
       " 'tumor',\n",
       " 'cytogenetics',\n",
       " 'department',\n",
       " '1st',\n",
       " 'internal',\n",
       " 'syndrome',\n",
       " 'supporting',\n",
       " 'difficulties',\n",
       " 'diagnosing',\n",
       " 'individuals',\n",
       " 'ras',\n",
       " 'medicine',\n",
       " 'krankenhaus',\n",
       " 'barmherzige',\n",
       " 'schwestern',\n",
       " 'link',\n",
       " 'austria',\n",
       " 'opathies',\n",
       " 'correctly',\n",
       " 'clinical',\n",
       " 'genetic',\n",
       " 'heterogeneity',\n",
       " 'observed',\n",
       " 'ras',\n",
       " 'opathies',\n",
       " 'challenge',\n",
       " 'genetic',\n",
       " 'testing',\n",
       " 'advantages',\n",
       " 'nextgeneration',\n",
       " 'se',\n",
       " 'fencing',\n",
       " 'allows',\n",
       " 'screening',\n",
       " 'large',\n",
       " 'number',\n",
       " 'genes',\n",
       " 'simultaneously',\n",
       " 'understanding',\n",
       " 'genetic',\n",
       " 'basis',\n",
       " 'intellectual',\n",
       " 'disability',\n",
       " 'id',\n",
       " 'important',\n",
       " 'impact',\n",
       " 'correct',\n",
       " 'diagnosis',\n",
       " 'prognosis',\n",
       " 'treat',\n",
       " 'increased',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'use',\n",
       " 'sensitive',\n",
       " 'pangenomic',\n",
       " 'techniques',\n",
       " 'm1e6n2t',\n",
       " 'patients',\n",
       " 'future',\n",
       " 'sha',\n",
       " '2015',\n",
       " 'glasgow',\n",
       " 'scotland',\n",
       " 'uk',\n",
       " 'wwweshgorg',\n",
       " 'ach',\n",
       " 'snparray',\n",
       " 'contributes',\n",
       " 'discover',\n",
       " 'new',\n",
       " 'genes',\n",
       " 'associated',\n",
       " 'synabstracts',\n",
       " 'posters',\n",
       " 'back',\n",
       " 'index',\n",
       " 'comic',\n",
       " 'nonsyndromic',\n",
       " 'id',\n",
       " 'tory',\n",
       " 'action',\n",
       " 'alternative',\n",
       " 'splicing',\n",
       " 'large',\n",
       " 'tissuespecific',\n",
       " 'gene',\n",
       " 'networks',\n",
       " 'report',\n",
       " '18monthly',\n",
       " 'turkish',\n",
       " 'child',\n",
       " 'psychomotoric',\n",
       " 'develop',\n",
       " 'materials',\n",
       " 'methods',\n",
       " 'oligonucleotide',\n",
       " 'arraycgh',\n",
       " 'analysis',\n",
       " 'using',\n",
       " 'agi',\n",
       " 'mental',\n",
       " 'disorder',\n",
       " 'presents',\n",
       " 'unstable',\n",
       " 'sitting',\n",
       " 'severe',\n",
       " 'hypotonia',\n",
       " 'lent',\n",
       " '4x180k',\n",
       " 'platform',\n",
       " 'performed',\n",
       " '1200',\n",
       " 'patients',\n",
       " 'mental',\n",
       " 'regard',\n",
       " 'speech',\n",
       " 'delay',\n",
       " 'without',\n",
       " 'dysmorphisms',\n",
       " 'snp',\n",
       " 'array',\n",
       " 'analysis',\n",
       " 'identified',\n",
       " 'tion',\n",
       " 'autism',\n",
       " 'spectrum',\n",
       " 'disorders',\n",
       " 'congenital',\n",
       " 'anomalies',\n",
       " 'â\\x80\\x9ede',\n",
       " 'novoâ\\x80\\x9c',\n",
       " '185',\n",
       " 'kb',\n",
       " 'deletion',\n",
       " 'chromosome',\n",
       " '20q12',\n",
       " 'encompassing',\n",
       " 'de',\n",
       " 'results',\n",
       " 'cohort',\n",
       " 'identified',\n",
       " '6',\n",
       " 'cases',\n",
       " 'intragenic',\n",
       " 'deletions',\n",
       " 'rb',\n",
       " 'legion',\n",
       " '4',\n",
       " 'exon',\n",
       " 'single',\n",
       " 'coding',\n",
       " 'gene',\n",
       " 'named',\n",
       " 'part',\n",
       " 'proteintyrosine',\n",
       " 'fox1',\n",
       " '4',\n",
       " 'additional',\n",
       " 'cn',\n",
       " 'chromosomes',\n",
       " 'cns',\n",
       " 'phosphate',\n",
       " 'receptor',\n",
       " 'type',\n",
       " 'part',\n",
       " 'gene',\n",
       " 'entry',\n",
       " 'omit',\n",
       " 'database',\n",
       " 'located',\n",
       " '16p133',\n",
       " 'extended',\n",
       " '12mb',\n",
       " 'region',\n",
       " '60879837',\n",
       " '608712',\n",
       " 'includes',\n",
       " 'single',\n",
       " 'variant',\n",
       " 'unknown',\n",
       " 'significancethr1365met',\n",
       " '207012',\n",
       " 'arch37hg19',\n",
       " 'corresponds',\n",
       " 'ironic',\n",
       " 'regions',\n",
       " '5',\n",
       " 'nbsp',\n",
       " 'rs199947379',\n",
       " '6',\n",
       " 'patients',\n",
       " 'five',\n",
       " 'patients',\n",
       " 'inherited',\n",
       " 'deletion',\n",
       " 'unaffec',\n",
       " 'variant',\n",
       " 'revealed',\n",
       " 'due',\n",
       " 'some',\n",
       " 'sequencing',\n",
       " '3',\n",
       " 'dutch',\n",
       " 'sits',\n",
       " 'se',\n",
       " 'ted',\n",
       " 'progenitor',\n",
       " 'one',\n",
       " 'unknown',\n",
       " 'inheritance',\n",
       " 'pattern',\n",
       " 'were',\n",
       " 'intellectual',\n",
       " 'disability',\n",
       " 'published',\n",
       " 'schuurshoeijmakerset',\n",
       " 'al',\n",
       " '2013',\n",
       " 'conclusions',\n",
       " 'phenotypes',\n",
       " 'children',\n",
       " 'common',\n",
       " 'wide',\n",
       " 'authors',\n",
       " 'noted',\n",
       " 'part',\n",
       " 'gene',\n",
       " 'expressed',\n",
       " 'brain',\n",
       " 'neuro',\n",
       " 'ly',\n",
       " 'described',\n",
       " 'neurological',\n",
       " 'alterations',\n",
       " 'rearrangement',\n",
       " 'identified',\n",
       " 'lead',\n",
       " 'nal',\n",
       " 'tissue',\n",
       " 'encodes',\n",
       " 'transmembrane',\n",
       " 'receptor',\n",
       " 'protein',\n",
       " 'tyrosine',\n",
       " 'reduction',\n",
       " 'box1',\n",
       " 'expression',\n",
       " 'corroborating',\n",
       " 'importance',\n",
       " 'box1',\n",
       " 'phosphatase',\n",
       " 'family',\n",
       " 'important',\n",
       " 'proteins',\n",
       " 'signal',\n",
       " 'transduction',\n",
       " 'haploinsufficiency',\n",
       " 'phenotypes',\n",
       " 'continuing',\n",
       " 'followup',\n",
       " 'described',\n",
       " 'complex',\n",
       " 'phenotype',\n",
       " '5',\n",
       " 'affected',\n",
       " 'sisters',\n",
       " 'including',\n",
       " 'beta',\n",
       " 'patients',\n",
       " 'similar',\n",
       " 'cases',\n",
       " 'help',\n",
       " 'understanding',\n",
       " 'phenotypic',\n",
       " 'viral',\n",
       " 'problems',\n",
       " 'microcephaly',\n",
       " 'congenital',\n",
       " 'heart',\n",
       " 'defects',\n",
       " 'short',\n",
       " 'stature',\n",
       " 'diversity',\n",
       " 'patients',\n",
       " 'diaphragmatic',\n",
       " 'herniation',\n",
       " 'combination',\n",
       " 'heterozygous',\n",
       " 'misses',\n",
       " 'va',\n",
       " 'pm0860',\n",
       " 'giant',\n",
       " 'heterozygous',\n",
       " 'ironic',\n",
       " 'deletion',\n",
       " '150',\n",
       " 'kb',\n",
       " 'suggests',\n",
       " 'autosomal',\n",
       " 'mysticism',\n",
       " 'start',\n",
       " 'codon',\n",
       " 'mutation',\n",
       " 'mec',\n",
       " 'p2p08',\n",
       " '19s',\n",
       " 'enables',\n",
       " 'elucidating',\n",
       " 'genetic',\n",
       " 'background',\n",
       " 'id',\n",
       " 'chromosomal',\n",
       " 'microarray',\n",
       " 'analysis',\n",
       " 'patients',\n",
       " 'intellectual',\n",
       " 'performed',\n",
       " 'wes',\n",
       " 'cohort',\n",
       " '250',\n",
       " 'individuals',\n",
       " 'unexplained',\n",
       " 'id',\n",
       " 'disability',\n",
       " 'autism',\n",
       " 'multiple',\n",
       " 'congenital',\n",
       " 'anomalies',\n",
       " 'presenting',\n",
       " 'identified',\n",
       " 'heterozygous',\n",
       " 'de',\n",
       " 'novo',\n",
       " 'cnn1',\n",
       " 'mutations',\n",
       " 'five',\n",
       " 'unrelated',\n",
       " 'india',\n",
       " 'genetic',\n",
       " 'services',\n",
       " 'palumbo',\n",
       " 'p',\n",
       " 'palumbo',\n",
       " 'r',\n",
       " 'stallion',\n",
       " 'palladium',\n",
       " 'l',\n",
       " 'relate',\n",
       " 'cappella',\n",
       " 'visuals',\n",
       " 'three',\n",
       " 'frameshift',\n",
       " 'one',\n",
       " 'stop',\n",
       " 'one',\n",
       " 'splice',\n",
       " 'mutation',\n",
       " 'five',\n",
       " 'path',\n",
       " 'circus',\n",
       " 'casa',\n",
       " 'sollievo',\n",
       " 'della',\n",
       " 'sofferenza',\n",
       " 'san',\n",
       " 'giovanni',\n",
       " 'rotunda',\n",
       " 'italy',\n",
       " 'ends',\n",
       " 'severe',\n",
       " 'motor',\n",
       " 'delay',\n",
       " 'profound',\n",
       " 'speech',\n",
       " 'impairment',\n",
       " 'hypotonia',\n",
       " 'trunk',\n",
       " 'hypertonia',\n",
       " 'legs',\n",
       " 'craniofacial',\n",
       " 'phenotype',\n",
       " 'comprises',\n",
       " 'microcephaly',\n",
       " '45',\n",
       " 'consistent',\n",
       " 'facial',\n",
       " 'features',\n",
       " 'broad',\n",
       " 'nasal',\n",
       " 'tip',\n",
       " 'copy',\n",
       " 'number',\n",
       " 'variations',\n",
       " 'cns',\n",
       " 'common',\n",
       " 'identifiable',\n",
       " 'causes',\n",
       " 'small',\n",
       " 'alan',\n",
       " 'nasa',\n",
       " 'long',\n",
       " 'philtrum',\n",
       " 'thin',\n",
       " 'upper',\n",
       " 'lip',\n",
       " 'vermillion',\n",
       " 'intellectual',\n",
       " 'disabilitydevelopmental',\n",
       " 'delay',\n",
       " 'ddd',\n",
       " 'autism',\n",
       " 'spectrum',\n",
       " 'betacatenin',\n",
       " 'key',\n",
       " 'downstream',\n",
       " 'component',\n",
       " 'canonical',\n",
       " 'wnt',\n",
       " 'signaling',\n",
       " 'disorders',\n",
       " 'adds',\n",
       " 'multiple',\n",
       " 'congenital',\n",
       " 'anomalies',\n",
       " 'mas',\n",
       " 'chromosome',\n",
       " 'pathway',\n",
       " 'acts',\n",
       " 'negative',\n",
       " 'regulator',\n",
       " 'centrosome',\n",
       " 'cohesion',\n",
       " 'whereas',\n",
       " 'mal',\n",
       " 'microarray',\n",
       " 'analysis',\n",
       " 'cma',\n",
       " '1020',\n",
       " 'diagnostic',\n",
       " 'yield',\n",
       " 'idea',\n",
       " 'somatic',\n",
       " 'gainoffunction',\n",
       " 'mutations',\n",
       " 'already',\n",
       " 'found',\n",
       " 'various',\n",
       " 'tu',\n",
       " 'tiny',\n",
       " 'cnsâ\\x89¤1',\n",
       " 'mb',\n",
       " 'report',\n",
       " 'experience',\n",
       " 'use',\n",
       " 'affymetrix',\n",
       " 'mor',\n",
       " 'types',\n",
       " 'germane',\n",
       " 'lossoffunction',\n",
       " 'mutations',\n",
       " 'suspected',\n",
       " 'animal',\n",
       " 'snp',\n",
       " 'arrays',\n",
       " '1600',\n",
       " 'italian',\n",
       " 'patients',\n",
       " 'past',\n",
       " '6',\n",
       " 'years',\n",
       " '20082013',\n",
       " 'models',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "increased-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['viable', 'gobble', 'note', 'white', 'test']\n"
     ]
    }
   ],
   "source": [
    "# runs forever without finishing. Find alternative or skip it?\n",
    "spell = ['wibble', 'wobble', 'nope', 'shite', 'test']\n",
    "spell_test = []\n",
    "\n",
    "for word in spell:\n",
    "    spell_test.append(check(word))    \n",
    "\n",
    "print(spell_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "focused-rover",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ef4d6c6731a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mabstract_t_w_l_np_nsw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mabstract_t_w_l_np_cs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract_t_w_l_np_cs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36mautocorrect_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mautocorrect_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         return re.sub(\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0mword_regexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mlambda\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocorrect_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(match)\u001b[0m\n\u001b[0;32m    128\u001b[0m         return re.sub(\n\u001b[0;32m    129\u001b[0m             \u001b[0mword_regexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[1;32mlambda\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocorrect_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         )\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36mautocorrect_word\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# in case the word is capitalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36mget_candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexisting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexisting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexisting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble_typos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             )\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36mexisting\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;34m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\__init__.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;34m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\autocorrect\\typos.py\u001b[0m in \u001b[0;36m_replaces\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_inserts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# runs forever without finishing. Find alternative or skip it?\n",
    "\n",
    "abstract_t_w_l_np_cs = []\n",
    "\n",
    "for word in abstract_t_w_l_np_nsw:\n",
    "    abstract_t_w_l_np_cs.append(check(word))    \n",
    "\n",
    "print(abstract_t_w_l_np_cs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "increased-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p08', '37', 'hospit', 'â\\x80\\x93', 'osi', 'bilbaobasurto', 'â\\x80\\x93', 'osakidetza', 'bilbao', 'spain']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "abstract_t_w_l_np_nsw_stem = [porter.stem(word) for word in abstract_t_w_l_np_nsw]\n",
    "print(abstract_t_w_l_np_nsw_stem[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "consistent-commitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(abstract_t_w_l_np_nsw_stem)\n",
    "print(type(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "insured-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gene', 3536), ('none', 3535), ('patient', 3517), ('genet', 3106), ('de', 2423), ('univers', 2211), ('disord', 2200), ('autism', 2057), ('mutat', 1878), ('asd', 1737), ('clinic', 1705), ('studi', 1582), ('syndrom', 1559), ('delet', 1557), ('associ', 1535), ('variant', 1439), ('case', 1427), ('chromosom', 1398), ('hospit', 1252), ('use', 1240), ('medic', 1236), ('phenotyp', 1227), ('analysi', 1171), ('result', 1163), ('report', 1160), ('unit', 1126), ('c', 1116), ('famili', 1099), ('j', 1091), ('e', 1034), ('genom', 1025), ('two', 983), ('franc', 978), ('spectrum', 973), ('includ', 958), ('region', 953), ('p', 952), ('present', 947), ('sequenc', 934), ('diseas', 930), ('featur', 921), ('cnv', 906), ('identifi', 882), ('caus', 872), ('sever', 854), ('show', 804), ('delay', 794), ('disabl', 784), ('human', 770), ('itali', 761), ('institut', 756), ('one', 752), ('l', 733), ('medicin', 725), ('2', 720), ('r', 714), ('intellectu', 713), ('molecular', 707), ('duplic', 694), ('cell', 687), ('b', 683), ('detect', 665), ('affect', 664), ('g', 660), ('individu', 658), ('development', 647), ('involv', 646), ('develop', 636), ('rare', 633), ('1', 620), ('found', 607), ('research', 597), ('mental', 593), ('perform', 588), ('also', 581), ('center', 581), ('state', 581), ('abnorm', 576), ('n', 573), ('function', 570), ('k', 569), ('express', 568), ('differ', 562), ('children', 561), ('f', 560), ('method', 557), ('normal', 549), ('reveal', 539), ('autist', 538), ('year', 537), ('control', 536), ('test', 535), ('retard', 534), ('id', 528), ('brain', 523), ('novo', 518), ('number', 504), ('suggest', 499), ('character', 496), ('parent', 493)]\n"
     ]
    }
   ],
   "source": [
    "print(counts.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-basis",
   "metadata": {},
   "source": [
    "## Person-first and identity first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "import re \n",
    "import string \n",
    "import nltk \n",
    "import spacy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy -q\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_lg -q\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity = nltk.sent_tokenize(bag_of_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(person_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "person =\"\"\n",
    "\n",
    "for sentence in person_identity:\n",
    "    if any(s in sentence for s in ['autisic', 'Autisic', 'autism', 'Autism']):\n",
    "        person += sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in person_doc: \n",
    "  print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = [{\"TEXT\": {\"REGEX\": \"^[Aa]utistic$\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_1\", [pattern_1]) \n",
    "\n",
    "autistic =[]\n",
    "\n",
    "matches = matcher(person_doc) \n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = person_doc[start:end]  # The matched span\n",
    "    autistic.append(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "autistic_no_dups = list(set(autistic))\n",
    "with open('..\\\\counts\\\\ESHG\\\\autism.csv', \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in autistic_no_dups:\n",
    "            write.writerow([item])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in autistic_no_dups:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_2 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]utism$\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_1\", [pattern_2]) \n",
    "\n",
    "person_with =[]\n",
    "matches = matcher(person_doc) \n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = person_doc[start:end]  # The matched span\n",
    "    person_with.append(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_with_no_dups = list(set(person_with))\n",
    "with open('..\\\\counts\\\\ESHG\\\\person_with.csv', \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in person_with_no_dups:\n",
    "            write.writerow([item])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
