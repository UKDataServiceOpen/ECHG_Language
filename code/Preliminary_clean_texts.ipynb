{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-democrat",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\python39\\lib\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "!pip install autocorrect          \n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "\n",
    "import re                         # things we need for RegEx corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-token",
   "metadata": {},
   "source": [
    "# Test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceramic-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001abstractICHG.txt',\n",
       " '2002Abstracts.txt',\n",
       " '2003Abstracts.txt',\n",
       " '2004Abstracts.txt',\n",
       " '2005Abstracts.txt',\n",
       " '2006Abstracts.txt',\n",
       " '2007Abstracts.txt',\n",
       " '2008Abstracts.txt',\n",
       " '2009Abstracts.txt',\n",
       " '2010Abstracts.txt',\n",
       " '2011Abstracts.txt',\n",
       " '2012Abstracts.txt',\n",
       " '2013Abstracts.txt',\n",
       " '2014Abstracts.txt',\n",
       " '2015Abstracts.txt',\n",
       " '2016Abstracts.txt',\n",
       " '2017 electronic posters.txt',\n",
       " '2017 oral presentations.txt',\n",
       " '2017 posters.txt',\n",
       " '2018 electronic posters.txt',\n",
       " '2018 EMPAG.txt',\n",
       " '2018 oral presentation.txt',\n",
       " '2018 posters.txt',\n",
       " '2019 oral presentation.txt',\n",
       " '2019 posters.txt',\n",
       " '2019 posters2.txt',\n",
       " '2020 eposters.txt',\n",
       " '2020 interactive eposter.txt',\n",
       " '2020 oral presentation.txt',\n",
       " '2021 eposters.txt',\n",
       " '2021 oral presentations.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\\\\for_analysis\\\\ESHG\") # This is how to see the contents of any folders shown in the last contents check\n",
    "                            # For example, 'input_pdfs' which is likely to contain things we want to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "leading-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_separators(regex, s):\n",
    "    matches = list(filter(None, regex.split(s)))\n",
    "    return matches\n",
    "\n",
    "regex = re.compile(r\"(e.*?)\")\n",
    "\n",
    "def split_test(input, output):\n",
    "    for filename in os.listdir(input):\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='utf-8') as f:\n",
    "            name = filename.replace(r'.txt', \"\")\n",
    "            for line in f:\n",
    "                matches = split_with_separators(regex, line)\n",
    "                del matches[0]\n",
    "                with open(output + \"\\\\\" + name + \".txt\", \"w\", encoding='utf-8') as fp:\n",
    "                    for match in matches:\n",
    "                        row_contents = matches[0] + \" \" + matches[1]\n",
    "                        del matches [0]\n",
    "                        del matches [0]\n",
    "                        fp.write(\"%s\\n\" % row_contents)\n",
    "                    print('Done ' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solar-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done input_pdf_1\n",
      "Done input_pdf_2\n",
      "Done input_pdf_3\n"
     ]
    }
   ],
   "source": [
    "split_test (\"..\\output_texts\\Test\", \"..\\\\for_analysis\\\\Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_separators(regex, s):\n",
    "    matches = list(filter(None, regex.split(s)))\n",
    "    return matches\n",
    "\n",
    "regex_ESHG = re.compile(r\"([PL|S|C|E]\\d+)\")\n",
    "\n",
    "def split_ESHG(input, output):\n",
    "    totals = []\n",
    "    for filename in os.listdir(input):\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='utf-8') as f:\n",
    "            name = filename.replace(r'.txt', \"\")\n",
    "            for line in f:\n",
    "                matches = split_with_separators(regex_ESHG, line)\n",
    "                length_matches = len(matches)\n",
    "                del matches[0]\n",
    "                with open(output + \"\\\\\" + name + \".txt\", \"w\", encoding='utf-8') as fp:\n",
    "                    for match in matches:\n",
    "                        row_contents = matches[0] + \" \" + matches[1]\n",
    "                        del matches [0]\n",
    "                        del matches [0]\n",
    "                        fp.write(\"%s\\n\" % row_contents)\n",
    "                totals_row = [name, length_matches]\n",
    "                totals.append(totals_row)\n",
    "    with open(output + \"\\\\\" + \"totals.csv\", \"w\", encoding='utf-8') as out_total:\n",
    "        writer = csv.writer(out_total)\n",
    "        for row in totals:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "australian-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ESHG (\"..\\output_texts\\ESHG\", \"..\\\\counts\\\\ESHG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instant-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_strings_in_text(input, output, list_of_strings):\n",
    "    list_of_results = []\n",
    "    # Open the file in read only mode\n",
    "    for filename in os.listdir(input):\n",
    "        name = filename.replace(r'.txt', \"\")\n",
    "        line_number = 0\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='ISO-8859-1') as read_obj:\n",
    "            for line in read_obj:\n",
    "                line_number += 1\n",
    "                # For each line, check if line contains any string from the list of strings\n",
    "                for string_to_search in list_of_strings:\n",
    "                    if string_to_search in line:\n",
    "                    # If any string is found in line, then append that line along with line number in list\n",
    "                        list_of_results.append((name, string_to_search, line_number, line.rstrip()))\n",
    "    no_dups_results = list(set(list_of_results))\n",
    "    with open(output + \"\\\\select.csv\", \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        write.writerows(no_dups_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supposed-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    " match_strings_in_text('..\\\\for_analysis\\\\ESHG', \"..\\\\counts\\\\ESHG\", ['autis', 'Autis', 'ASD', 'Asperger', 'asperger',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
