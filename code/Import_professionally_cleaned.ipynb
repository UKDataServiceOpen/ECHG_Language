{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Get ready\n",
    "\n",
    "First, download, import, prep packages and such. \n",
    "\n",
    "Then, check the file location and import the .csv files. Remove any with empty text fields. \n",
    "\n",
    "Save a data frame with all the texts and another with only those texts that mention the keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n",
    "!pip install autocorrect        \n",
    "!pip install pyspellchecker \n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize  \n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )\n",
    "\n",
    "files = []\n",
    "def import_results(input):\n",
    "    for f in os.listdir(input):\n",
    "        f = pd.read_csv(input + '\\\\'+ f)\n",
    "        files.append(f)\n",
    "    output = pd.concat(files)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = import_results(\"..\\\\results\")\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results[~all_results['Text'].isnull()]\n",
    "len(no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "# Count word frequencies \n",
    "## Bag of words\n",
    "\n",
    "Proceed through the 'bag of words' steps for the data frames with all texts and then again for the data frame with only the texts that mention the keywords of interest. This approach finds word frequencies for all years together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):\n",
    "    holding_string = \"\"\n",
    "    for text in input['Text']:\n",
    "        holding_string += text\n",
    "    holding_string = word_tokenize(holding_string)\n",
    "    holding_string = [word.lower() for word in holding_string]\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]\n",
    "    holding_string = [porter.stem(token) for token in holding_string]\n",
    "    list_for_count = []\n",
    "    for token in holding_string:\n",
    "        list_for_count.append(token)\n",
    "    counts = Counter(list_for_count)\n",
    "    return counts.most_common(how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(no_null_texts, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(matched_texts, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292c709",
   "metadata": {},
   "source": [
    "## Word Frequency by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def track_word_over_time(input, target_word):\n",
    "    years = input['Year'].drop_duplicates()\n",
    "    target_counts = []\n",
    "    for year in years:\n",
    "        year_bag = \"\"\n",
    "        for text in no_null_texts['Text'][no_null_texts['Year']==year]:\n",
    "            year_bag += text\n",
    "        year_bag = word_tokenize(year_bag)\n",
    "        year_bag = [word.lower() for word in year_bag]\n",
    "        year_bag = [w.translate(table_punctuation) for w in year_bag]\n",
    "        year_bag = (list(filter(lambda x: x, year_bag)))\n",
    "        year_bag = [token for token in year_bag if not token.isdigit()]\n",
    "        year_bag = [token for token in year_bag if token not in stop_words]\n",
    "        year_bag = [porter.stem(token) for token in year_bag]\n",
    "        list_for_count = []\n",
    "        for token in year_bag:\n",
    "            list_for_count.append(token)\n",
    "        counts = Counter(list_for_count)\n",
    "        target_counts.append(counts[target_word])\n",
    "        \n",
    "    target_word_by_year = pd.DataFrame(list(zip(years, target_counts)), columns = ['Year', str(target_word)])\n",
    "    return target_word_by_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_list = ['gene', 'cancer', 'risk']\n",
    "\n",
    "for word in target_word_list:\n",
    "    word  = track_word_over_time(no_null_texts, word)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [target_word.set_index('Year') for target_word in target_word_list]\n",
    "to_plot[0].join(to_proh[1:])\n",
    "\n",
    "\n",
    "#combine = gene.set_index('Year').join(cancer.set_index('Year'), how='left')\n",
    "to_plot.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = track_word_over_time(no_null_texts, 'gene')\n",
    "plt.plot(x=gene['Year'], y=gene['Count'], linestyle = 'dotted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
