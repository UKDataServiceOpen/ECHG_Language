{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rolled-channels",
   "metadata": {},
   "source": [
    "# Import and get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "\n",
    "!pip install autocorrect        \n",
    "!pip install pyspellchecker \n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize    \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from collections import Counter\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "\n",
    "import string \n",
    "import spacy \n",
    "import math \n",
    "#from tqdm import tqdm \n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\ESHG\")) # This is how to see the contents of any folders shown in the last contents check\n",
    "print(os.listdir(\"..\\\\counts\\\\ESHG\")) # This is how to see the contents of any folders shown in the last contents check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-content",
   "metadata": {},
   "source": [
    "# Read in and organise files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = pd.read_csv('..\\\\counts\\\\ESHG\\\\totals.csv', header=None)\n",
    "totals_df.columns = [\"filename\", \"abstracts\"]\n",
    "#totals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_select_abstracts = []\n",
    "\n",
    "with open('..\\\\counts\\\\ESHG\\\\select.csv', newline='', encoding = \"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        raw_select_abstracts.append(row)\n",
    "        \n",
    "select_abstracts = (list(filter(lambda x: x, raw_select_abstracts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = pd.DataFrame.from_records(select_abstracts)\n",
    "total_select = select_df.iloc[:,0].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "autism_count = select_df.iloc[:,0].value_counts().rename_axis('unique_values').to_frame('counts')\n",
    "\n",
    "autism_count = autism_count.reset_index()\n",
    "autism_count.columns = ['filename', 'autism_abstracts']\n",
    "\n",
    "#autism_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts = pd.merge(totals_df, autism_count, on = 'filename')\n",
    "merged_counts['year'] = merged_counts['filename'].str.extract(r'(\\d{4})')\n",
    "merged_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_counts['abstracts'].sum())\n",
    "print(merged_counts['autism_abstracts'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-explanation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_counts[['year', 'abstracts']].groupby('year').sum('abstracts'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-closure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_counts[['year', 'autism_abstracts']].groupby('year').sum('autism_abstracts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-picking",
   "metadata": {},
   "source": [
    "# Working with the contents of the selected abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-identifier",
   "metadata": {},
   "source": [
    "##  Dump the content of the selected abtsracts into one big string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naughty-laundry",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select_abstracts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m bag_of_abstracts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m abstract \u001b[38;5;129;01min\u001b[39;00m (\u001b[43mselect_abstracts\u001b[49m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(abstract[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      5\u001b[0m     bag_of_abstracts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m abstract[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'select_abstracts' is not defined"
     ]
    }
   ],
   "source": [
    "bag_of_abstracts = \"\"\n",
    "\n",
    "for abstract in (select_abstracts):\n",
    "    print(abstract[3])\n",
    "    bag_of_abstracts += abstract[3]\n",
    "        \n",
    "print(type(bag_of_abstracts))\n",
    "#print(bag_of_abstracts[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-priority",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "### Prepping\n",
    "\n",
    "The bag of words approach starts by tokenizing that big string into word-tokens and then works to clean up those tokens by making them all lowercase, removing punctuation, removing whitespace, etc.\n",
    "\n",
    "This step also invloves removing stopwords, with an optional step of checking which words count as stop words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_token_word = word_tokenize(bag_of_abstracts)      # make all the words into tokens\n",
    "lower = [word.lower() for word in abstract_token_word]     # make those tokens lowercase\n",
    "no_punct = [w.translate(table_punctuation) for w in lower] # remove the punctuation\n",
    "no_space = (list(filter(lambda x: x, no_punct)))           # remove any extra whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stop_words))        # OPTIONAL: check what counts as a stopword if you want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = []\n",
    "\n",
    "for word in no_space:\n",
    "    if word not in stop_words:\n",
    "        no_stops.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-blind",
   "metadata": {},
   "source": [
    "### Consolidate\n",
    "\n",
    "The consolidation step is about trying to get as many versions of the words to be \"the same\" as reasonably possible. \n",
    "\n",
    "This means correcting spelling errors (optional), substituting synonyms (also optional) and stemming the words. \n",
    "\n",
    "#### Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "spell = SpellChecker()\n",
    "\n",
    "def reTokenize(doc):\n",
    "    tokens = WORD.findall(doc)\n",
    "    return tokens\n",
    "\n",
    "text = [\"Hi, welcmoe to speling.\",\"This is jsut an exapmle, but cosnider a veri big coprus.\"]\n",
    "\n",
    "def spell_correct(text):\n",
    "    sptext =  [' '.join([spell.correction(w).lower() for w in reTokenize(doc)])  for doc in text]    \n",
    "    return sptext    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in no_stops[:10]:\n",
    "    print(spell.correction(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs forever without finishing. Find alternative or skip it?\n",
    "correct_spell = []\n",
    "\n",
    "with open('..\\\\counts\\\\ESHG\\\\spell_checked.txt', \"w\", encoding = \"utf8\") as outfile:\n",
    "    for word in no_stops:\n",
    "        corrected = spell.correction(word)\n",
    "        outfile.write(corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-fireplace",
   "metadata": {},
   "source": [
    "#### Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in no_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-exclusion",
   "metadata": {},
   "source": [
    "### NLP analysis - count word frequencies\n",
    "\n",
    "This step actually does the NLP work for the 'bag-of-words' approach. here, we can find how many times the 35 most popular words occur and also find the exact occurence counts for select words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(stemmed)\n",
    "print(type(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.most_common(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts['autist'])\n",
    "print(counts['asd'])\n",
    "print(counts['asperg'])\n",
    "print(counts['autism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-basis",
   "metadata": {},
   "source": [
    "## Person-first and identity first\n",
    "\n",
    "This is the second NLP approach, this time taking the words in context instead of as isolated objects. \n",
    "\n",
    "It too has a prepping step, which consists of tokenising the actracts into sentences, selecting those sentences which contain the keywords of interest, and converting the select sentences into SpaCy objects (which also part-of-speech tags them and lemmatises them). \n",
    "\n",
    "### Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity = nltk.sent_tokenize(bag_of_abstracts)   # this creates a list of sentences\n",
    "#print(type(person_identity))\n",
    "#print(person_identity[:10])\n",
    "#print(len(person_identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_sentences = []\n",
    "\n",
    "for sentence in person_identity:\n",
    "    if any(s in sentence for s in ['autistic', 'Autistic', 'autism', 'Autism', 'ASD', 'asd', 'Asperger', 'asperger']):\n",
    "        #person += sentence\n",
    "        select_sentences.append(sentence)\n",
    "\n",
    "print(select_sentences[:10])\n",
    "print(len(select_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i_string = ' '                   # create a blank string to store the output\n",
    "for x in select_sentences:         # iterate over the list, appending each string to the previous\n",
    "    p_i_string += ' ' + x\n",
    "    \n",
    "#print(p_i_string[:100])           # optional check to see what the output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i_doc = nlp(p_i_string)         # convert the string to a SpaCy object, \n",
    "                                  # this also POS-tags and lemmatises the words as it goes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-clock",
   "metadata": {},
   "source": [
    "### Person first pattern\n",
    "\n",
    "The second NLP analysis - examining the context of words that fit certain patterns. In this case, this means identifying, counting and examening strings of words that match a person-first pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_2 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]utism$\"}}]\n",
    "\n",
    "pattern_3 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]sperger$\"}}]\n",
    "\n",
    "pattern_4 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^ASD$\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_1\", [pattern_2, pattern_3, pattern_4]) \n",
    "\n",
    "person_first =[]\n",
    "matches = matcher(p_i_doc) \n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = p_i_doc[start:end]  # The matched span\n",
    "    person_first.append(span.text)\n",
    "    \n",
    "#print(person_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f_lower = [word.lower() for word in person_first]     # make those tokens lowercase\n",
    "p_f_no_punct = [w.translate(table_punctuation) for w in p_f_lower] # remove the punctuation\n",
    "p_f_no_space = (list(filter(lambda x: x, p_f_no_punct)))           # remove any extra whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_first_no_dups = list(set(p_f_no_space))\n",
    "with open('..\\\\counts\\\\ESHG\\\\person_first.csv', \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in person_first_no_dups:\n",
    "            write.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(p_f_no_space))\n",
    "print(len(person_first_no_dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-desert",
   "metadata": {},
   "source": [
    "### Identity first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = [{\"TEXT\": {\"REGEX\": \"^[Aa]utistic\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_b = [{\"TEXT\": {\"REGEX\": \"^[Aa]sperger\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_c = [{\"TEXT\": {\"REGEX\": \"^ASD\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_1\", [pattern_a, pattern_b, pattern_c]) \n",
    "\n",
    "identity_first =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "matches = matcher(p_i_doc) \n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = p_i_doc[start:end]  # The matched span\n",
    "    identity_first.append(span.text)\n",
    "\n",
    "#print(matches[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_f_lower = [word.lower() for word in identity_first]     # make those tokens lowercase\n",
    "i_f_no_punct = [w.translate(table_punctuation) for w in i_f_lower] # remove the punctuation\n",
    "i_f_no_space = (list(filter(lambda x: x, i_f_no_punct)))           # remove any extra whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_first_no_dups = list(set(i_f_no_space))\n",
    "with open('..\\\\counts\\\\ESHG\\\\identity_first.csv', \"w\", encoding='utf8') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in identity_first_no_dups:\n",
    "            write.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(identity_first))\n",
    "print(len(identity_first_no_dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-adjustment",
   "metadata": {},
   "source": [
    "### Word counts by part of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_p_i = []\n",
    "\n",
    "for token in p_i_doc:\n",
    "    this_token = [token.text, token.lemma_, token.pos_, token.tag_]\n",
    "    if any (s in token.text for s in ['autistic', 'Autistic', 'autism', 'Autism', 'ASD', 'asd', 'Asperger', 'asperger']):\n",
    "        POS_p_i.append(this_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\counts\\\\ESHG\\\\POS.csv', \"w\", encoding='utf8') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in POS_p_i:\n",
    "            write.writerow([item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
