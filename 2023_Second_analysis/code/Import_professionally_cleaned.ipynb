{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Get ready\n",
    "\n",
    "First, download, import, prep packages and such. \n",
    "\n",
    "Then, check the file location and import the .csv files. Remove any with empty text fields. \n",
    "\n",
    "Save a data frame with all the texts and another with only those texts that mention the keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2004.csv', 'ESHG2001abstractICHG.csv', 'ESHG2002Abstracts.csv', 'ESHG2003Abstracts.csv', 'ESHG2005Abstracts.csv', 'ESHG2006Abstracts.csv', 'ESHG2007Abstracts.csv', 'ESHG2008Abstracts.csv', 'ESHG2009Abstracts.csv', 'ESHG2010Abstracts.csv', 'ESHG2011Abstracts.csv', 'ESHG2012Abstracts.csv', 'ESHG2013Abstracts.csv', 'ESHG2014Abstracts.csv', 'ESHG2015Abstracts.csv', 'ESHG2016Abstracts.csv', 'ESHG2017 electronic posters.csv', 'ESHG2017 oral presentation.csv', 'ESHG2018 electronic posters.csv', 'ESHG2018 oral presentation.csv', 'ESHG2019 electronic posters.csv', 'ESHG2019 oral presentation.csv', 'ESHG2020 electronic posters.csv', 'ESHG2020 oral presentation.csv', 'ESHG2021 electronic posters.csv', 'ESHG2021 oral presentation.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )\n",
    "\n",
    "files = []\n",
    "def import_results(input):\n",
    "    for f in os.listdir(input):\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')\n",
    "        files.append(f)\n",
    "    output = pd.concat(files)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\..\\\\2022_First_analysis\\\\for_analysis\\\\ESHG\\\\2004.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\"\\n\") for line in stripped if line)\n",
    "    with open('log.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(('title', 'intro'))\n",
    "        writer.writerows(lines)\n",
    "        \n",
    "        \n",
    "#read_file.to_csv (r'Path where the CSV will be saved\\File name.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = import_results(\"..\\\\results\")\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts_by_year = all_results['Year'].value_counts()\n",
    "all_counts_by_year = pd.DataFrame(all_counts_by_year)\n",
    "all_counts_by_year = all_counts_by_year.rename(columns={\"Year\": \"Counts\"})\n",
    "all_counts_by_year = all_counts_by_year.rename_axis('Year').reset_index()\n",
    "all_counts_by_year = all_counts_by_year.sort_values(by=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results[~all_results['Text'].isnull()]\n",
    "len(no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27593f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_counts_by_year = no_null_texts['Year'].value_counts()\n",
    "no_null_counts_by_year = pd.DataFrame(no_null_counts_by_year)\n",
    "no_null_counts_by_year = no_null_counts_by_year.rename(columns={\"Year\": \"All\"})\n",
    "no_null_counts_by_year = no_null_counts_by_year.rename_axis('Year').reset_index()\n",
    "no_null_counts = no_null_counts_by_year.sort_values(by=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_counts_by_year = matched_texts['Year'].value_counts()\n",
    "matched_counts_by_year = pd.DataFrame(matched_counts_by_year)\n",
    "matched_counts_by_year = matched_counts_by_year.rename(columns={\"Year\": \"Matches\"})\n",
    "matched_counts_by_year = matched_counts_by_year.rename_axis('Year').reset_index()\n",
    "matched_counts_by_year = matched_counts_by_year.sort_values(by=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85569dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year = no_null_counts_by_year.merge(matched_counts_by_year, on='Year', how='left')\n",
    "counts_year = counts_year.sort_values(by='Year')\n",
    "counts_year = counts_year.set_index('Year')\n",
    "counts_year.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "# Count word frequencies \n",
    "## Bag of words\n",
    "\n",
    "Proceed through the 'bag of words' steps for the data frames with all texts and then again for the data frame with only the texts that mention the keywords of interest. This approach finds word frequencies for all years together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):\n",
    "    holding_string = \"\"\n",
    "    for text in input['Text']:\n",
    "        holding_string += text\n",
    "    holding_string = word_tokenize(holding_string)\n",
    "    holding_string = [word.lower() for word in holding_string]\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]\n",
    "    holding_string = [porter.stem(token) for token in holding_string]\n",
    "    list_for_count = []\n",
    "    for token in holding_string:\n",
    "        list_for_count.append(token)\n",
    "    counts = Counter(list_for_count)\n",
    "    return counts.most_common(how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(no_null_texts, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(matched_texts, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292c709",
   "metadata": {},
   "source": [
    "### Word Frequency by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def track_word_over_time(input, target_word):\n",
    "    years = input['Year'].drop_duplicates()\n",
    "    target_counts = []\n",
    "    for year in years:\n",
    "        year_bag = \"\"\n",
    "        for text in input['Text'][input['Year']==year]:\n",
    "            year_bag += text\n",
    "        year_bag = word_tokenize(year_bag)\n",
    "        year_bag = [word.lower() for word in year_bag]\n",
    "        year_bag = [w.translate(table_punctuation) for w in year_bag]\n",
    "        year_bag = (list(filter(lambda x: x, year_bag)))\n",
    "        year_bag = [token for token in year_bag if not token.isdigit()]\n",
    "        year_bag = [token for token in year_bag if token not in stop_words]\n",
    "        year_bag = [porter.stem(token) for token in year_bag]\n",
    "        list_for_count = []\n",
    "        for token in year_bag:\n",
    "            list_for_count.append(token)\n",
    "        counts = Counter(list_for_count)\n",
    "        target_counts.append(counts[target_word])\n",
    "        \n",
    "    target_word_by_year = pd.DataFrame(list(zip(years, target_counts)), columns = ['Year', str(target_word)])\n",
    "    return target_word_by_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = track_word_over_time(no_null_texts, 'gene')\n",
    "autism = track_word_over_time(no_null_texts, 'autism')\n",
    "mutat = track_word_over_time(no_null_texts, 'mutat')\n",
    "\n",
    "target_words = gene.merge(autism, on='Year').merge(mutat, on='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = target_words.set_index('Year')\n",
    "target_words.plot.line()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseas = track_word_over_time(no_null_texts, 'diseas')\n",
    "disord = track_word_over_time(no_null_texts, 'disord')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc50968",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = track_word_over_time(no_null_texts, 'condit')\n",
    "syndrom = track_word_over_time(no_null_texts, 'syndrom')\n",
    "\n",
    "target_words_2 = diseas.merge(disord, on='Year').merge(syndrom, on='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words_2 = target_words.set_index('Year')\n",
    "target_words_2.plot.line()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e741a",
   "metadata": {},
   "source": [
    "### Word frequency by session code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01707a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912a2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256560e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0f63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
