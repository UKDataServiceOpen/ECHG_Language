{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Get ready\n",
    "\n",
    "First, download, import, prep packages and such. \n",
    "\n",
    "Then, check the file location and import the .csv files. Remove any with empty text fields. \n",
    "\n",
    "Save a data frame with all the texts and another with only those texts that mention the keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize  \n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )\n",
    "\n",
    "files = []\n",
    "def import_results(input):\n",
    "    for f in os.listdir(input):\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')\n",
    "        files.append(f)\n",
    "    output = pd.concat(files)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = import_results(\"..\\\\results\")\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_04 = pd.read_csv('..\\\\results\\\\ESHG2004.csv')\n",
    "year_04 = year_04.iloc[:, [0,1]]\n",
    "year_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, year_04])\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results[~all_results['Text'].isnull()]\n",
    "len(no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d89f25",
   "metadata": {},
   "source": [
    "# Sent tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17421e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = [sent_tokenize(abstract) for abstract in matched_texts['Text'] ] # make tokenized sentences list\n",
    "matched_texts['Sentence'] = sentences                                         # copy that list back into df as a new column\n",
    "matched_texts = matched_texts.explode('Sentence')                             # explode to create 1 row per sentence token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = matched_texts[matched_texts['Sentence'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "                                                                            # keep only those rows that contain the keywords\n",
    "matched_sents = matched_sents.drop_duplicates()                             # drop any duplicates\n",
    "len(matched_sents)                                                          # check length of remaining data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0de07",
   "metadata": {},
   "source": [
    "## Person-first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d739a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_2 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]utism$\"}}]\n",
    "\n",
    "pattern_3 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]sperger$\"}}]\n",
    "\n",
    "pattern_4 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^ASD$\"}}]\n",
    "\n",
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_1\", [pattern_2, pattern_3, pattern_4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern1_match(input):\n",
    "    thingy = nlp(input)\n",
    "    match = matcher(thingy)\n",
    "    if match == []:\n",
    "        out_value = ''\n",
    "    else:\n",
    "        hold_multi_spans = []\n",
    "        for match_id, start, end in match:\n",
    "                string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                span = thingy[start:end]  # The matched span\n",
    "                hold_multi_spans.append(span)\n",
    "        out_value = hold_multi_spans\n",
    "    return out_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents['Person-first'] = matched_sents.apply(lambda row: find_pattern1_match(row.Sentence), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc982",
   "metadata": {},
   "source": [
    "## Identity first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = [{\"TEXT\": {\"REGEX\": \"^[Aa]utistic\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_b = [{\"TEXT\": {\"REGEX\": \"^[Aa]sperger\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_c = [{\"TEXT\": {\"REGEX\": \"^ASD\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_2\", [pattern_a, pattern_b, pattern_c]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents['Identity-first'] = matched_sents.apply(lambda row: find_pattern1_match(row.Sentence), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58536fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = matched_sents.explode('Person-first')                             # explode to create 1 row per sentence token\n",
    "len(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80451426",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = matched_sents.explode('Identity-first')                             # explode to create 1 row per sentence token\n",
    "len(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d91446",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = matched_sents[(matched_sents['Person-first'] != '') | (matched_sents['Identity-first'] != '')]\n",
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcca95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('..\\\\output\\\\text_match_results.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73bc9c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2023_Second_analysis\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)  )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\..\\\\2023_Second_analysis\\\\output\")  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2281a",
   "metadata": {},
   "source": [
    "## Chart person first or identity first by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_first = pd.read_csv('..\\\\..\\\\2023_Second_analysis\\\\output\\\\text_match_results.csv')\n",
    "person_identity_first = person_identity_first.dropna(how='all')\n",
    "person_identity_first['Year'] = person_identity_first['Year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_count = person_identity_first.groupby(['Year'])['Person-first'].count()\n",
    "identity_count = person_identity_first.groupby(['Year'])['Identity-first'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count=pd.concat([person_count,identity_count],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_examples = person_identity_first.groupby(['Person-first'])['Person-first'].count()\n",
    "identity_examples = person_identity_first.groupby(['Identity-first'])['Identity-first'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1443d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples=pd.concat([person_examples,identity_examples],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6184b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f05d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_person = person_identity_first[~person_identity_first['Person-first'].isnull()]\n",
    "len(has_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_identity = person_identity_first[~person_identity_first['Identity-first'].isnull()]\n",
    "len(has_identity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82b88",
   "metadata": {},
   "source": [
    "## Count abstracts by the structures they use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd85bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_by_title = person_identity_first.groupby(['Title'])['Person-first'].count()\n",
    "identity_by_title = person_identity_first.groupby(['Title'])['Identity-first'].count()\n",
    "title = pd.concat([person_by_title,identity_by_title],axis=1)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f461ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Person-first','Identity-first']\n",
    "filter_ = (title[columns] > 0).all(axis=1)\n",
    "title[filter_]\n",
    "len(title[filter_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13737d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e03f1",
   "metadata": {},
   "source": [
    "## Word counts by part of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1452278",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_p_i = []\n",
    "\n",
    "for token in p_i_doc:\n",
    "    this_token = [token.text, token.lemma_, token.pos_, token.tag_]\n",
    "    if any (s in token.text for s in ['autistic', 'Autistic', 'autism', 'Autism', 'ASD', 'asd', 'Asperger', 'asperger']):\n",
    "        POS_p_i.append(this_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175766b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\counts\\\\ESHG\\\\POS.csv', \"w\", encoding='utf8') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in POS_p_i:\n",
    "            write.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918896cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f_lower = [word.lower() for word in person_first]     # make those tokens lowercase\n",
    "p_f_no_punct = [w.translate(table_punctuation) for w in p_f_lower] # remove the punctuation\n",
    "p_f_no_space = (list(filter(lambda x: x, p_f_no_punct)))           # remove any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving output\n",
    "os.makedirs('folder/subfolder', exist_ok=True)  \n",
    "df.to_csv('folder/subfolder/out.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
