{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Get ready\n",
    "\n",
    "First, download, import, prep packages and such. \n",
    "\n",
    "Then, check the file location and import the .csv files. Remove any with empty text fields. \n",
    "\n",
    "Save a data frame with all the texts and another with only those texts that mention the keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize  \n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')            # one for all of the texts and then\n",
    "matched_texts = pd.read_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')    # one for just those that match the keyword                            # check the length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10626120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results['Year'].drop_duplicates())     # quick check shows that 2004 (a known problem file) has not imported properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc45084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results['Year'].isna().sum())                         # Let's just count how many rows NaN instead of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_Nan_in_Year = all_results[~all_results['Year'].isnull()]          # remove the 'Year' = Nan rows\n",
    "len(no_Nan_in_Year)                                                  # check length again  - do the numbers make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_04 = pd.read_csv('..\\\\results\\\\ESHG2004.csv')      # specifically read in year 2004 (it needed a bit of extra work)\n",
    "year_04 = year_04.iloc[:, [0,1]]                        # cut a two-column slice out of it with only the year and text\n",
    "year_04                                                 # check how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_corrected = pd.concat([no_Nan_in_Year, year_04])     # add in those 2004 rows\n",
    "len(all_results_corrected)                                       # check length again - are we back up to where we started?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results_corrected[~all_results_corrected['Text'].isnull()] \n",
    "                                                                    # remove any rows where the 'text' column is empty\n",
    "len(no_null_texts)                                                  # check length again - still making sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "                                                                   # keep only rows where text contains a keyword of interest\n",
    "len(matched_texts)                                                 # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts = matched_texts.dropna(axis=1, how=\"all\")   # remove all columns which contain only NaNs\n",
    "print(len(no_nans_matched_texts))                                 # just check the length has not changed\n",
    "no_nans_matched_texts                                             # have a look at the columns that remain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d89f25",
   "metadata": {},
   "source": [
    "# Sent tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17421e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = [sent_tokenize(abstract) for abstract in no_nans_matched_texts['Text'] ] \n",
    "                                                                    # tokenize the text column and store as a list\n",
    "no_nans_matched_texts['Sentence'] = sentences                       # copy that list back into df as a new column\n",
    "sentence_per_row = no_nans_matched_texts.explode('Sentence')        # explode the new column to create 1 row per sentence token\n",
    "len(sentence_per_row)                                               # check the length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_per_row                                                    # have a look. For the first two rows, \n",
    "                                                                    # 'Text' should be same, but 'Sentence' should not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = sentence_per_row[sentence_per_row['Sentence'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "                                                                            # keep only those sentences that contain the keywords\n",
    "len(matched_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = matched_sentences.drop_duplicates()                         # drop any duplicates\n",
    "len(matched_sentences)                                                          # check length of remaining data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0de07",
   "metadata": {},
   "source": [
    "## Person-first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d739a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_2 = [{\"POS\": \"NOUN\"},                                                   # define the person-first pattern(s)\n",
    "             {'LOWER': 'with'},                                                 # I made 3 for clarity rather than one with \n",
    "             {'DEP':'amod', 'OP':\"?\"},                                          # a real complex regex string\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]utism$\"}}]\n",
    "\n",
    "pattern_3 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^[Aa]sperger$\"}}]\n",
    "\n",
    "pattern_4 = [{\"POS\": \"NOUN\"},\n",
    "             {'LOWER': 'with'},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"^ASD$\"}}]\n",
    "\n",
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab)                                                  # define a matcher class object\n",
    "matcher.add(\"matching_1\", [pattern_2, pattern_3, pattern_4])                  # add my three person-first patterns to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern_match(input):                                               # define a function that applies the person-first\n",
    "    thingy = nlp(input)                                                       # matcher class object to strings\n",
    "    match = matcher(thingy)                                                   # and returns any matches to the pattern(s)\n",
    "    if match == []:\n",
    "        out_value = ''\n",
    "    else:\n",
    "        hold_multi_spans = []\n",
    "        for match_id, start, end in match:\n",
    "                string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                span = thingy[start:end]  # The matched span\n",
    "                hold_multi_spans.append(span)\n",
    "        out_value = hold_multi_spans\n",
    "    return out_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences['Person-first'] = matched_sentences.apply(lambda row: find_pattern_match(row.Sentence), axis = 1)\n",
    "                                                                        # apply the newly defined person-first matcher function\n",
    "                                                                        # and store the returned output in a new column\n",
    "len(matched_sentences)                                                  # double check length remains same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc982",
   "metadata": {},
   "source": [
    "## Identity first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = [{\"TEXT\": {\"REGEX\": \"^[Aa]utistic\"}},                        # do the same for identity-first patterns\n",
    "             {'DEP':'amod', 'OP':\"?\"},                                   # again, i wrote 3 patterns for clarity sake\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_b = [{\"TEXT\": {\"REGEX\": \"^[Aa]sperger\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "pattern_c = [{\"TEXT\": {\"REGEX\": \"^ASD\"}},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Matcher class object                                         \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_2\", [pattern_a, pattern_b, pattern_c])            # this overwrites the matcher object to identity-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences['Identity-first'] = matched_sentences.apply(lambda row: find_pattern_match(row.Sentence), axis = 1)\n",
    "                                                                        # apply the newly overwritten matcher function\n",
    "                                                                        # and store the returned output in a new column\n",
    "len(matched_sentences)                                                  # check the length - why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d91446",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_sentences[(matched_sentences['Person-first'] != '') | (matched_sentences['Identity-first'] != '')]\n",
    "                                                     # keep only rows w/ non-null 'Person-first' and/or 'Identity-first' columns\n",
    "len(matched_patterns)                                # check length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58536fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_patterns.explode('Person-first')    # explode 'Person-first' column to create 1 row per match\n",
    "len(matched_patterns)                                          # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80451426",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_patterns.explode('Identity-first')  # Do the same for 'Identity-first' column\n",
    "len(matched_patterns)                                          # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []                                                        # create empty list to hold names of files in 'results'\n",
    "def import_results(input):                                        # create a function import the contents of the\n",
    "    for f in os.listdir(input):                                   # folder named in the function input\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')        # by reading them in as csv files, one by one\n",
    "        files.append(f)                                           # appending the newly read csv file to a temporary list\n",
    "    output = pd.concat(files)                                     # then concatenating that temp list to the pre-defined list\n",
    "    return output         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcca95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lem = WordNetLemmatizer()\n",
    "person_lemma_list = []\n",
    "lemmatized = []\n",
    "for phrase in matched_patterns['Person-first']:\n",
    "    x = str(phrase)\n",
    "    words = x.split()\n",
    "    for word in words :\n",
    "        lemword = Lem.lemmatize(word)\n",
    "        lemmatized.append(lemword)\n",
    "    person_lemma_list.append(lemmatized)\n",
    "    lemmatized = []\n",
    "\n",
    "person_lemma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69719ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_lemma_list = []\n",
    "lemmatized = []\n",
    "for phrase in matched_patterns['Identity-first']:\n",
    "    x = str(phrase)\n",
    "    words = x.split()\n",
    "    for word in words :\n",
    "        lemword = Lem.lemmatize(word)\n",
    "        lemmatized.append(lemword)\n",
    "    identity_lemma_list.append(lemmatized)\n",
    "    lemmatized = []\n",
    "\n",
    "identity_lemma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns['Person-first_lemmatized'] = person_lemma_list\n",
    "matched_patterns['Identy-first_lemmatized'] = identity_lemma_list\n",
    "matched_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('..\\\\output\\\\text_match_results.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\..\\\\2023_Second_analysis\\\\output\")  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.loc[473]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2281a",
   "metadata": {},
   "source": [
    "## Chart person first or identity first by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_first = pd.read_csv('..\\\\..\\\\2023_Second_analysis\\\\output\\\\text_match_results.csv')\n",
    "person_identity_first = person_identity_first.dropna(how='all')\n",
    "person_identity_first['Year'] = person_identity_first['Year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_count = person_identity_first.groupby(['Year'])['Person-first'].count()\n",
    "identity_count = person_identity_first.groupby(['Year'])['Identity-first'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count=pd.concat([person_count,identity_count],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_examples = person_identity_first.groupby(['Person-first'])['Person-first'].count()\n",
    "identity_examples = person_identity_first.groupby(['Identity-first'])['Identity-first'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1443d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples=pd.concat([person_examples,identity_examples],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6184b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f05d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_person = person_identity_first[~person_identity_first['Person-first'].isnull()]\n",
    "len(has_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_identity = person_identity_first[~person_identity_first['Identity-first'].isnull()]\n",
    "len(has_identity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82b88",
   "metadata": {},
   "source": [
    "## Count abstracts by the structures they use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd85bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_by_title = person_identity_first.groupby(['Title'])['Person-first'].count()\n",
    "identity_by_title = person_identity_first.groupby(['Title'])['Identity-first'].count()\n",
    "title = pd.concat([person_by_title,identity_by_title],axis=1)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f461ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Person-first','Identity-first']\n",
    "filter_ = (title[columns] > 0).all(axis=1)\n",
    "title[filter_]\n",
    "len(title[filter_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13737d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e03f1",
   "metadata": {},
   "source": [
    "## Word counts by part of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1452278",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_p_i = []\n",
    "\n",
    "for token in p_i_doc:\n",
    "    this_token = [token.text, token.lemma_, token.pos_, token.tag_]\n",
    "    if any (s in token.text for s in ['autistic', 'Autistic', 'autism', 'Autism', 'ASD', 'asd', 'Asperger', 'asperger']):\n",
    "        POS_p_i.append(this_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175766b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\counts\\\\ESHG\\\\POS.csv', \"w\", encoding='utf8') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        for item in POS_p_i:\n",
    "            write.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918896cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f_lower = [word.lower() for word in person_first]     # make those tokens lowercase\n",
    "p_f_no_punct = [w.translate(table_punctuation) for w in p_f_lower] # remove the punctuation\n",
    "p_f_no_space = (list(filter(lambda x: x, p_f_no_punct)))           # remove any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving output\n",
    "os.makedirs('folder/subfolder', exist_ok=True)  \n",
    "df.to_csv('folder/subfolder/out.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
