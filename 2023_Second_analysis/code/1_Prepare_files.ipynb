{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46e5d4f",
   "metadata": {},
   "source": [
    "# Step 1 - Prepare\n",
    "\n",
    "Admittedly, quite a bit of work has already taken place. The .pdf files were sent off to a research support team that scraped the text out of them and stored the contents of those files in .csv files which have useful headers. Each .pdf file was processed to produce a single .csv file. This processing was not universally successful because the original .pdf files were not all encoded in the same format and most had a totally unique layout. In effect, this means that each .pdf to .csv process was not equally able to automatically capture and segment the text in the .pdf into .csv columns such as author, affiliation, session code, etc. This means that some of the .csv files have empty columns while other .csv files have contents written in the same columns. This also means that some of the rows are probably faulty. Obviously, a detailed manual inspection would correct some of these errors but total accuracy is not the point of this research effort. \n",
    "\n",
    "The file for 2004 was particularly tricky and needs separate attention because the output from the research support team was not structured in a way that matches the others very well. \n",
    "\n",
    "Nevertheless, we have all the files in a somewhat useful way that allows us to use natural language processing methods to investigate person-first and identity-first language. The next step is to import the various .csv files, consolidate them in one data frame, tidy up some of the erroneous rows and columns and then same the output in a new .csv. \n",
    "\n",
    "## Get ready\n",
    "\n",
    "All of my jupyter notebooks begin with some code cells focussed on downloading/importing necessary packages, loading useful short names, and so forth. \n",
    "\n",
    "I also like to check the relevant file locations before importing the .csv files to work on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture                         \n",
    "                                  # The above capture statement is optional. \n",
    "                                  # You can remove this to see the chatter normally produced during import steps. \n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "\n",
    "import pandas as pd               # pandas is necessary for working with data frames - shortening it to pd just saves time. \n",
    "pd.set_option('display.max_colwidth', 200)   # some of the files are big so set a big column width. \n",
    "import numpy as np                # like pandas, numpy is useful and useful to have a short name for\n",
    "import statistics                 # gotsta have stats\n",
    "\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "import re                         # things we need for RegEx corrections\n",
    "import string \n",
    "import math \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88df952",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having got all the packages we need and having checked the files, let's import them. This requires:\n",
    "* defining a function to import multiple files from a known location (better than one-by-one importing!)\n",
    "* checking the output of the mass-import for length and contents (since I suspect 2004 may not have worked correctly)\n",
    "* having found an error, investigating it a bit\n",
    "* correcting the error by removing problem rows, manually incorporating better rows, adding correct rows back in and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da8258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018 EMPAG.csv', '2018 posters.csv', '2019 posters2.csv', '2020 eposters.csv', '2020 interactive eposter.csv', '2021 eposters.csv', 'ESHG2001abstractICHG.csv', 'ESHG2002Abstracts.csv', 'ESHG2003Abstracts.csv', 'ESHG2004.csv', 'ESHG2005Abstracts.csv', 'ESHG2006Abstracts.csv', 'ESHG2007Abstracts.csv', 'ESHG2008Abstracts.csv', 'ESHG2009Abstracts.csv', 'ESHG2010Abstracts.csv', 'ESHG2011Abstracts.csv', 'ESHG2012Abstracts.csv', 'ESHG2013Abstracts.csv', 'ESHG2014Abstracts.csv', 'ESHG2015Abstracts.csv', 'ESHG2016Abstracts.csv', 'ESHG2017 electronic posters.csv', 'ESHG2017 oral presentation.csv', 'ESHG2017posters.csv', 'ESHG2018 electronic posters.csv', 'ESHG2018 oral presentation.csv', 'ESHG2019 electronic posters.csv', 'ESHG2019 oral presentation.csv', 'ESHG2020 electronic posters.csv', 'ESHG2020 oral presentation.csv', 'ESHG2021 electronic posters.csv', 'ESHG2021 oral presentation.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c6ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []                                                        # create empty list to hold names of files in 'results'\n",
    "def import_results(input):                                        # create a function import the contents of the\n",
    "    for f in os.listdir(input):                                   # folder named in the function input\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')        # by reading them in as csv files, one by one\n",
    "        files.append(f)                                           # appending the newly read csv file to a temporary list\n",
    "    output = pd.concat(files)                                     # then concatenating that temp list to the pre-defined list\n",
    "    return output                                                 # returning the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05b6499",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\results\\\\2018.EMPAG.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m2018.EMPAG.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m f\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\results\\\\2018.EMPAG.csv'"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(\"..\\\\results\\\\2018.EMPAG.csv\",encoding='latin1')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = import_results(\"..\\\\results\")      # run the newly defined function on the 'results' folder\n",
    "how_many_total = len(all_results)                # check the length \n",
    "how_many_total                                   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10626120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2018.0\n",
      "0    2019.0\n",
      "0    2020.0\n",
      "0    2021.0\n",
      "0    2001.0\n",
      "0    2002.0\n",
      "0    2003.0\n",
      "0       NaN\n",
      "0    2005.0\n",
      "0    2006.0\n",
      "0    2007.0\n",
      "0    2008.0\n",
      "0    2009.0\n",
      "0    2010.0\n",
      "0    2011.0\n",
      "0    2012.0\n",
      "0    2013.0\n",
      "0    2014.0\n",
      "0    2015.0\n",
      "0    2016.0\n",
      "0    2017.0\n",
      "Name: Year, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(all_results['Year'].drop_duplicates())     # quick check shows that 2004 (a known problem file) has not imported properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc45084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "how_many_no_year = all_results['Year'].isna().sum() # Let's just count how many rows NaN instead of the year\n",
    "how_many_no_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571d6639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36674"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_Nan_in_Year = all_results[~all_results['Year'].isnull()]          # remove the 'Year' = Nan rows\n",
    "how_many_without_Nan_year = len(no_Nan_in_Year)                      # check length again now that Nan rows are nemoved\n",
    "how_many_without_Nan_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (how_many_total - how_many_no_year )                   # print the total number minus the number that are missing a year\n",
    "print (how_many_without_Nan_year)                            # compare to the new total just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_04 = pd.read_csv('..\\\\results\\\\ESHG2004.csv')      # specifically read in year 2004 (it needed a bit of extra work)\n",
    "year_04 = year_04.iloc[:, [0,1]]                        # cut a two-column slice out of it with only the year and text\n",
    "year_04                                                 # check how it looks (which also shows us how many rows are in it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_corrected = pd.concat([no_Nan_in_Year, year_04])     # add those specially imported 2004 rows back into the output\n",
    "                                                                 # which had the weird no-year rows removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7790cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_total_new = len(all_results_corrected)                  # check length again - are we back up to where we started?\n",
    "\n",
    "if how_many_total == how_many_total_new :                        # write a quick check to be sure the totals before and after\n",
    "    print('The numbers add up!')                                 # removing/replacing the 2004 rows are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0560fad",
   "metadata": {},
   "source": [
    "## Remove rows that imported correctly for reasons unrelated to 2004. \n",
    "\n",
    "Having imported all the various .csv files and storing them in one data frame (even that tricksy 2004 .csv) I do a bit of clean up. Turning .pdf files to .csv is not a straightforward or fool proof process, so I want to remove any rows that have nothing in the 'Text' column and check the length again to see how many we have lost. Then I want to remove any columns that are entirely empty (which is probably the result of badly imported rows that are shifted over) and have a quick look at the remaining columns and what might be in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results_corrected[~all_results_corrected['Text'].isnull()] \n",
    "                                                                    # remove any rows where the 'text' column is empty\n",
    "how_many_no_null_texts = len(no_null_texts)                         # check length again - still making sense?\n",
    "print (how_many_no_null_texts)\n",
    "print (how_many_total_new - how_many_no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = no_null_texts.dropna(axis=1, how=\"all\")   # remove all columns which contain only NaNs\n",
    "print(len(no_null_texts))                                 # just check the length has not changed\n",
    "no_null_texts                                             # have a nosy at which columns remain, what it is them, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d103a0",
   "metadata": {},
   "source": [
    "## Clean up the 'Text' column a little bit\n",
    "\n",
    "It was not obvious what text cleaning steps were most valuable in the first passes through the data. However, having run through later steps several times, I have concluded that this is the best place to insert a few steps relating to tidying up the texts. The first tidying step does basic things like removing multiple sequential whitespaces and inserting spaces when two sentences have been jammed together. \n",
    "\n",
    "Following that, the second tiyding step cleans up some problems with the keywords of interest, some of which were only noted in very late stages of processing but which are perhaps useful to correct early on. This second tidying step also consolidates on a small number of ways that the keywords of interest are often used. For example, this would turn 'Asperger's syndrome', 'Asperger syndrome disorder', 'autistic spectrum disorder' and 'autism spectrum disorder' into 'ASD'.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spacing_errors (input):\n",
    "    no_extra_spaces = re.sub(r'(\\s)(\\s+)', r'\\1', input)                        # turn 2+ sequential whitespaces into 1\n",
    "    no_run_ons1 = re.sub(r'([a-z][.|?|;])([A-Z])', r'\\1 \\2', no_extra_spaces)   # removes run-ons (e.g. \"word.New sentence \")\n",
    "    no_run_ons2 = re.sub(r'([A-Z][.|?|;])([A-Z])', r'\\1 \\2', no_run_ons1)       # removes run-ons (e.g. \"ACRONYM.New sentence \")\n",
    "    space1 = re.sub(r'([a-z]+)(disorder|disability|spectrum|disease)', r'\\1 \\2', no_run_ons2) # adds a space in select run-ons\n",
    "    space2 = re.sub(r'(spec) (trum)', r'\\1\\2', space1)                          # removes a space between 'spec' and 'trum'\n",
    "    space3 = re.sub(r'(psychopa) (thy)', r'\\1\\2', space2)                          # removes a space between 'spec' and 'trum'\n",
    "\n",
    "    return(space3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e357be",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Optional cell code block to test or understand what the remove_spacing_errors function does\n",
    "    \n",
    "spacing_error_test = \"word.New sentence   extra spaces    test. \\\n",
    "            ACRONYM.New sentence    \\\n",
    "            Asperger'sdisorder sdisorder Autisticdisability autismspectrum \\\n",
    "            autismdisease spec trum \"\n",
    "\n",
    "remove_spacing_errors(spacing_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43395654",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_run_ons = [remove_spacing_errors(abstract) for abstract in no_null_texts['Text'] ] \n",
    "                                             # create list of texts without extra spaces/run-on erors \n",
    "                                             # this is to improve sentence tokenisation later \n",
    "no_null_texts['Text'] = no_run_ons           # Overwrite the 'Text' column with the new no extra space/run-on abstract list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa834092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_up_terminology (input):\n",
    "    no_apost = re.sub(r'([Aa]sperger)(\\'?)(s?)', r'asperger', input)   # Standardises case and apostrophe in '[Aa]sperger's'\n",
    "    lower1 = re.sub(r'[Ss]pectrums|[Ss]pectra|[Ss]pectrum', r'spectrum', no_apost) # lowercases / removes plurals for spectrum\n",
    "    lower2 = re.sub(r'[Ss]yndromes|[Ss]yndrome', r'syndrome', lower1)              # lowercases / removes plurals for syndrome\n",
    "    lower3 = re.sub(r'[Dd]isorders|Disorder', r'disorder', lower2)                 # lowercases / removes plurals for disorder\n",
    "    lower4 = re.sub(r'[Dd]iseases|Disease', r'disease', lower3)                    # lowercases / removes plurals for disease\n",
    "    lower5 = re.sub(r'[Aa]utis', r'autis', lower4)                                  # lowercases 'Autism' and 'Autistic'\n",
    "    plur = re.sub(r'ASD([\\'*?])(s?)', r'ASD', lower5)                            # removes plural for more than one ASD\n",
    "    aut0 = re.sub(r'(asperger )(autis)', r'\\2', plur)                  # removes '[Aa]sperger' from '[Aa]sperger' autis'\n",
    "    aut1 = re.sub(r'(autistic )(psychopathy)', r'autism', aut0)           # standardises 'autistic psychopathy' to 'autism'\n",
    "    stan0 = re.sub(r'(autism|autistic|asperger) syndrome', r'autism', aut1 )       # turns select 'syndrome' to 'spectrum'\n",
    "    stan1 = re.sub(r'(autism|autistic|asperger) spectrum', r'autism', stan0)       # turns select 'spectrum' to 'autism'\n",
    "    stan2 = re.sub(r'(autism|autistic|asperger) disease', r'autism', stan1)        # turns select 'disease' to 'autism'\n",
    "    stan3 = re.sub(r'(autism|autistic|asperger) disability', r'autism', stan2)     # turns select 'disability' to 'autism'\n",
    "    stan4 = re.sub(r'(autism|autistic|asperger) disorder', r'autism', stan3)       # turns select 'disorder' to 'autism'\n",
    "    stan5 = re.sub(r'(autism|autistic|asperger) \\(ASD\\)', r'autism', stan4)     # turns select former abbreviations to 'autism'\n",
    "    stan6 = re.sub(r'(autism|autistic|asperger) \\(AS\\)', r'autism', stan5)      # turns select former abbreviations to 'autism'\n",
    "    stan7 = re.sub(r'AS ', r'ASD ', stan6)                              # standardises 'AS ' to 'ASD ' - note trailing space\n",
    "\n",
    "    return(stan7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b747c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Optional cell code block to test or understand what the tidy_up_terminology function does\n",
    "    \n",
    "tidy_test = \"1 Aspergers Asperger's Asperger Spectrum \\\n",
    "            2 Spectra Syndrome Syndrome \\\n",
    "            3 Disorders disorders Diseases disease ASDs ASD's \\\n",
    "            4 asperger's autism autistic psychopathy\\\n",
    "            5 autistic syndrome \\\n",
    "            6 autism disease \\\n",
    "            7 autistic disability  autism spectrum (AS)\\\n",
    "            8 asperger disability autistic disease \\\n",
    "            9 autism spectrum disorder (ASD) \\\n",
    "            10 autism spectrum (ASD) \\\n",
    "            11 autistic spectrum disease (ASD) \\\n",
    "            12 autism (ASD) \\\n",
    "            13 Asperger syndrome (AS)\\\n",
    "            14 autistic spectrum \"\n",
    "\n",
    "tidy_up_terminology(tidy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_text = [tidy_up_terminology(abstract) for abstract in no_null_texts['Text'] ] \n",
    "                                             # create abstract list that has the terminology tidied and standardised\n",
    "                                             # this is to improve pattern recognition later\n",
    "no_null_texts['Text'] = tidy_text            # Over-write the 'Text' column with the tidied/standardised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = no_null_texts         # A backup may be useful at this step if you want to adjust/test the tidy functions more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31104",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = backup          # If you need the backup, re-run this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce6eab",
   "metadata": {},
   "source": [
    "## Save the consolidated output as .csv\n",
    "\n",
    "Having imported, consolidated, tidied and checked everything, I want to save the output in a new .csv file. It is important to use a good name for the file, because bad file names are the bane of my existance. \n",
    "\n",
    "For simplicity sake, I will also create a new data frame containing only those rows for which the 'Text' column contains one of the keywords of interest, check its length and save it as a new .csv file with a good name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_null_texts)                          # Let's just double check what kind of a thing 'no_null_texts' is\n",
    "                                             # This lets us know what kind of write-out-to-csv function we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts.to_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')  # write out the data frame to a .csv, with a useful name\n",
    "                                                                     # which clarifies this is ALL abstracts with non-null texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts = no_null_texts[no_null_texts['Text'].str.contains('[Aa]utis|ASD|AS|[Aa]sperger')]\n",
    "                                                         # keep only rows where text contains one or more original keywords\n",
    "len(no_nans_matched_texts)                               # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts.to_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')  # write out the matched texts df to a .csv too\n",
    "                                                                                 # again, with a clear and useful name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0ce19",
   "metadata": {},
   "source": [
    "## Manually check the saved .csv files\n",
    "\n",
    "You may want to go and check that the two files you have created here have been created and saved correctly. You may even want to open them up and have a nosy through them to see what they look like. \n",
    "\n",
    "The next notebook picks up where this leaves off, by importing those files and working with them to produce some stats that help explore the research question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
