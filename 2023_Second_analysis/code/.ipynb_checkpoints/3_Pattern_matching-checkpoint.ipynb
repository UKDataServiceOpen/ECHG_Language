{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Pattern matching\n",
    "\n",
    "\n",
    "We will start in the same way as the last notebook started  - by downloading/importing the packages needed and importing the .csv file(s) needed. In this case, we only need the .csv file that has the matched abstracts as we are specifically looking at person-first and identity-first patterns that are \"about\" autism (or ASD, Asperger's syndrome, etc.). \n",
    "\n",
    "We could use the same basic approach to look at person-first and identity-first language for other conditions for which there are good noun and adjective forms of the words (diabetes? obesity? cancer? something else?). Doing that would mean using the .csv file with all of the abstracts or potentially creating and entirely new file of abstracts matched to another condition of interest. However, that lies outside the scope of this research, so I will not address it further here. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize  \n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import string \n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e4ea5",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import the specific file of interest for this step of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = pd.read_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')    # one for just those that match the keyword\n",
    "len(matched_texts)                                                                # check the length "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d89f25",
   "metadata": {},
   "source": [
    "## Cleaning phase\n",
    "\n",
    "Cleaning begins by turning any instances of extra whitespaces (two or more in a row) into a single whitespace. Then, identifying any run-on sentences (where a lowercase letter, a full stop, and an uppercase letter are clustered without a whitespace) and inserting a whitespace between the full stop and the uppercase letter. Both of these steps will improve the sentence tokenisation that happens next. \n",
    "\n",
    "Then, we proceed to sentence tokenising the text. Like word tokens, sentence tokens become the unit for analysisis. As a trivial example, sentence tokenisation would turn a short text such as \n",
    "\n",
    "\n",
    "''' The cat named Cat is one of five cats. Honestly, I wonder why I have so many cats.\n",
    "''' \n",
    "\n",
    "into a list of sentence tokens like\n",
    "\n",
    "''' [[The cat named Cat is one of five cats.]\n",
    "\n",
    "[Honestly, I wonder why I have so many cats.]]\n",
    "\n",
    "''' \n",
    "\n",
    "An important difference is that the punctuation within the sentences that contributes to its structured and meaning (e.g. the comma and the full stops) are retained. This punctuation, like the capitalisation at the start of the sentences or for the poper nouns, is also retained as it helps the sentence-tokenisation process identify the words within the sentence correctly for their parts of speech (e.g. which of the words are nouns, verbs, etc. ). \n",
    "\n",
    "\n",
    "\n",
    "The sentence tokens are then put on individual rows, filtered to retain only those that contain one or more of the keywords of interest, and then filtered to ensure that there are no empty rows or duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfa42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"Testing.Run-on removal.   Extra whitespace removal. \"\n",
    "\n",
    "def remove_errors (input):\n",
    "    no_extra_spaces = re.sub(r'(\\s)(\\s+)', r'\\1', input)               # turn 2+ sequential whitespaces into 1\n",
    "    no_run_ons = re.sub(r'([a-z].)([A-Z])', r'\\1 \\2', no_extra_spaces) # identifies run-ons (e.g. \"word.New sentence \")\n",
    "\n",
    "    return(no_run_ons)\n",
    "\n",
    "remove_errors(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_run_ons = [remove_errors(abstract) for abstract in matched_texts['Text'] ] \n",
    "                                             # create abstract list without extra spaces/run-ons \n",
    "                                             # this is to improve sentence tokenisation later \n",
    "matched_texts['Sentence'] = no_run_ons       # copy the no extra space/run-on abstract list back into df as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17421e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = [sent_tokenize(abstract) for abstract in matched_texts['Sentence'] ] # create tokenised list of cleaner abstracts\n",
    "matched_texts['Sentence'] = sentences                                   # copy that list back into df as a new column\n",
    "sentence_per_row = matched_texts.explode('Sentence')                    # explode column in new df with 1 row/sentence token\n",
    "len(sentence_per_row)                                                   # check the length of new df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_per_row                                                    # have a look. For the first two rows, \n",
    "                                                                    # 'Text' should be same, but 'Sentence' should not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = sentence_per_row[sentence_per_row['Sentence'].str.contains('[Aa]utis|ASD|[Aa]sperger')]\n",
    "                                                     # create a new data frame with only the sentences that contain keywords\n",
    "len(matched_sentences)                               # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = matched_sentences[~matched_sentences['Sentence'].isnull()]  # remove any rows with empty 'Sentence' column\n",
    "matched_sentences = matched_sentences.drop_duplicates()                         # drop any duplicates\n",
    "len(matched_sentences)                                                          # check length of remaining data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_test = \"Autism spectrum intellectual disability and autism ID, ASD Autisticspectrum autisticspectrumdisorder ASD intellectual disability Intellectual Disability (ID)\"\n",
    "\n",
    "def tidy_up_terminology (input):\n",
    "    space1 = re.sub(r'([a-z])(spec)', r'\\1 \\2', input)               # turn 2+ sequential whitespaces into 1\n",
    "    space2 = re.sub(r'([a-z])(disord)', r'\\1 \\2', space1)               # turn 2+ sequential whitespaces into 1\n",
    "    ASD1 = re.sub(r'(autism spectrum)', r'autistic spectrum', space2)   # turn 2+ sequential whitespaces into 1\n",
    "    ASD2 = re.sub(r'([Aa]utistic [Ss]pectrum [Dd]isorder)', r'ASD', ASD1)\n",
    "    ASD3 = re.sub(r'([Aa]utistic spectrum)', r'Autism', ASD2)\n",
    "    ID1 = re.sub(r'[Ii]ntellectual [Dd]isability \\(ID\\)', r'ID', ASD3)\n",
    "    ID2 = re.sub(r'[Ii]ntellectual [Dd]isability', r'ID', ID1)\n",
    "    ID3 = re.sub(r'(ID and )([Au]utis|ASD)', r'\\2', ID2)\n",
    "    ID4 = re.sub(r'(ID, )([Aa]utis|ASD)', r'\\2', ID3)\n",
    "\n",
    "    return(ID4)\n",
    "\n",
    "tidy_up_terminology(tidy_test)\n",
    "\n",
    "\n",
    "### NOTE TO SELF  -\n",
    "# need to change ([Aa]sperger [Ss]yndrome) and ([Aa]sperger [Ss]yndrome \\(AS\\))  to asperger or autism\n",
    "# need to change ([Aa]utistic disorder) to autism or ASD\n",
    "# asperger autism to autism\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_text = [tidy_up_terminology(sentence) for sentence in matched_texts['Sentence'] ] \n",
    "                                             # create abstract list without extra spaces/run-ons \n",
    "                                             # this is to improve sentence tokenisation later \n",
    "matched_texts['Sentence'] = tidy_text       # copy the no extra space/run-on abstract list back into df as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = matched_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0de07",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Following the cleaning phase, we move on to the extraction phase. This has two parts, first for the person-first extraction and then for the identity-first extraction. \n",
    "\n",
    "The results of both extractions are saved in their own column to make it easy to read and also to allow for a single sentence-token to contain both kinds of patterns. \n",
    "\n",
    "### Person-first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d739a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = [{\"POS\": \"NOUN\"},                                        # define the person-first pattern - start with a noun\n",
    "             {'DEP':'amod', 'OP':\"?\"},                               # followed by an optional modifier\n",
    "             {\"TEXT\": {\"REGEX\": \"(with|by|from)\"}},                  # followed by some words that set up the p-f pattern\n",
    "             {'DEP':'amod', 'OP':\"?\"},                               # then space for up to three optional modifiers\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"TEXT\": {\"REGEX\": \"(^[Aa]utis|^[Aa]sperger|^ASD)\"}}]   # finally, the keywords\n",
    "\n",
    "# Matcher class object \n",
    "matcher = Matcher(nlp.vocab)                                         # define a matcher class object\n",
    "matcher.add(\"matching_1\", [pattern_1])                               # add my three person-first patterns to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern_match(input):                                               # define a function that applies the person-first\n",
    "    thingy = nlp(input)                                                       # matcher class object to strings\n",
    "    match = matcher(thingy)                                                   # and returns any matches to the pattern(s)\n",
    "    if match == []:\n",
    "        out_value = ''\n",
    "    else:\n",
    "        hold_multi_spans = []\n",
    "        for match_id, start, end in match:\n",
    "                string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                span = thingy[start:end]  # The matched span\n",
    "                hold_multi_spans.append(span)\n",
    "        out_value = hold_multi_spans\n",
    "    return out_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences['Person-first'] = matched_sentences.apply(lambda row: find_pattern_match(row.Sentence), axis = 1)\n",
    "                                                                        # apply the newly defined person-first matcher function\n",
    "                                                                        # and store the returned output in a new column\n",
    "len(matched_sentences)                                                  # double check length remains same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc982",
   "metadata": {},
   "source": [
    "### Identity-first pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = [{'DEP':'amod', 'OP':\"?\"},                                 # same for identity-first patterns,\n",
    "             {'DEP':'amod', 'OP':\"?\"},                                 # starting with two optional modifiers\n",
    "             {\"TEXT\": {\"REGEX\": \"(^[Aa]utis|^[Aa]sperger|^ASD)\"}},     # then the keywords\n",
    "             {'DEP':'amod', 'OP':\"?\"},                                 # then upt to three more optional modifiers\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {'DEP':'amod', 'OP':\"?\"},\n",
    "             {\"POS\": \"NOUN\"}]                                          # and then a noun\n",
    "\n",
    "# Matcher class object                                         \n",
    "matcher = Matcher(nlp.vocab) \n",
    "matcher.add(\"matching_2\", [pattern_a])            # this overwrites the matcher object to identity-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences['Identity-first'] = matched_sentences.apply(lambda row: find_pattern_match(row.Sentence), axis = 1)\n",
    "                                                                        # apply the newly overwritten matcher function\n",
    "                                                                        # and store the returned output in a new column\n",
    "len(matched_sentences)                                                  # check the length - why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406db79a",
   "metadata": {},
   "source": [
    "### Consolidation\n",
    "\n",
    "Following the cleaning and extraction phases, the last phase is consolidation. This phase further refines the data by removing all the rows that do not contain a match for one or both of the patterns. For example, there would be a row for \"The child was tested for autism.\" because it contains a keyword of interest. However, this sentence would be eliminated in the consolidation phase as the keyword does not fit into either the person-first or identity-first patterns. \n",
    "\n",
    "Further, this phase goes on to lemmatise the extracted patterns so that they can be counted more easily. This phase also lowercases all occurrences of \"Autistic\", \"Autism\", and \"Asperger's\" as well as removing the apostrophe, the 's' and any non-white characters that might intrude between the 'r' and the 's' of \"Asperger's\". This phase also removes any square brackets, quotes and extra commas introduced by the lemmatisation process. \n",
    "\n",
    "This phase ends by writing out the consolidated data frame to a .csv for manual inspection. I could not find a feasible way of identifying whether or not the nouns matched in the extraction phase are person-nouns or not. As the list is not a totally unreasonable length (in the hundreds) I found it workable to \n",
    "* open in excel, \n",
    "* save the file under another name (e.g. pattern_matches_reviewed), \n",
    "* order the entire data set alphabetically by 'Person-first', \n",
    "* scan through the ordered results check whether each result in the 'Person-first' column is about a person, \n",
    "* removing entire rows if the 'Person-first' match is not about a person (checking the 'Sentence' or 'Text' column if needed)\n",
    "* re-order the entire data set alphabetically by 'Identity-first', \n",
    "* scan through the ordered results check whether each result in the 'Identity-first' column is about a person, \n",
    "* removing entire rows if the 'Identity-first' match is not about a person, \n",
    "* save file again. \n",
    "\n",
    "For example, 'association with autism' matches the person-first pattern but is not about a person, so this row was removed. Many more rows were removed in the 'Identity-first' matches as things like 'autistic behaviours' and 'autism testing' were removed for not being about people. \n",
    "\n",
    "NOTE: There were several instances of \"ASD dataset\" which are not easy to determine if they are about people or not. Do they mean dataset composed from blood tests taken as part of ASD testing? If so, each row in the data set would be a blood test with the possibility that more than one test comes from the same person. Or do they mean a pool of case records, each of which represents a single person? The former would not be \"about people\" but the second would. I did not remove these rows as we cannot be certain. Leaving them out would also have been a valid option, as long as the choice was clear. \n",
    "\n",
    "Coincidentally, during this manual checking part of the consolidation phase I learned that, in the context of human genetics research \"proband\" is a person-noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d91446",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_sentences[(matched_sentences['Person-first'] != '') | (matched_sentences['Identity-first'] != '')]\n",
    "                                                     # keep only rows w/ non-null 'Person-first' and/or 'Identity-first' columns\n",
    "len(matched_patterns)                                # check length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58536fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_patterns.explode('Person-first')    # explode 'Person-first' column to create 1 row per match\n",
    "                                                               # if there were two matches within the same sentence\n",
    "len(matched_patterns)                                          # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80451426",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns = matched_patterns.explode('Identity-first')  # Do the same for 'Identity-first' column\n",
    "len(matched_patterns)                                          # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns                                               # have a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f32a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lem = WordNetLemmatizer()                         # Define a short way to call the WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e44521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_matched_patterns (input):         # \n",
    "    final_lemma_list = []\n",
    "    temp_lemma_list = []\n",
    "    for phrase in input:                       # start for loop looking at each pattern in the person-first pattern column\n",
    "        phrase_as_string = str(phrase)                               # hold the current pattern\n",
    "        words_in_phrase = phrase_as_string.split() # split the current pattern into words\n",
    "        for word in words_in_phrase :                            # for each word in the split up words\n",
    "            lemma = Lem.lemmatize(word)             # turn that word into a lemma\n",
    "            temp_lemma_list.append(lemma)                # append that lemma to a temporary list\n",
    "        string_lem = str(temp_lemma_list)              # turn that temporary list into a string\n",
    "        stripped_lem = re.sub(r\"\\[|\\]|\\'|\\,\",'', string_lem)  # remove  square brackets, commas and '' marks from the string\n",
    "        lower_autis = re.sub(r'Autis', r'autis', stripped_lem)               # lowercases Autism or Autistic\n",
    "        lower_asp = re.sub(r'Asperger', r'asperger', lower_autis)          # lowercases Asperger\n",
    "        no_apost = re.sub(r'(asperger[\\S*?]s)', r'asperger', lower_asp)    # Removes any apostrophe and non-whitespace in asperger's\n",
    "        no_s = re.sub(r'(aspergers)', r'asperger', no_apost)               # Removes any s following asperger \n",
    "                                                                       # also removes any 's or s after asperger\n",
    "        final_lemma_list.append(no_s)        # append the string version of the list to the output list\n",
    "        temp_lemma_list = []                               # ensure the temp variable is empty\n",
    "\n",
    "    return(final_lemma_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b670fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_lemma_list = consolidate_matched_patterns(matched_patterns['Person-first'])\n",
    "identity_lemma_list = consolidate_matched_patterns(matched_patterns['Identity-first'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns['Person-first'] = person_lemma_list    # copy the person-first output to new column in data frame \n",
    "matched_patterns['Identity-first'] = identity_lemma_list  # copy the identity-first output to new column in data frame \n",
    "matched_patterns = matched_patterns.drop_duplicates()                         # drop any duplicates\n",
    "matched_patterns                                                   # have a look at the data frame with its new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_patterns.to_csv('..\\\\output\\\\pattern_matches_to_review.csv')        \n",
    "                                                            # Write the data frame to a .csv for manual processing in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897ba53",
   "metadata": {},
   "source": [
    "At this point, I open the file in Excel (for example), removed the brackets, quotation marks and commas in the Person-first lemmatised and Identity-first lemmatised columns, then sort by each of one of these columns. I then scan through the results, removing any rows that are obviously not about people (e.g. \"autistic testing\") and checking the 'Text' column on any that are unclear 'autistic quartets'). I then sort by the other column and repeat the step of reviewing and deleting non-person rows. Save under \"pattern_matches_reviewed.csv\" for the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2281a",
   "metadata": {},
   "source": [
    "## Chart person-first or identity-first by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_matches = pd.read_csv('..\\\\output\\\\pattern_matches_reviewed.csv')    # one for just those that match the keyword\n",
    "print(len(reviewed_matches)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_abstracts = reviewed_matches['Title'].nunique()\n",
    "print(total_unique_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1437cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_first = reviewed_matches[~reviewed_matches['Person-first'].isnull()]\n",
    "person_first['Year'] = person_first['Year'].astype('Int64')\n",
    "len(person_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c040bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_first = reviewed_matches[~reviewed_matches['Identity-first'].isnull()]\n",
    "identity_first['Year'] = identity_first['Year'].astype('Int64')\n",
    "len(identity_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_count = person_first.groupby(['Year'])['Person-first'].count()\n",
    "identity_count = identity_first.groupby(['Year'])['Identity-first'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count=pd.concat([person_count,identity_count],axis=1)\n",
    "person_identity_count = person_identity_count.fillna(0)\n",
    "person_identity_count = person_identity_count.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_count.plot()\n",
    "plt.show()\n",
    "plt.savefig('..\\\\output\\\\matches_count.jpg')    # we can right click on the plot above to save it, or save it via command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_examples = reviewed_matches.groupby(['Person-first'])['Person-first'].count()\n",
    "identity_examples = reviewed_matches.groupby(['Identity-first'])['Identity-first'].count()\n",
    "print(len(person_examples))\n",
    "print(len(identity_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1443d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples=pd.concat([person_examples,identity_examples],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Person-first'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.sort_values(by=['Identity-first'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6184b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_identity_examples.notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82b88",
   "metadata": {},
   "source": [
    "## Count abstracts by the structures they use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd85bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_by_title = reviewed_matches.groupby(['Title'])['Person-first'].count()\n",
    "identity_by_title = reviewed_matches.groupby(['Title'])['Identity-first'].count()\n",
    "title = pd.concat([person_by_title,identity_by_title],axis=1)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f461ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "title.sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Person-first','Identity-first']\n",
    "filter_ = (title[columns] > 0).all(axis=1)\n",
    "title[filter_]\n",
    "len(title[filter_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Person-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13737d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title[filter_].sort_values(by=['Identity-first'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_pf = title[title['Person-first'] > 0]\n",
    "has_both = has_pf[has_pf['Identity-first'] > 0]\n",
    "print(len(has_both))\n",
    "has_both"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
