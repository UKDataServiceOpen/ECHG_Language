{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "\n",
    "Now that we have the abstracts in two nice neat .csv files, we need to download/import the packages needed, import the .csv files, and then can get on with the first part of the analysis. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef24df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'all_abstracts_no_null_texts.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364e65f",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import and check them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076b8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')            # one for all of the texts and then\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38228\n"
     ]
    }
   ],
   "source": [
    "print (len(all_texts))                        # it is always useful to double check that the length matches your expectations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d22b75",
   "metadata": {},
   "source": [
    "## Get some basic stats about how texts are spread out over time\n",
    "\n",
    "We know that all of the rows in the files have at least two columns with contents - 'Year' and 'Text'. This means that it is probably a useful thing to get a little schematic and/or table that counts row according to year. Let's do that now!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc03bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021.0    2524\n",
      "2013.0    2373\n",
      "2001.0    2336\n",
      "2014.0    2240\n",
      "2004.0    2205\n",
      "2016.0    2040\n",
      "2011.0    1967\n",
      "2015.0    1951\n",
      "2008.0    1896\n",
      "2012.0    1871\n",
      "2020.0    1853\n",
      "2010.0    1716\n",
      "2009.0    1704\n",
      "2018.0    1617\n",
      "2017.0    1601\n",
      "2019.0    1589\n",
      "2007.0    1541\n",
      "2005.0    1520\n",
      "2006.0    1422\n",
      "2002.0    1266\n",
      "2003.0     996\n",
      "Name: Year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_counts_by_year = all_texts['Year'].value_counts()         # this creates a little table with two columns - year and count\n",
    "print(all_counts_by_year)                                     # however, when we print it we can see it has no headers,\n",
    "                                                              # is not in order, has the years appearing as  floats, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0be49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year   All\n",
      "2   2001  2336\n",
      "19  2002  1266\n",
      "20  2003   996\n",
      "4   2004  2205\n",
      "17  2005  1520\n",
      "18  2006  1422\n",
      "16  2007  1541\n",
      "8   2008  1896\n",
      "12  2009  1704\n",
      "11  2010  1716\n",
      "6   2011  1967\n",
      "9   2012  1871\n",
      "1   2013  2373\n",
      "3   2014  2240\n",
      "7   2015  1951\n",
      "5   2016  2040\n",
      "14  2017  1601\n",
      "13  2018  1617\n",
      "15  2019  1589\n",
      "10  2020  1853\n",
      "0   2021  2524\n"
     ]
    }
   ],
   "source": [
    "all_counts_by_year = pd.DataFrame(all_counts_by_year)                            # Convert the imported table to a data frame,\n",
    "all_counts_by_year = all_counts_by_year.rename(columns={\"Year\": \"All\"})          # rename the columns,\n",
    "all_counts_by_year = all_counts_by_year.rename_axis('Year').reset_index()        # set the axis to 'Year' and reset the index,\n",
    "all_counts_by_year = all_counts_by_year.sort_values(by=['Year']).astype('Int64') # retype 'Year' column and sort by year.\n",
    "print(all_counts_by_year)                                                        # Let's just check it worked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "## Count word frequencies - 'bag of words'\n",
    "\n",
    "Now that we have some basic descriptive stats about how many abstracts were imported properly with text in the 'Text' column, we can get on to the actual natural language processing steps. The most basic NLP option is to count the most frequent words found in the two sets of abstracts - meaning we need to find the most frequent words found in ALL of the abstracts and then compare that to the most frequnet words found in only those abstracts that contain a keyword of interest. \n",
    "\n",
    "To this end, we use the 'bag of words' method which whacks all of the words from all of the texts together, turns them into 'tokens' then processes to make them as unified as possible by removing uppercase letters, punctuation, digits, empty strings, stop words (e.g. 'the', 'and', 'for', etc. ) and word forms (e.g. pluralisations, verb endings, etc. ). \n",
    "\n",
    "Let's demo this with a simple example. If the text we want to 'bag of words' is \"The cat named Cat was one of 5 cats.\" it would become a list of stemmed word-tokens like \n",
    "'''[[cat]\n",
    "[name]\n",
    "[cat]\n",
    "[be]\n",
    "[cat]]''' \n",
    "and the most common word would obviously be '''[cat]'''. \n",
    "\n",
    "Applying the 'bag of words' method to our texts is not so trivial, but should also be more enlightening. We would expect that the most common words from all of the texts would be similar to, but not identical to, the most common words from only the abstracts that contain a keyword of interest.\n",
    "\n",
    "This bag of words approach ignores years, session codes, authors and everything else. Subsetting the texts by those things might be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):     # define a 'bag of words' function with 2 arguments, an input and a quantity \n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in input['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    holding_string = [porter.stem(token) for token in holding_string]                # stems the word-tokens\n",
    "    list_for_count = []                                                              # and creates an empty list\n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    return counts.most_common(how_many)                                  # and returns the tokens with highest counts \n",
    "                                                                         # up to the quantity specified as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_all = bag_of_words_analysis(all_texts, 20)   # apply bag of words function to all texts, and save the output table\n",
    "                                                           # this will take a while. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c47d79",
   "metadata": {},
   "source": [
    "### Steps to take\n",
    "\n",
    "* revise the bag_of_words_analysis function to group by year? ALternatively, create one bag of words for each year. \n",
    "* write a new function that scans the year-bags-of-words for all words on an input list\n",
    "* save the output of that function to a list (or .csv?) \n",
    "* use the outputs to create graphs that track the popularity of all the words on that list over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts['autist'])\n",
    "print(counts['asd'])\n",
    "print(counts['asperg'])\n",
    "print(counts['autism'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
