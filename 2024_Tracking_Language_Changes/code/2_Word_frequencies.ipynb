{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "\n",
    "Now that we have the abstracts in two nice neat .csv files, we need to download/import the packages needed, import the .csv files, and then can get on with the first part of the analysis. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef24df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364e65f",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import and check them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369308f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_list = []\n",
    "batches_dict = {}\n",
    "\n",
    "for batch in os.listdir(\"..\\\\output\"):\n",
    "        if batch.endswith(\".csv\"):\n",
    "            name = batch.rsplit('.', maxsplit=1)[0]\n",
    "            batches_list.append(name)\n",
    "\n",
    "for batch in batches_list:\n",
    "    df = pd.read_csv('..\\\\output\\\\' + batch + \".csv\").drop(['Unnamed: 0', 'Year'],axis=1)\n",
    "    year = batch.rsplit('_', maxsplit=1)[1]\n",
    "    batches_dict[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32613fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a4509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c965cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "## Count word frequencies - 'bag of words'\n",
    "\n",
    "Now that we have some basic descriptive stats about how many abstracts were imported properly with text in the 'Text' column, we can get on to the actual natural language processing steps. The most basic NLP option is to count the most frequent words found in the two sets of abstracts - meaning we need to find the most frequent words found in ALL of the abstracts and then compare that to the most frequnet words found in only those abstracts that contain a keyword of interest. \n",
    "\n",
    "To this end, we use the 'bag of words' method which whacks all of the words from all of the texts together, turns them into 'tokens' then processes to make them as unified as possible by removing uppercase letters, punctuation, digits, empty strings, stop words (e.g. 'the', 'and', 'for', etc. ) and word forms (e.g. pluralisations, verb endings, etc. ). \n",
    "\n",
    "Let's demo this with a simple example. If the text we want to 'bag of words' is \"The cat named Cat was one of 5 cats.\" it would become a list of stemmed word-tokens like \n",
    "'''[[cat]\n",
    "[name]\n",
    "[cat]\n",
    "[be]\n",
    "[cat]]''' \n",
    "and the most common word would obviously be '''[cat]'''. \n",
    "\n",
    "Applying the 'bag of words' method to our texts is not so trivial, but should also be more enlightening. We would expect that the most common words from all of the texts would be similar to, but not identical to, the most common words from only the abstracts that contain a keyword of interest.\n",
    "\n",
    "This bag of words approach ignores years, session codes, authors and everything else. Subsetting the texts by those things might be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33601e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACEMENTS_1 = [\n",
    "    ('mutations', 'mutation' ),\n",
    "    ('variants', \"variant\"),\n",
    "    ('changes', \"change\"),\n",
    "    ('alterations', \"alteration\"),\n",
    "    ('diseases', \"disease\"), \n",
    "    ('disorders', \"disorder\"),\n",
    "    ('illnesses', \"illness\"), \n",
    "    ('conditions', \"condition\"),\n",
    "    ('diagnoses', \"diagnosis\"), \n",
    "    ('syndromes', \"syndrome\"), \n",
    "    ('patients', \"patient\"),\n",
    "    ('individuals', \"individual\"),\n",
    "    ('people', \"person\"),\n",
    "    ('probands', \"proband\"), \n",
    "    ('subjects', \"subject\"), \n",
    "    ('cases', \"case\"), \n",
    "    ('normals', \"normal\"), \n",
    "    ('typicals', \"typical\"), \n",
    "    ('wilds', \"wild\"), \n",
    "    ('types', \"type\"), \n",
    "    ('abnormals', \"abnormal\"), \n",
    "    ('atypicals', \"atypical\"), \n",
    "    ]\n",
    "\n",
    "REPLACEMENTS_2 = [\n",
    "    ('gene change', \"genechange\"), \n",
    "    ('gene alteration', \"genealteration\"), \n",
    "    ('suffering from', \"sufferingfrom\"), \n",
    "    ('living with', \"livingwith\"), \n",
    "    ('wild type', \"wildtype\"), \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stop_words))                                # OPTIONAL: check what counts as a stopword if you want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f498e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mutation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>variant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genechange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>genealteration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>diagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>proband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>diagnosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>sufferingfrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>impacted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>livingwith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>typical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>wildtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>atypical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    set           words\n",
       "0     1        mutation\n",
       "1     1         variant\n",
       "2     1      genechange\n",
       "3     1  genealteration\n",
       "4     2         disease\n",
       "5     2        disorder\n",
       "6     2         illness\n",
       "7     2       condition\n",
       "8     2       diagnosis\n",
       "9     2        syndrome\n",
       "10    3         patient\n",
       "11    3      individual\n",
       "12    3          person\n",
       "13    3         proband\n",
       "14    3         subject\n",
       "15    3            case\n",
       "16    4        affected\n",
       "17    4       diagnosed\n",
       "18    4   sufferingfrom\n",
       "19    4        impacted\n",
       "20    4      livingwith\n",
       "21    5          normal\n",
       "22    5         typical\n",
       "23    5        wildtype\n",
       "24    5        abnormal\n",
       "25    5        atypical"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
    "words = ['mutation', 'variant', 'genechange', 'genealteration', \n",
    "         'disease', 'disorder', 'illness', 'condition', 'diagnosis', 'syndrome', \n",
    "         'patient', 'individual', 'person', 'proband', 'subject', 'case', \n",
    "         'affected', 'diagnosed', 'sufferingfrom', 'impacted', 'livingwith', \n",
    "         'normal', 'typical', 'wildtype', 'abnormal', 'atypical']\n",
    "\n",
    "target_word_dataframe = pd.DataFrame(list(zip(sets, words)), columns=['set','words'])\n",
    "target_word_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866182c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    set           words    2001  2002    2003    2004    2005  2006    2007  \\\n",
      "0     1        mutation  3035.0  2009  1567.0  2192.0  2595.0  2571  2243.0   \n",
      "1     1         variant   295.0   164   152.0   136.0   294.0   266   308.0   \n",
      "2     1      genechange     NaN     1     NaN     NaN     NaN     1     NaN   \n",
      "3     1  genealteration     5.0     2     1.0     NaN     4.0     1     2.0   \n",
      "4     2         disease  1343.0   765   572.0   615.0  1002.0  1045   925.0   \n",
      "5     2        disorder   654.0   436   310.0   337.0   560.0   587   521.0   \n",
      "6     2         illness    15.0    11     3.0     9.0     3.0     9     7.0   \n",
      "7     2       condition   203.0   132    72.0    95.0   130.0   183   136.0   \n",
      "8     2       diagnosis   655.0   488   287.0   366.0   485.0   497   414.0   \n",
      "9     2        syndrome  1078.0   673   499.0   838.0   835.0   900   885.0   \n",
      "10    3         patient  2778.0  1763  1572.0  1900.0  2634.0  2456  2415.0   \n",
      "11    3      individual   658.0   320   286.0   186.0   384.0   499   445.0   \n",
      "12    3          person    88.0    44    42.0     7.0    52.0    91    83.0   \n",
      "13    3         proband   136.0   112   101.0    86.0   138.0   121   108.0   \n",
      "14    3         subject   244.0   105    58.0    71.0   132.0   146   173.0   \n",
      "15    3            case  1666.0  1088   794.0  1041.0  1355.0  1168  1248.0   \n",
      "16    4        affected   504.0   322   223.0   266.0   373.0   363   314.0   \n",
      "17    4       diagnosed   144.0   104    71.0    88.0   153.0   135   131.0   \n",
      "18    4   sufferingfrom    18.0     7     9.0    10.0    12.0    22    15.0   \n",
      "19    4        impacted     4.0     1     NaN     NaN     2.0     1     NaN   \n",
      "20    4      livingwith     NaN     7     NaN     1.0     NaN     3     3.0   \n",
      "21    5          normal   734.0   439   377.0   365.0   523.0   482   550.0   \n",
      "22    5         typical    84.0    58    53.0    63.0    74.0    81    89.0   \n",
      "23    5        wildtype    77.0    46    27.0    47.0    53.0    47    36.0   \n",
      "24    5        abnormal   173.0   125   108.0    93.0   175.0   123   153.0   \n",
      "25    5        atypical    35.0    13    30.0    26.0    30.0    30    24.0   \n",
      "\n",
      "      2008  ...    2012    2013  2014  2015    2016    2017  2018    2019  \\\n",
      "0   2885.0  ...  2802.0  4090.0  3708  3241  3194.0  2710.0  2247  1797.0   \n",
      "1    484.0  ...   750.0  1268.0  1436  1492  2124.0  1633.0  2071  2765.0   \n",
      "2      1.0  ...     NaN     NaN     1     1     NaN     NaN     1     NaN   \n",
      "3      NaN  ...     NaN     6.0     2     2     3.0     1.0     3     2.0   \n",
      "4   1199.0  ...  1086.0  1619.0  1646  1509  1437.0  1197.0  1213  1453.0   \n",
      "5    659.0  ...   713.0   883.0   887   737   827.0   739.0   838   763.0   \n",
      "6     25.0  ...    12.0     9.0     8    16    18.0    10.0    13    11.0   \n",
      "7    232.0  ...   195.0   255.0   255   236   242.0   171.0   211   217.0   \n",
      "8    559.0  ...   529.0   741.0   771   697   712.0   650.0   598   628.0   \n",
      "9    972.0  ...  1035.0  1365.0  1136  1020  1035.0   895.0  1035   701.0   \n",
      "10  3216.0  ...  3405.0  4251.0  4134  3479  3592.0  2914.0  2939  2951.0   \n",
      "11   518.0  ...   514.0   684.0   639   607   562.0   451.0   440   611.0   \n",
      "12   111.0  ...    67.0    55.0   112    91    87.0    37.0    52    52.0   \n",
      "13   133.0  ...   121.0   176.0   192   133   171.0   160.0   147   162.0   \n",
      "14   214.0  ...   180.0   213.0   250   170   151.0    84.0   134   106.0   \n",
      "15  1546.0  ...  1413.0  1903.0  1616  1374  1489.0  1097.0  1122  1174.0   \n",
      "16   378.0  ...   400.0   545.0   524   415   458.0   363.0   342   301.0   \n",
      "17   192.0  ...   195.0   234.0   247   206   255.0   169.0   168   146.0   \n",
      "18    12.0  ...    17.0    19.0    13    14    18.0     7.0     8     9.0   \n",
      "19     1.0  ...     1.0     4.0     3     2     6.0     3.0     4     7.0   \n",
      "20     7.0  ...     1.0     1.0     1     2     1.0     1.0     1     3.0   \n",
      "21   657.0  ...   541.0   589.0   538   398   514.0   312.0   314   241.0   \n",
      "22    85.0  ...    98.0   118.0    95    86    84.0    59.0    80    46.0   \n",
      "23    56.0  ...    65.0   102.0    79    71    98.0    58.0    51    70.0   \n",
      "24   220.0  ...   143.0   199.0   160   155   135.0   126.0   106   116.0   \n",
      "25    32.0  ...    38.0    67.0    39    54    43.0    37.0    39    42.0   \n",
      "\n",
      "      2020  2021  \n",
      "0   2011.0  1807  \n",
      "1   3653.0  4755  \n",
      "2      NaN     1  \n",
      "3      2.0     3  \n",
      "4   1535.0  1889  \n",
      "5    987.0  1320  \n",
      "6      8.0     6  \n",
      "7    234.0   279  \n",
      "8    808.0  1004  \n",
      "9   1296.0  1222  \n",
      "10  3702.0  4266  \n",
      "11   589.0   825  \n",
      "12    42.0    90  \n",
      "13   207.0   276  \n",
      "14    76.0    91  \n",
      "15  1373.0  1721  \n",
      "16   395.0   459  \n",
      "17   223.0   313  \n",
      "18    19.0     6  \n",
      "19     3.0     6  \n",
      "20     NaN     5  \n",
      "21   351.0   342  \n",
      "22    88.0    78  \n",
      "23    66.0    82  \n",
      "24   125.0   101  \n",
      "25    63.0    57  \n",
      "\n",
      "[26 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "for k,v in batches_dict.items():\n",
    "    year = str(k)\n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in v['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    for plural, singular in REPLACEMENTS_1:\n",
    "        holding_string = holding_string.replace(plural, singular)\n",
    "    for multi, single in REPLACEMENTS_2:\n",
    "        holding_string = holding_string.replace(multi, single)\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    list_for_count = []                                                              # and creates an empty list        \n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        if token in target_word_dataframe.values:\n",
    "            list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    temp_df = pd.DataFrame.from_records(list(dict(counts).items()), columns=['words',year])\n",
    "    target_word_dataframe = pd.merge(target_word_dataframe, temp_df, on='words', how='outer')\n",
    "\n",
    "    \n",
    "print(target_word_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_dataframe.to_csv('..\\\\output\\\\final\\\\target_words_by_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c47d79",
   "metadata": {},
   "source": [
    "### Steps to take\n",
    "\n",
    "* revise the bag_of_words_analysis function to group by year? ALternatively, create one bag of words for each year. \n",
    "* write a new function that scans the year-bags-of-words for all words on an input list\n",
    "* save the output of that function to a list (or .csv?) \n",
    "* use the outputs to create graphs that track the popularity of all the words on that list over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b77abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a3d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
