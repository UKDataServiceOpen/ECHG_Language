{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "\n",
    "Now that we have the abstracts in two nice neat .csv files, we need to download/import the packages needed, import the .csv files, and then can get on with the first part of the analysis. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef24df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'batch_2001.csv', 'batch_2002.csv', 'batch_2003.csv', 'batch_2004.csv', 'batch_2005.csv', 'batch_2006.csv', 'batch_2007.csv', 'batch_2008.csv', 'batch_2009.csv', 'batch_2010.csv', 'batch_2011.csv', 'batch_2012.csv', 'batch_2013.csv', 'batch_2014.csv', 'batch_2015.csv', 'batch_2016.csv', 'batch_2017.csv', 'batch_2018.csv', 'batch_2019.csv', 'batch_2020.csv', 'batch_2021.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364e65f",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import and check them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c965cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2001 = pd.read_csv('..\\\\output\\\\batch_2001.csv')            # start importing the files, each saved under its own name\n",
    "batch_2002 = pd.read_csv('..\\\\output\\\\batch_2002.csv')             \n",
    "batch_2003 = pd.read_csv('..\\\\output\\\\batch_2003.csv')             \n",
    "batch_2004 = pd.read_csv('..\\\\output\\\\batch_2004.csv')            \n",
    "batch_2005 = pd.read_csv('..\\\\output\\\\batch_2005.csv')             \n",
    "batch_2006 = pd.read_csv('..\\\\output\\\\batch_2006.csv')             \n",
    "batch_2007 = pd.read_csv('..\\\\output\\\\batch_2007.csv')            \n",
    "batch_2008 = pd.read_csv('..\\\\output\\\\batch_2008.csv')             \n",
    "batch_2009 = pd.read_csv('..\\\\output\\\\batch_2009.csv')            \n",
    "batch_2010 = pd.read_csv('..\\\\output\\\\batch_2010.csv')            \n",
    "batch_2011 = pd.read_csv('..\\\\output\\\\batch_2011.csv')             \n",
    "batch_2012 = pd.read_csv('..\\\\output\\\\batch_2012.csv')             \n",
    "batch_2013 = pd.read_csv('..\\\\output\\\\batch_2013.csv')            \n",
    "batch_2014 = pd.read_csv('..\\\\output\\\\batch_2014.csv')             \n",
    "batch_2015 = pd.read_csv('..\\\\output\\\\batch_2015.csv')             \n",
    "batch_2016 = pd.read_csv('..\\\\output\\\\batch_2016.csv')            \n",
    "batch_2017 = pd.read_csv('..\\\\output\\\\batch_2017.csv')             \n",
    "batch_2018 = pd.read_csv('..\\\\output\\\\batch_2018.csv')             \n",
    "batch_2019 = pd.read_csv('..\\\\output\\\\batch_2019.csv')            \n",
    "batch_2020 = pd.read_csv('..\\\\output\\\\batch_2020.csv')             \n",
    "batch_2021 = pd.read_csv('..\\\\output\\\\batch_2021.csv')             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "## Count word frequencies - 'bag of words'\n",
    "\n",
    "Now that we have some basic descriptive stats about how many abstracts were imported properly with text in the 'Text' column, we can get on to the actual natural language processing steps. The most basic NLP option is to count the most frequent words found in the two sets of abstracts - meaning we need to find the most frequent words found in ALL of the abstracts and then compare that to the most frequnet words found in only those abstracts that contain a keyword of interest. \n",
    "\n",
    "To this end, we use the 'bag of words' method which whacks all of the words from all of the texts together, turns them into 'tokens' then processes to make them as unified as possible by removing uppercase letters, punctuation, digits, empty strings, stop words (e.g. 'the', 'and', 'for', etc. ) and word forms (e.g. pluralisations, verb endings, etc. ). \n",
    "\n",
    "Let's demo this with a simple example. If the text we want to 'bag of words' is \"The cat named Cat was one of 5 cats.\" it would become a list of stemmed word-tokens like \n",
    "'''[[cat]\n",
    "[name]\n",
    "[cat]\n",
    "[be]\n",
    "[cat]]''' \n",
    "and the most common word would obviously be '''[cat]'''. \n",
    "\n",
    "Applying the 'bag of words' method to our texts is not so trivial, but should also be more enlightening. We would expect that the most common words from all of the texts would be similar to, but not identical to, the most common words from only the abstracts that contain a keyword of interest.\n",
    "\n",
    "This bag of words approach ignores years, session codes, authors and everything else. Subsetting the texts by those things might be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33601e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACEMENTS_1 = [\n",
    "    ('mutations', 'mutation' ),\n",
    "    ('variants', \"variant\"),\n",
    "    ('changes', \"change\"),\n",
    "    ('alterations', \"alteration\"),\n",
    "    ('diseases', \"disease\"), \n",
    "    ('disorders', \"disorder\"),\n",
    "    ('illnesses', \"illness\"), \n",
    "    ('conditions', \"condition\"),\n",
    "    ('diagnoses', \"diagnosis\"), \n",
    "    ('syndromes', \"syndrome\"),\n",
    "    ('patients', \"patient\"),\n",
    "    ('individuals', \"individual\"),\n",
    "    ('people', \"person\"),\n",
    "    ('probands', \"proband\"), \n",
    "    ('subjects', \"subject\"), \n",
    "    ('cases', \"case\"), \n",
    "    ('normals', \"normal\"), \n",
    "    ('typicals', \"typical\"), \n",
    "    ('wilds', \"wild\"), \n",
    "    ('types', \"type\"), \n",
    "    ('abnormals', \"abnormal\"), \n",
    "    ('atypicals', \"atypical\"), \n",
    "    ]\n",
    "\n",
    "REPLACEMENTS_2 = [\n",
    "    ('gene change', \"genechange\"), \n",
    "    ('gene alteration', \"genealteration\"), \n",
    "    ('suffering from', \"sufferingfrom\"), \n",
    "    ('living with', \"livingwith\"), \n",
    "    ('wild type', \"wildtype\"), \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ac0c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stop_words))                                # OPTIONAL: check what counts as a stopword if you want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f498e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mutation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>variant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genechange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>genealteration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>diagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>proband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>diagnosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>sufferingfrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>impacted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>livingwith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>typical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>wildtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>atypical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    set           words\n",
       "0     1        mutation\n",
       "1     1         variant\n",
       "2     1      genechange\n",
       "3     1  genealteration\n",
       "4     2         disease\n",
       "5     2        disorder\n",
       "6     2         illness\n",
       "7     2       condition\n",
       "8     2       diagnosis\n",
       "9     2        syndrome\n",
       "10    3         patient\n",
       "11    3      individual\n",
       "12    3          person\n",
       "13    3         proband\n",
       "14    3         subject\n",
       "15    3            case\n",
       "16    4        affected\n",
       "17    4       diagnosed\n",
       "18    4   sufferingfrom\n",
       "19    4        impacted\n",
       "20    4      livingwith\n",
       "21    5          normal\n",
       "22    5         typical\n",
       "23    5        wildtype\n",
       "24    5        abnormal\n",
       "25    5        atypical"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
    "words = ['mutation', 'variant', 'genechange', 'genealteration', \n",
    "         'disease', 'disorder', 'illness', 'condition', 'diagnosis', 'syndrome', \n",
    "         'patient', 'individual', 'person', 'proband', 'subject', 'case', \n",
    "         'affected', 'diagnosed', 'sufferingfrom', 'impacted', 'livingwith', \n",
    "         'normal', 'typical', 'wildtype', 'abnormal', 'atypical']\n",
    "\n",
    "target_word_dataframe = pd.DataFrame(list(zip(sets, words)), columns=['set','words'])\n",
    "target_word_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df663c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_a_bag_of_words (input, target_words, year):\n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in input['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    for plural, singular in REPLACEMENTS_1:\n",
    "        holding_string = holding_string.replace(plural, singular)\n",
    "    for multi, single in REPLACEMENTS_2:\n",
    "        holding_string = holding_string.replace(multi, single)\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    list_for_count = []                                                              # and creates an empty list        \n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        if token in target_words.values:\n",
    "            list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    year  = pd.DataFrame.from_records(list(dict(counts).items()), columns=['words',year])\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bb9e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2001 = create_a_bag_of_words(batch_2001, target_word_dataframe, '2001')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2001, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df92a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2002 = create_a_bag_of_words(batch_2002, target_word_dataframe, '2002')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2002, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1961d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2003 = create_a_bag_of_words(batch_2003, target_word_dataframe, '2003')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2003, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60169931",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2004 = create_a_bag_of_words(batch_2004, target_word_dataframe, '2004')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2004, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da674c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2005 = create_a_bag_of_words(batch_2005, target_word_dataframe, '2005')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2005, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238004a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2006 = create_a_bag_of_words(batch_2006, target_word_dataframe, '2006')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2006, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b82c986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2007 = create_a_bag_of_words(batch_2007, target_word_dataframe, '2007')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2007, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8bbfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2008 = create_a_bag_of_words(batch_2008, target_word_dataframe, '2008')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2008, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e9f736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2009 = create_a_bag_of_words(batch_2009, target_word_dataframe, '2009')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2009, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a51ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2010 = create_a_bag_of_words(batch_2010, target_word_dataframe, '2010')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2010, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c1efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2011 = create_a_bag_of_words(batch_2011, target_word_dataframe, '2011')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2011, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da094f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2012 = create_a_bag_of_words(batch_2012, target_word_dataframe, '2012')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2012, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "964e7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2013 = create_a_bag_of_words(batch_2013, target_word_dataframe, '2013')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2013, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86095021",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2014 = create_a_bag_of_words(batch_2014, target_word_dataframe, '2014')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2014, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ea0f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2015 = create_a_bag_of_words(batch_2015, target_word_dataframe, '2015')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2015, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e0bfb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2016 = create_a_bag_of_words(batch_2016, target_word_dataframe, '2016')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2016, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc99dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2017 = create_a_bag_of_words(batch_2017, target_word_dataframe, '2017')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2017, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02cc4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2018 = create_a_bag_of_words(batch_2018, target_word_dataframe, '2018')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2018, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f32cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2019 = create_a_bag_of_words(batch_2019, target_word_dataframe, '2019')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2019, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71e32503",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2020 = create_a_bag_of_words(batch_2020, target_word_dataframe, '2020')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2020, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b98eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_2021 = create_a_bag_of_words(batch_2021, target_word_dataframe, '2021')\n",
    "target_word_dataframe = pd.merge(target_word_dataframe, bag_of_2021, on='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d00f5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>words</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>...</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mutation</td>\n",
       "      <td>3035</td>\n",
       "      <td>2009</td>\n",
       "      <td>1567</td>\n",
       "      <td>2192</td>\n",
       "      <td>2595</td>\n",
       "      <td>2571</td>\n",
       "      <td>2243</td>\n",
       "      <td>2885</td>\n",
       "      <td>...</td>\n",
       "      <td>2802</td>\n",
       "      <td>4090</td>\n",
       "      <td>3708</td>\n",
       "      <td>3241</td>\n",
       "      <td>3194</td>\n",
       "      <td>2710</td>\n",
       "      <td>2247</td>\n",
       "      <td>1797</td>\n",
       "      <td>2011</td>\n",
       "      <td>1807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>variant</td>\n",
       "      <td>295</td>\n",
       "      <td>164</td>\n",
       "      <td>152</td>\n",
       "      <td>136</td>\n",
       "      <td>294</td>\n",
       "      <td>266</td>\n",
       "      <td>308</td>\n",
       "      <td>484</td>\n",
       "      <td>...</td>\n",
       "      <td>750</td>\n",
       "      <td>1268</td>\n",
       "      <td>1436</td>\n",
       "      <td>1492</td>\n",
       "      <td>2124</td>\n",
       "      <td>1633</td>\n",
       "      <td>2071</td>\n",
       "      <td>2765</td>\n",
       "      <td>3653</td>\n",
       "      <td>4755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>disease</td>\n",
       "      <td>1343</td>\n",
       "      <td>765</td>\n",
       "      <td>572</td>\n",
       "      <td>615</td>\n",
       "      <td>1002</td>\n",
       "      <td>1045</td>\n",
       "      <td>925</td>\n",
       "      <td>1199</td>\n",
       "      <td>...</td>\n",
       "      <td>1086</td>\n",
       "      <td>1619</td>\n",
       "      <td>1646</td>\n",
       "      <td>1509</td>\n",
       "      <td>1437</td>\n",
       "      <td>1197</td>\n",
       "      <td>1213</td>\n",
       "      <td>1453</td>\n",
       "      <td>1535</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>disorder</td>\n",
       "      <td>654</td>\n",
       "      <td>436</td>\n",
       "      <td>310</td>\n",
       "      <td>337</td>\n",
       "      <td>560</td>\n",
       "      <td>587</td>\n",
       "      <td>521</td>\n",
       "      <td>659</td>\n",
       "      <td>...</td>\n",
       "      <td>713</td>\n",
       "      <td>883</td>\n",
       "      <td>887</td>\n",
       "      <td>737</td>\n",
       "      <td>827</td>\n",
       "      <td>739</td>\n",
       "      <td>838</td>\n",
       "      <td>763</td>\n",
       "      <td>987</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>illness</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>condition</td>\n",
       "      <td>203</td>\n",
       "      <td>132</td>\n",
       "      <td>72</td>\n",
       "      <td>95</td>\n",
       "      <td>130</td>\n",
       "      <td>183</td>\n",
       "      <td>136</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>195</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>236</td>\n",
       "      <td>242</td>\n",
       "      <td>171</td>\n",
       "      <td>211</td>\n",
       "      <td>217</td>\n",
       "      <td>234</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>655</td>\n",
       "      <td>488</td>\n",
       "      <td>287</td>\n",
       "      <td>366</td>\n",
       "      <td>485</td>\n",
       "      <td>497</td>\n",
       "      <td>414</td>\n",
       "      <td>559</td>\n",
       "      <td>...</td>\n",
       "      <td>529</td>\n",
       "      <td>741</td>\n",
       "      <td>771</td>\n",
       "      <td>697</td>\n",
       "      <td>712</td>\n",
       "      <td>650</td>\n",
       "      <td>598</td>\n",
       "      <td>628</td>\n",
       "      <td>808</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>syndrome</td>\n",
       "      <td>1078</td>\n",
       "      <td>673</td>\n",
       "      <td>499</td>\n",
       "      <td>838</td>\n",
       "      <td>835</td>\n",
       "      <td>900</td>\n",
       "      <td>885</td>\n",
       "      <td>972</td>\n",
       "      <td>...</td>\n",
       "      <td>1035</td>\n",
       "      <td>1365</td>\n",
       "      <td>1136</td>\n",
       "      <td>1020</td>\n",
       "      <td>1035</td>\n",
       "      <td>895</td>\n",
       "      <td>1035</td>\n",
       "      <td>701</td>\n",
       "      <td>1296</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>patient</td>\n",
       "      <td>2778</td>\n",
       "      <td>1763</td>\n",
       "      <td>1572</td>\n",
       "      <td>1900</td>\n",
       "      <td>2634</td>\n",
       "      <td>2456</td>\n",
       "      <td>2415</td>\n",
       "      <td>3216</td>\n",
       "      <td>...</td>\n",
       "      <td>3405</td>\n",
       "      <td>4251</td>\n",
       "      <td>4134</td>\n",
       "      <td>3479</td>\n",
       "      <td>3592</td>\n",
       "      <td>2914</td>\n",
       "      <td>2939</td>\n",
       "      <td>2951</td>\n",
       "      <td>3702</td>\n",
       "      <td>4266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>individual</td>\n",
       "      <td>658</td>\n",
       "      <td>320</td>\n",
       "      <td>286</td>\n",
       "      <td>186</td>\n",
       "      <td>384</td>\n",
       "      <td>499</td>\n",
       "      <td>445</td>\n",
       "      <td>518</td>\n",
       "      <td>...</td>\n",
       "      <td>514</td>\n",
       "      <td>684</td>\n",
       "      <td>639</td>\n",
       "      <td>607</td>\n",
       "      <td>562</td>\n",
       "      <td>451</td>\n",
       "      <td>440</td>\n",
       "      <td>611</td>\n",
       "      <td>589</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>person</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>91</td>\n",
       "      <td>83</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "      <td>112</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>37</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>proband</td>\n",
       "      <td>136</td>\n",
       "      <td>112</td>\n",
       "      <td>101</td>\n",
       "      <td>86</td>\n",
       "      <td>138</td>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>176</td>\n",
       "      <td>192</td>\n",
       "      <td>133</td>\n",
       "      <td>171</td>\n",
       "      <td>160</td>\n",
       "      <td>147</td>\n",
       "      <td>162</td>\n",
       "      <td>207</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>subject</td>\n",
       "      <td>244</td>\n",
       "      <td>105</td>\n",
       "      <td>58</td>\n",
       "      <td>71</td>\n",
       "      <td>132</td>\n",
       "      <td>146</td>\n",
       "      <td>173</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>213</td>\n",
       "      <td>250</td>\n",
       "      <td>170</td>\n",
       "      <td>151</td>\n",
       "      <td>84</td>\n",
       "      <td>134</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>case</td>\n",
       "      <td>1666</td>\n",
       "      <td>1088</td>\n",
       "      <td>794</td>\n",
       "      <td>1041</td>\n",
       "      <td>1355</td>\n",
       "      <td>1168</td>\n",
       "      <td>1248</td>\n",
       "      <td>1546</td>\n",
       "      <td>...</td>\n",
       "      <td>1413</td>\n",
       "      <td>1903</td>\n",
       "      <td>1616</td>\n",
       "      <td>1374</td>\n",
       "      <td>1489</td>\n",
       "      <td>1097</td>\n",
       "      <td>1122</td>\n",
       "      <td>1174</td>\n",
       "      <td>1373</td>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>affected</td>\n",
       "      <td>504</td>\n",
       "      <td>322</td>\n",
       "      <td>223</td>\n",
       "      <td>266</td>\n",
       "      <td>373</td>\n",
       "      <td>363</td>\n",
       "      <td>314</td>\n",
       "      <td>378</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>545</td>\n",
       "      <td>524</td>\n",
       "      <td>415</td>\n",
       "      <td>458</td>\n",
       "      <td>363</td>\n",
       "      <td>342</td>\n",
       "      <td>301</td>\n",
       "      <td>395</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>diagnosed</td>\n",
       "      <td>144</td>\n",
       "      <td>104</td>\n",
       "      <td>71</td>\n",
       "      <td>88</td>\n",
       "      <td>153</td>\n",
       "      <td>135</td>\n",
       "      <td>131</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>195</td>\n",
       "      <td>234</td>\n",
       "      <td>247</td>\n",
       "      <td>206</td>\n",
       "      <td>255</td>\n",
       "      <td>169</td>\n",
       "      <td>168</td>\n",
       "      <td>146</td>\n",
       "      <td>223</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>sufferingfrom</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "      <td>734</td>\n",
       "      <td>439</td>\n",
       "      <td>377</td>\n",
       "      <td>365</td>\n",
       "      <td>523</td>\n",
       "      <td>482</td>\n",
       "      <td>550</td>\n",
       "      <td>657</td>\n",
       "      <td>...</td>\n",
       "      <td>541</td>\n",
       "      <td>589</td>\n",
       "      <td>538</td>\n",
       "      <td>398</td>\n",
       "      <td>514</td>\n",
       "      <td>312</td>\n",
       "      <td>314</td>\n",
       "      <td>241</td>\n",
       "      <td>351</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>typical</td>\n",
       "      <td>84</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>118</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>84</td>\n",
       "      <td>59</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>77</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>102</td>\n",
       "      <td>79</td>\n",
       "      <td>71</td>\n",
       "      <td>98</td>\n",
       "      <td>58</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>173</td>\n",
       "      <td>125</td>\n",
       "      <td>108</td>\n",
       "      <td>93</td>\n",
       "      <td>175</td>\n",
       "      <td>123</td>\n",
       "      <td>153</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>143</td>\n",
       "      <td>199</td>\n",
       "      <td>160</td>\n",
       "      <td>155</td>\n",
       "      <td>135</td>\n",
       "      <td>126</td>\n",
       "      <td>106</td>\n",
       "      <td>116</td>\n",
       "      <td>125</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>atypical</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>67</td>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    set          words  2001  2002  2003  2004  2005  2006  2007  2008  ...  \\\n",
       "0     1       mutation  3035  2009  1567  2192  2595  2571  2243  2885  ...   \n",
       "1     1        variant   295   164   152   136   294   266   308   484  ...   \n",
       "2     2        disease  1343   765   572   615  1002  1045   925  1199  ...   \n",
       "3     2       disorder   654   436   310   337   560   587   521   659  ...   \n",
       "4     2        illness    15    11     3     9     3     9     7    25  ...   \n",
       "5     2      condition   203   132    72    95   130   183   136   232  ...   \n",
       "6     2      diagnosis   655   488   287   366   485   497   414   559  ...   \n",
       "7     2       syndrome  1078   673   499   838   835   900   885   972  ...   \n",
       "8     3        patient  2778  1763  1572  1900  2634  2456  2415  3216  ...   \n",
       "9     3     individual   658   320   286   186   384   499   445   518  ...   \n",
       "10    3         person    88    44    42     7    52    91    83   111  ...   \n",
       "11    3        proband   136   112   101    86   138   121   108   133  ...   \n",
       "12    3        subject   244   105    58    71   132   146   173   214  ...   \n",
       "13    3           case  1666  1088   794  1041  1355  1168  1248  1546  ...   \n",
       "14    4       affected   504   322   223   266   373   363   314   378  ...   \n",
       "15    4      diagnosed   144   104    71    88   153   135   131   192  ...   \n",
       "16    4  sufferingfrom    18     7     9    10    12    22    15    12  ...   \n",
       "17    5         normal   734   439   377   365   523   482   550   657  ...   \n",
       "18    5        typical    84    58    53    63    74    81    89    85  ...   \n",
       "19    5       wildtype    77    46    27    47    53    47    36    56  ...   \n",
       "20    5       abnormal   173   125   108    93   175   123   153   220  ...   \n",
       "21    5       atypical    35    13    30    26    30    30    24    32  ...   \n",
       "\n",
       "    2012  2013  2014  2015  2016  2017  2018  2019  2020  2021  \n",
       "0   2802  4090  3708  3241  3194  2710  2247  1797  2011  1807  \n",
       "1    750  1268  1436  1492  2124  1633  2071  2765  3653  4755  \n",
       "2   1086  1619  1646  1509  1437  1197  1213  1453  1535  1889  \n",
       "3    713   883   887   737   827   739   838   763   987  1320  \n",
       "4     12     9     8    16    18    10    13    11     8     6  \n",
       "5    195   255   255   236   242   171   211   217   234   279  \n",
       "6    529   741   771   697   712   650   598   628   808  1004  \n",
       "7   1035  1365  1136  1020  1035   895  1035   701  1296  1222  \n",
       "8   3405  4251  4134  3479  3592  2914  2939  2951  3702  4266  \n",
       "9    514   684   639   607   562   451   440   611   589   825  \n",
       "10    67    55   112    91    87    37    52    52    42    90  \n",
       "11   121   176   192   133   171   160   147   162   207   276  \n",
       "12   180   213   250   170   151    84   134   106    76    91  \n",
       "13  1413  1903  1616  1374  1489  1097  1122  1174  1373  1721  \n",
       "14   400   545   524   415   458   363   342   301   395   459  \n",
       "15   195   234   247   206   255   169   168   146   223   313  \n",
       "16    17    19    13    14    18     7     8     9    19     6  \n",
       "17   541   589   538   398   514   312   314   241   351   342  \n",
       "18    98   118    95    86    84    59    80    46    88    78  \n",
       "19    65   102    79    71    98    58    51    70    66    82  \n",
       "20   143   199   160   155   135   126   106   116   125   101  \n",
       "21    38    67    39    54    43    37    39    42    63    57  \n",
       "\n",
       "[22 rows x 23 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word_dataframe.to_csv('..\\\\output\\\\batch_' + str(year) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beae41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts['autist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60683fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06bf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634b455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis_1(input, target_words_list):     # define a 'bag of words' function with 2 arguments, an input and a quantity \n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in input['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    list_for_count = []                                                              # and creates an empty list\n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    return counts.most_common(how_many)                                  # and returns the tokens with highest counts \n",
    "                                                                         # up to the quantity specified as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):     # define a 'bag of words' function with 2 arguments, an input and a quantity \n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in input['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    holding_string = [porter.stem(token) for token in holding_string]                # stems the word-tokens\n",
    "    list_for_count = []                                                              # and creates an empty list\n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    return counts.most_common(how_many)                                  # and returns the tokens with highest counts \n",
    "                                                                         # up to the quantity specified as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = bag_of_words_analysis(batch_2001, 20)   # apply bag of words function to all texts, and save the output table\n",
    "                                                           # this will take a while. \n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c47d79",
   "metadata": {},
   "source": [
    "### Steps to take\n",
    "\n",
    "* revise the bag_of_words_analysis function to group by year? ALternatively, create one bag of words for each year. \n",
    "* write a new function that scans the year-bags-of-words for all words on an input list\n",
    "* save the output of that function to a list (or .csv?) \n",
    "* use the outputs to create graphs that track the popularity of all the words on that list over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b77abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(stemmed)\n",
    "print(type(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.most_common(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts['autist'])\n",
    "print(counts['asd'])\n",
    "print(counts['asperg'])\n",
    "print(counts['autism'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
