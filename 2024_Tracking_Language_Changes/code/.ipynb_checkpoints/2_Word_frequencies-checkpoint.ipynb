{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "\n",
    "Now that we have the abstracts in two nice neat .csv files, we need to download/import the packages needed, import the .csv files, and then can get on with the first part of the analysis. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fb2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_abstracts_no_null_texts.csv', 'matched_abstracts_no_null_texts.csv', 'text_match_results.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )                                # check 'results' folder is not empty/has correct stuff\n",
    "\n",
    "#no_null_texts.to_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')\n",
    "\n",
    "#no_nans_matched_texts.to_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bade240",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import and check them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "076b8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')            # one for all of the texts and then\n",
    "matched_texts = pd.read_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')    # one for just those that match the keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33979\n",
      "906\n"
     ]
    }
   ],
   "source": [
    "print (len(all_texts))                        # it is always useful to double check that the length matches your expectations\n",
    "print (len(matched_texts))                    # in this case, we already know how many rows to expect in each file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ae0d8",
   "metadata": {},
   "source": [
    "## Get some basic stats about how texts are spread out over time\n",
    "\n",
    "We know that all of the rows in the files have at least two columns with contents - 'Year' and 'Text'. This means that it is probably a useful thing to get a little schematic and/or table that counts row according to year. Let's do that now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b304d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013.0    2373\n",
      "2001.0    2336\n",
      "2014.0    2240\n",
      "2004.0    2205\n",
      "2016.0    2040\n",
      "2011.0    1967\n",
      "2015.0    1951\n",
      "2008.0    1896\n",
      "2012.0    1871\n",
      "2010.0    1716\n",
      "2009.0    1704\n",
      "2021.0    1570\n",
      "2007.0    1541\n",
      "2005.0    1520\n",
      "2019.0    1437\n",
      "2006.0    1422\n",
      "2002.0    1266\n",
      "2003.0     996\n",
      "2020.0     693\n",
      "2017.0     651\n",
      "2018.0     584\n",
      "Name: Year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_counts_by_year = all_texts['Year'].value_counts()         # this creates a little table with two columns - year and count\n",
    "print(all_counts_by_year)                                     # however, when we print it we can see it has no headers,\n",
    "                                                              # is not in order, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be0be49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year  Counts\n",
      "1   2001.0    2336\n",
      "16  2002.0    1266\n",
      "17  2003.0     996\n",
      "3   2004.0    2205\n",
      "13  2005.0    1520\n",
      "15  2006.0    1422\n",
      "12  2007.0    1541\n",
      "7   2008.0    1896\n",
      "10  2009.0    1704\n",
      "9   2010.0    1716\n",
      "5   2011.0    1967\n",
      "8   2012.0    1871\n",
      "0   2013.0    2373\n",
      "2   2014.0    2240\n",
      "6   2015.0    1951\n",
      "4   2016.0    2040\n",
      "19  2017.0     651\n",
      "20  2018.0     584\n",
      "14  2019.0    1437\n",
      "18  2020.0     693\n",
      "11  2021.0    1570\n"
     ]
    }
   ],
   "source": [
    "all_counts_by_year = pd.DataFrame(all_counts_by_year)                      # so we convert the table to a data frame\n",
    "all_counts_by_year = all_counts_by_year.rename(columns={\"Year\": \"Counts\"}) # name the columns\n",
    "all_counts_by_year = all_counts_by_year.rename_axis('Year').reset_index()  # set the axis to year and reset the index\n",
    "all_counts_by_year = all_counts_by_year.sort_values(by=['Year'])           # and sort by value of year.\n",
    "print(all_counts_by_year)                                                  # Let's just check it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f48ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results[~all_results['Text'].isnull()]\n",
    "len(no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27593f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_counts_by_year = no_null_texts['Year'].value_counts()\n",
    "no_null_counts_by_year = pd.DataFrame(no_null_counts_by_year)\n",
    "no_null_counts_by_year = no_null_counts_by_year.rename(columns={\"Year\": \"All\"})\n",
    "no_null_counts_by_year = no_null_counts_by_year.rename_axis('Year').reset_index()\n",
    "no_null_counts = no_null_counts_by_year.sort_values(by=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_counts_by_year = matched_texts['Year'].value_counts()\n",
    "matched_counts_by_year = pd.DataFrame(matched_counts_by_year)\n",
    "matched_counts_by_year = matched_counts_by_year.rename(columns={\"Year\": \"Matches\"})\n",
    "matched_counts_by_year = matched_counts_by_year.rename_axis('Year').reset_index()\n",
    "matched_counts_by_year = matched_counts_by_year.sort_values(by=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85569dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year = no_null_counts_by_year.merge(matched_counts_by_year, on='Year', how='left')\n",
    "counts_year = counts_year.sort_values(by='Year')\n",
    "counts_year = counts_year.set_index('Year')\n",
    "counts_year.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "# Count word frequencies \n",
    "## Bag of words\n",
    "\n",
    "Proceed through the 'bag of words' steps for the data frames with all texts and then again for the data frame with only the texts that mention the keywords of interest. This approach finds word frequencies for all years together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):\n",
    "    holding_string = \"\"\n",
    "    for text in input['Text']:\n",
    "        holding_string += text\n",
    "    holding_string = word_tokenize(holding_string)\n",
    "    holding_string = [word.lower() for word in holding_string]\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]\n",
    "    holding_string = [porter.stem(token) for token in holding_string]\n",
    "    list_for_count = []\n",
    "    for token in holding_string:\n",
    "        list_for_count.append(token)\n",
    "    counts = Counter(list_for_count)\n",
    "    return counts.most_common(how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(no_null_texts, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_analysis(matched_texts, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292c709",
   "metadata": {},
   "source": [
    "### Word Frequency by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def track_word_over_time(input, target_word):\n",
    "    years = input['Year'].drop_duplicates()\n",
    "    target_counts = []\n",
    "    for year in years:\n",
    "        year_bag = \"\"\n",
    "        for text in input['Text'][input['Year']==year]:\n",
    "            year_bag += text\n",
    "        year_bag = word_tokenize(year_bag)\n",
    "        year_bag = [word.lower() for word in year_bag]\n",
    "        year_bag = [w.translate(table_punctuation) for w in year_bag]\n",
    "        year_bag = (list(filter(lambda x: x, year_bag)))\n",
    "        year_bag = [token for token in year_bag if not token.isdigit()]\n",
    "        year_bag = [token for token in year_bag if token not in stop_words]\n",
    "        year_bag = [porter.stem(token) for token in year_bag]\n",
    "        list_for_count = []\n",
    "        for token in year_bag:\n",
    "            list_for_count.append(token)\n",
    "        counts = Counter(list_for_count)\n",
    "        target_counts.append(counts[target_word])\n",
    "        \n",
    "    target_word_by_year = pd.DataFrame(list(zip(years, target_counts)), columns = ['Year', str(target_word)])\n",
    "    return target_word_by_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD = track_word_over_time(no_null_texts, 'asd')\n",
    "autism = track_word_over_time(no_null_texts, 'autism')\n",
    "autistic = track_word_over_time(no_null_texts, 'autist')\n",
    "asperger = track_word_over_time(no_null_texts, 'asperger')\n",
    "\n",
    "target_words = ASD.merge(autism, on='Year').merge(asperger, on='Year').merge(autistic, on='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = target_words.set_index('Year')\n",
    "target_words = target_words.sort_values(by=['Year'])\n",
    "target_words.plot.line()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc50968",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseas = track_word_over_time(no_null_texts, 'diseas')\n",
    "disord = track_word_over_time(no_null_texts, 'disord')\n",
    "condition = track_word_over_time(no_null_texts, 'condit')\n",
    "syndrom = track_word_over_time(no_null_texts, 'syndrom')\n",
    "\n",
    "target_words_2 = diseas.merge(disord, on='Year').merge(syndrom, on='Year').merge(condition, on='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words_2 = target_words_2.set_index('Year')\n",
    "target_words_2 = target_words_2.sort_values(by=['Year'])\n",
    "target_words_2.plot.line()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e741a",
   "metadata": {},
   "source": [
    "### Word frequency by session code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01707a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912a2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256560e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0f63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
