{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-democrat",
   "metadata": {},
   "source": [
    "# Preliminary work to clean the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-conviction",
   "metadata": {},
   "source": [
    "## Install and import necessary things\n",
    "\n",
    "Start off by installing the required packages (if you don't already have them installed) and then importing all required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "knowing-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "\n",
    "!pip install autocorrect          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welcome-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "\n",
    "import re                         # things we need for RegEx corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-token",
   "metadata": {},
   "source": [
    "# Split the strings writtin in the .txt file \n",
    "\n",
    "## Define the splitting functions\n",
    "\n",
    "So far, the contents of the .txt file is one loooooooooooooooong string. We want to split that string into multiple strings at specified points, according to patterns or regular expressions that are relevant to the specific text. So, I define a function that looks through a string for matches to a regular expression and returns those matches. \n",
    "\n",
    "I then define a function that takes an input folder, an output folder to store the split files, another output folder, and a pre-defined regular expression. This function looks in the input folder, applies the split_with_separators function (among other things) and writes them to a new file saved in the first output folder. It also records the filename and the number of matches to the regular expression in a new .csv file, also saves this in the second output folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "great-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_separators(regex, s):\n",
    "    matches = list(filter(None, regex.split(s)))\n",
    "    return matches\n",
    "\n",
    "def split_text(input, output1, output2, regex):\n",
    "    totals = []\n",
    "    for filename in os.listdir(input):\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='utf-8') as f:\n",
    "            name = filename.replace(r'.txt', \"\")\n",
    "            for line in f:\n",
    "                matches = split_with_separators(regex, line)\n",
    "                length_matches = len(matches)\n",
    "                del matches[0]\n",
    "                with open(output1 + \"\\\\\" + name + \".txt\", \"w\", encoding='utf-8') as fp:\n",
    "                    for match in matches:\n",
    "                        row_contents = matches[0] + \" \" + matches[1]\n",
    "                        del matches [0]\n",
    "                        del matches [0]\n",
    "                        fp.write(\"%s\\n\" % row_contents)\n",
    "                totals_row = [name, length_matches]\n",
    "                totals.append(totals_row)\n",
    "    with open(output2 + \"\\\\\" + \"totals.csv\", \"w\", encoding='utf-8') as out_total:\n",
    "        writer = csv.writer(out_total)\n",
    "        for row in totals:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-gambling",
   "metadata": {},
   "source": [
    "## Test the splitting function\n",
    "\n",
    "Let's apply these splitting functions to the Test folder. To start, we check the contents of the input folder, define the regular expression, run the function with the relevant arguments, and check the output folder. \n",
    "\n",
    "For this test, the regular expression is simple the letter 'e'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceramic-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_pdf_1.txt', 'input_pdf_2.txt', 'input_pdf_3.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\\\\output_texts\\\\Test\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "solar-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_test = re.compile(r\"(e.*?)\")\n",
    "\n",
    "\n",
    "split_text (\"..\\output_texts\\Test\", \"..\\\\for_analysis\\\\Test\", \"..\\\\counts\\\\Test\", regex_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "economic-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_pdf_1.txt', 'input_pdf_2.txt', 'input_pdf_3.txt']\n",
      "['totals.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\Test\"))\n",
    "print(os.listdir(\"..\\\\counts\\\\Test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-newton",
   "metadata": {},
   "source": [
    "All seems well, but you should probably inspect the actual files to be sure things look the way you expect. \n",
    "\n",
    "## Run the splitting function on the target files\n",
    "\n",
    "This time, the regular expression is more complicated and is meant to capture the way that a limited number of capitol letters followed by one or more digits mark the start of each abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arranged-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001abstractICHG.txt',\n",
       " '2002Abstracts.txt',\n",
       " '2003Abstracts.txt',\n",
       " '2004.txt',\n",
       " '2004Abstracts.txt',\n",
       " '2005Abstracts.txt',\n",
       " '2006Abstracts.txt',\n",
       " '2007Abstracts.txt',\n",
       " '2008Abstracts.txt',\n",
       " '2009Abstracts.txt',\n",
       " '2010Abstracts.txt',\n",
       " '2011Abstracts.txt',\n",
       " '2012Abstracts.txt',\n",
       " '2013Abstracts.txt',\n",
       " '2014Abstracts.txt',\n",
       " '2015Abstracts.txt',\n",
       " '2016Abstracts.txt',\n",
       " '2017 electronic posters.txt',\n",
       " '2017 oral presentations.txt',\n",
       " '2017 posters.txt',\n",
       " '2018 electronic posters.txt',\n",
       " '2018 EMPAG.txt',\n",
       " '2018 oral presentation.txt',\n",
       " '2018 posters.txt',\n",
       " '2019 oral presentation.txt',\n",
       " '2019 posters.txt',\n",
       " '2019 posters2.txt',\n",
       " '2020 eposters.txt',\n",
       " '2020 interactive eposter.txt',\n",
       " '2020 oral presentation.txt',\n",
       " '2021 eposters.txt',\n",
       " '2021 oral presentations.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\\\\output_texts\\\\ESHG\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "australian-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_ESHG = re.compile(r\"([PL|S|C|E]\\d+)\")\n",
    "\n",
    "split_text (\"..\\output_texts\\ESHG\", \"..\\\\for_analysis\\\\ESHG\", \"..\\\\counts\\\\ESHG\", regex_ESHG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "apart-melbourne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2001abstractICHG.txt', '2002Abstracts.txt', '2003Abstracts.txt', '2004.txt', '2004Abstracts.txt', '2005Abstracts.txt', '2006Abstracts.txt', '2007Abstracts.txt', '2008Abstracts.txt', '2009Abstracts.txt', '2010Abstracts.txt', '2011Abstracts.txt', '2012Abstracts.txt', '2013Abstracts.txt', '2014Abstracts.txt', '2015Abstracts.txt', '2016Abstracts.txt', '2017 electronic posters.txt', '2017 oral presentations.txt', '2017 posters.txt', '2018 electronic posters.txt', '2018 EMPAG.txt', '2018 oral presentation.txt', '2018 posters.txt', '2019 oral presentation.txt', '2019 posters.txt', '2019 posters2.txt', '2020 eposters.txt', '2020 interactive eposter.txt', '2020 oral presentation.txt', '2021 eposters.txt', '2021 oral presentations.txt']\n",
      "['autism.csv', 'identity_first.csv', 'person_with.csv', 'POS.csv', 'select.csv', 'totals.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\ESHG\"))\n",
    "print(os.listdir(\"..\\\\counts\\\\ESHG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-demonstration",
   "metadata": {},
   "source": [
    "# Check and return only select abstracts\n",
    "\n",
    "## Define the checking function\n",
    "\n",
    "Now, each abstract should be a row of its own within each .txt file. But not all of abstracts will be relevant to the research question, so we need to remove the irrelevant rows and keep the relevant ones. \n",
    "\n",
    "The first step to doing that is to define a function that takes an input folder, a list of keywords, and an output folder as arguments. The function opens the files in the input folder, searches  through each row in the current file for matches to the list of keywords, and writes the name of the file plus the contents of the row that contains a keyword match to a list. Finally, the function eliminates duplicates in that list and writes it to a .csv file is the output folder. \n",
    "\n",
    "For this research question, the keywords of interest should catch 'autistic', 'autism', 'asperger's' and 'aspergers' regardless of whether they start with an upper or lowercase letter, plus 'ASD'. \n",
    "\n",
    "Note: this function is not applied to the Test files because the function is not so very slow now (.txt files are much faster to work with than .pdf) and because the test and target files are now so very different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "instant-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_strings_in_text(input, output, list_of_strings):\n",
    "    list_of_results = []\n",
    "    # Open the file in read only mode\n",
    "    for filename in os.listdir(input):\n",
    "        name = filename.replace(r'.txt', \"\")\n",
    "        line_number = 0\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='ISO-8859-1') as read_obj:\n",
    "            for line in read_obj:\n",
    "                line_number += 1\n",
    "                # For each line, check if line contains any string from the list of strings\n",
    "                for string_to_search in list_of_strings:\n",
    "                    if string_to_search in line:\n",
    "                    # If any string is found in line, then append that line along with line number in list\n",
    "                        list_of_results.append((name, string_to_search, line_number, line.rstrip()))\n",
    "    no_dups_results = list(set(list_of_results))\n",
    "    with open(output + \"\\\\select.csv\", \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        write.writerows(no_dups_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "supposed-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    " match_strings_in_text('..\\\\for_analysis\\\\ESHG', \"..\\\\counts\\\\ESHG\", ['autis', 'Autis', 'ASD', 'Asperger', 'asperger',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-productivity",
   "metadata": {},
   "source": [
    "Always sensible to double check the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "orange-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select.csv', 'totals.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\counts\\\\ESHG\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
