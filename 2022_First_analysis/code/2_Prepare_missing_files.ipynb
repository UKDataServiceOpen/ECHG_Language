{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-democrat",
   "metadata": {},
   "source": [
    "# Preliminary work to prepare missing files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-conviction",
   "metadata": {},
   "source": [
    "## Install and import necessary things\n",
    "\n",
    "Start off by installing the required packages (if you don't already have them installed) and then importing all required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "\n",
    "import re                         # things we need for RegEx corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-token",
   "metadata": {},
   "source": [
    "# Load up the missing files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e1922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_csv(input, output):                                          # keep for referrence - these files need one by one\n",
    "    # Open the file in read only mode                                    # treatment  \n",
    "    for filename in os.listdir(input):\n",
    "        name = filename.replace(r'.txt', \"\")\n",
    "        abstracts = []\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='ISO-8859-1') as read_obj:\n",
    "            abstracts = []\n",
    "            for line in read_obj:\n",
    "                matches = re.split(r'([PL|S|C|E]\\d+.\\d+[A-Z] )', line)\n",
    "                del matches[0]\n",
    "                for match in matches:\n",
    "                    row_contents = []\n",
    "                    row_contents.append([matches[0]])\n",
    "                    row_contents.append([matches[1]])\n",
    "                    del matches[0]\n",
    "                    del matches[0]\n",
    "                    abstracts.append(row_contents)\n",
    "                with open(output + \"\\\\\" + name + \".csv\", \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "                    write = csv.writer(outfile)\n",
    "                    write.writerows(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2052119",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2017 posters.txt', \"r\", encoding='ISO-8859-1') as read_obj:\n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'([PL|S|C|E]\\d+.\\d+[A-Z] )', line)\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2017Test.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf267923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2018 EMPAG.txt', \"r\", encoding='ISO-8859-1') as read_obj:       # 2018 empag\n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'([A-Z]+[\\d].[\\d]+[A-Z]?)', line)\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2018 EMPAG.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e517a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2018 posters.txt', \"r\", encoding='ISO-8859-1') as read_obj:       # 2018 posters\n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'(P[\\d]+.[\\d]+[A-Z])', line)\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2018 posters.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77739dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2019 posters2.txt', \"r\", encoding='ISO-8859-1') as read_obj:       # 2019 posters2\n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'(E-P[\\d]+.[\\d]+)', line)\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2019 posters2.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f35f87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2020 eposters.txt', \"r\", encoding='ISO-8859-1') as read_obj:       # 2020 e posters\n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'(E-P[\\d]+.[\\d]+)', line)\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2020 eposters.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1575990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2020 interactive eposter.txt', \"r\", encoding='ISO-8859-1') as read_obj:  \n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'(P[\\d]+.[\\d]+.[A-Z])', line)                                           # 2020 interactive e posters\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2020 interactive eposter.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df0b83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\output_texts\\\\missing\\\\2021 eposters.txt', \"r\", encoding='ISO-8859-1') as read_obj:  \n",
    "    abstracts = []\n",
    "    for line in read_obj:\n",
    "        matches = re.split(r'(P[\\d]+.[\\d]+.[A-Z])', line)                                           # 2021 e posters\n",
    "        del matches[0]\n",
    "        for match in matches:\n",
    "            row_contents = []\n",
    "            row_contents.append([matches[0]])\n",
    "            row_contents.append([matches[1]])\n",
    "            del matches[0]\n",
    "            del matches[0]\n",
    "            abstracts.append(row_contents)\n",
    "        with open('..\\\\for_analysis\\\\missing\\\\2021 eposters.csv', \"w\", encoding='ISO-8859-1', newline= '') as outfile:\n",
    "            write = csv.writer(outfile)\n",
    "            write.writerows(abstracts)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('..\\\\output_texts\\\\missing', '..\\\\for_analysis\\\\missing' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c22a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a706ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted to split the missing files on the identified abstract session codes at the start of each abstract\n",
    "\n",
    "def split_text(input, output1, output2, regex):\n",
    "    totals = []\n",
    "    for filename in os.listdir(input):\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='utf-8') as f:\n",
    "            name = filename.replace(r'.txt', \"\")\n",
    "            for line in f:\n",
    "                matches = re.split(r'([PL|S|C|E]\\d+.\\d+[A-Z] )', line)\n",
    "                length_matches = len(matches)\n",
    "                del matches[0]\n",
    "                with open(output1 + \"\\\\\" + name + \".txt\", \"w\", encoding='utf-8') as fp:\n",
    "                    for match in matches:\n",
    "                        row_contents = matches[0] + \" \" + matches[1]\n",
    "                        del matches [0]\n",
    "                        del matches [0]\n",
    "                        fp.write(\"%s\\n\" % row_contents)\n",
    "                totals_row = [name, length_matches]\n",
    "                totals.append(totals_row)\n",
    "    with open(output2 + \"\\\\\" + \"totals.csv\", \"w\", encoding='utf-8') as out_total:\n",
    "        writer = csv.writer(out_total)\n",
    "        for row in totals:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-gambling",
   "metadata": {},
   "source": [
    "## Test the splitting function\n",
    "\n",
    "Let's apply these splitting functions to the Test folder. To start, we check the contents of the input folder, define the regular expression, run the function with the relevant arguments, and check the output folder. \n",
    "\n",
    "For this test, the regular expression is simple the letter 'e'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceramic-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017 posters.txt',\n",
       " '2018 EMPAG.txt',\n",
       " '2018 posters.txt',\n",
       " '2019 posters2.txt',\n",
       " '2020 eposters.txt',\n",
       " '2020 interactive eposter.txt',\n",
       " '2021 eposters.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\\\\output_texts\\\\missing\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31363eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_strings_in_text(input, output):\n",
    "    list_of_results = []\n",
    "    # Open the file in read only mode\n",
    "    for filename in os.listdir(input):\n",
    "        name = filename.replace(r'.txt', \"\")\n",
    "        line_number = 0\n",
    "        with open(input + \"\\\\\" + filename, \"r\", encoding='ISO-8859-1') as read_obj:\n",
    "            for line in read_obj:\n",
    "                line_number += 1\n",
    "                # For each line, check if line contains any string from the list of strings\n",
    "                for string_to_search in list_of_strings:\n",
    "                    if string_to_search in line:\n",
    "                    # If any string is found in line, then append that line along with line number in list\n",
    "                        list_of_results.append((name, string_to_search, line_number, line.rstrip()))\n",
    "    no_dups_results = list(set(list_of_results))\n",
    "    with open(output + \"\\\\select.csv\", \"w\", encoding='ISO-8859-1') as outfile:\n",
    "        write = csv.writer(outfile)\n",
    "        write.writerows(no_dups_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_test = re.compile(r\"(e.*?)\")\n",
    "\n",
    "\n",
    "split_text (\"..\\output_texts\\Test\", \"..\\\\for_analysis\\\\Test\", \"..\\\\counts\\\\Test\", regex_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\Test\"))\n",
    "print(os.listdir(\"..\\\\counts\\\\Test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-newton",
   "metadata": {},
   "source": [
    "All seems well, but you should probably inspect the actual files to be sure things look the way you expect. \n",
    "\n",
    "## Run the splitting function on the target files\n",
    "\n",
    "This time, the regular expression is more complicated and is meant to capture the way that a limited number of capitol letters followed by one or more digits mark the start of each abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"..\\\\output_texts\\\\ESHG\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_ESHG = re.compile(r\"([PL|S|C|E]\\d+)\")\n",
    "\n",
    "split_text (\"..\\output_texts\\ESHG\", \"..\\\\for_analysis\\\\ESHG\", \"..\\\\counts\\\\ESHG\", regex_ESHG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\for_analysis\\\\ESHG\"))\n",
    "print(os.listdir(\"..\\\\counts\\\\ESHG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-demonstration",
   "metadata": {},
   "source": [
    "# Check and return only select abstracts\n",
    "\n",
    "## Define the checking function\n",
    "\n",
    "Now, each abstract should be a row of its own within each .txt file. But not all of abstracts will be relevant to the research question, so we need to remove the irrelevant rows and keep the relevant ones. \n",
    "\n",
    "The first step to doing that is to define a function that takes an input folder, a list of keywords, and an output folder as arguments. The function opens the files in the input folder, searches  through each row in the current file for matches to the list of keywords, and writes the name of the file plus the contents of the row that contains a keyword match to a list. Finally, the function eliminates duplicates in that list and writes it to a .csv file is the output folder. \n",
    "\n",
    "For this research question, the keywords of interest should catch 'autistic', 'autism', 'asperger's' and 'aspergers' regardless of whether they start with an upper or lowercase letter, plus 'ASD'. \n",
    "\n",
    "Note: this function is not applied to the Test files because the function is not so very slow now (.txt files are much faster to work with than .pdf) and because the test and target files are now so very different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    " match_strings_in_text('..\\\\for_analysis\\\\ESHG', \"..\\\\counts\\\\ESHG\", ['autis', 'Autis', 'ASD', 'Asperger', 'asperger',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-productivity",
   "metadata": {},
   "source": [
    "Always sensible to double check the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\counts\\\\ESHG\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
