{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Get ready\n",
    "\n",
    "First, download, import, prep packages and such. \n",
    "\n",
    "Then, check the file location and import the .csv files. Remove any with empty text fields. \n",
    "\n",
    "Save a data frame with all the texts and another with only those texts that mention the keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n",
    "!pip install autocorrect        \n",
    "!pip install pyspellchecker \n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_lg -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "271c801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize  \n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "\n",
    "import string \n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESHG2001abstractICHG.csv', 'ESHG2002Abstracts.csv', 'ESHG2016Abstracts.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )\n",
    "\n",
    "files = []\n",
    "def import_results(input):\n",
    "    for f in os.listdir(input):\n",
    "        f = pd.read_csv(input + '\\\\'+ f)\n",
    "        files.append(f)\n",
    "    output = pd.concat(files)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5767"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = import_results(\"..\\\\results\")\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5604"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_null_texts = all_results[~all_results['Text'].isnull()]\n",
    "len(no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "# Count word frequencies \n",
    "## Bag of words\n",
    "\n",
    "Proceed through the 'bag of words' steps for the data frames with all texts and then again for the data frame with only the texts that mention the keywords of interest. This approach finds word frequencies for all years together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):\n",
    "    holding_string = \"\"\n",
    "    for text in input['Text']:\n",
    "        holding_string += text\n",
    "    holding_string = word_tokenize(holding_string)\n",
    "    holding_string = [word.lower() for word in holding_string]\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]\n",
    "    holding_string = [porter.stem(token) for token in holding_string]\n",
    "    list_for_count = []\n",
    "    for token in holding_string:\n",
    "        list_for_count.append(token)\n",
    "    counts = Counter(list_for_count)\n",
    "    return counts.most_common(how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2796355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gene', 10592),\n",
       " ('mutat', 8798),\n",
       " ('patient', 8155),\n",
       " ('genet', 5760),\n",
       " ('studi', 4831),\n",
       " ('use', 4583),\n",
       " ('chromosom', 4274),\n",
       " ('case', 4193),\n",
       " ('famili', 4162),\n",
       " ('analysi', 3989),\n",
       " ('result', 3921),\n",
       " ('diseas', 3458),\n",
       " ('sequenc', 3309),\n",
       " ('clinic', 3210),\n",
       " ('cell', 3141)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_analysis(no_null_texts, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cbabf8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patient', 286),\n",
       " ('gene', 253),\n",
       " ('syndrom', 187),\n",
       " ('mutat', 180),\n",
       " ('autism', 165),\n",
       " ('asd', 155),\n",
       " ('clinic', 149),\n",
       " ('disord', 146),\n",
       " ('genet', 145),\n",
       " ('studi', 142),\n",
       " ('chromosom', 138),\n",
       " ('delet', 134),\n",
       " ('case', 123),\n",
       " ('associ', 113),\n",
       " ('use', 109)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_analysis(matched_texts, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7a17e",
   "metadata": {},
   "source": [
    "## Word Frequency by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe48a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_sent = matched_texts.explode('tokenized_sents')\n",
    "len(isolated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = isolated_sent[~isolated_sent['tokenized_sents'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "len(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918896cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search for word count - mutation - show frequency decreasing over time, increase in variant? gene change? \n",
    "\n",
    "disease also to decrease and condition or disorder to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving output\n",
    "os.makedirs('folder/subfolder', exist_ok=True)  \n",
    "df.to_csv('folder/subfolder/out.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
